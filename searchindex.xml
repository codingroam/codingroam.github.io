<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>go 究竟是路径还是包名</title><url>https://codingroam.github.io/post/go-%E7%A9%B6%E7%AB%9F%E6%98%AF%E8%B7%AF%E5%BE%84%E8%BF%98%E6%98%AF%E5%8C%85%E5%90%8D/</url><categories><category>Golang</category></categories><tags><tag>Golang</tag><tag>Learning</tag></tags><content type="html"> go 究竟是路径还是包名
结论：源文件头部的包导入语句import后面的部分就是一个路径，路径的最后一个分段也不一定是包名，最后一个分段可与包名相同也可不同。
证明：
在$GOPATH/src/github.com/bigwhite/effective-go-book/chapter3-demo1下面建立cmd/app2和pkg/pkg2这两个目录：
$tree -LF 3 chapter3-demo1 chapter3-demo1 ├── cmd/ │ └── app2/ │ └── main.go └── pkg/ └── pkg2/ └── pkg2.go app2/main.go和pkg2/pkg2.go的源码如下：
// pkg2/pkg2.go package mypkg2 import "fmt" func Func1() { fmt.Println("mypkg2.Func1 invoked") } // app2/main.go package main import ( "github.com/bigwhite/effective-go-book/chapter3-demo1/pkg/pkg2" ) func main() { mypkg2.Func1() } 编译运行app2：
$go build github.com/bigwhite/effective-go-book/chapter3-demo1/cmd/app2 $./app2 mypkg2.Func1 invoked 我们看到app2的包导入语句中的路径末尾是pkg2，而在main函数中使用的包名却是mypkg2，这再次印证了包导入语句中的只是一个路径。不过Go语言有一个惯用法，那就是包导入路径的最后一段目录名最好与包名一致，就像pkg1那样：
// pkg1/pkg1.go package pkg1 import "fmt" func Func1() { fmt.Println("mypkg2.Func1 invoked") } // app1/main.go package main import ( "github.com/bigwhite/effective-go-book/chapter3-demo1/pkg/pkg1" ) func main() { pkg1.Func1() } pkg1包导入路径的最后一段目录名为pkg1，而包名也是pkg1。也就是说上面代码中出现的两个pkg1虽然书写上是一模一样的，但代表的含义完全不同：包导入路径上的pkg1表示的是一个目录名，而main函数体中的pkg1则是包名。
关于包导入，Go语言还有一个惯用法：当包名与包导入路径中的最后一个目录名不同时，最好用下面的语法将包名显式放入包导入语句。以上面的app2为例
// app2/main.go package main import ( mypkg2 "github.com/bigwhite/effective-go-book/chapter3-demo1/pkg/pkg2" ) func main() { mypkg2.Func1() } 显然，这种惯用法让代码可读性更好。</content></entry><entry><title>Golang OOP、继承、组合、接口</title><url>https://codingroam.github.io/post/golang-oop%E7%BB%A7%E6%89%BF%E7%BB%84%E5%90%88%E6%8E%A5%E5%8F%A3/</url><categories><category>Golang</category></categories><tags><tag>Golang</tag><tag>Learning</tag></tags><content type="html"> Golang概念初识，oop对比。
传统 OOP 概念
OOP（面向对象编程）是对真实世界的一种抽象思维方式，可以在更高的层次上对所涉及到的实体和实体之间的关系进行更好的管理。
流传很广的 OOP 的三要素是：封装、继承、多态。
对象：可以看做是一些特征的集合，这些特征主要由 属性 和 方法 来体现。
封装：划定了对象的边界，也就是定义了对象。
继承：表明了子对象和父对象之间的关系，子对象是对父对象的扩展，实际上，子对象 “是” 父对象。相当于说“码农是人”。从特征的集合这个意义上说，子对象包含父对象，父对象有的公共特征，子对象全都有。
多态：根据继承的含义，子对象在特性上全包围了父对象，因此，在需要父对象的时候，子对象可以替代父对象。
传统的 OOP 语言，例如 Java，C++，C#，对 OOP 的实现也各不相同。以 Java 为例： Java 支持 extends，也支持 interface。这是两种不同的抽象方式。
extends 就是继承，A extends B，表明 A 是 B 的一种，是概念上的抽象关系。
Class Human { name:string age:int function eat(){} function speak(){} } Class Man extends Human { function fish(){} function drink(){} } Golang 的 OOP
回到 Golang。Golang 并没有 extends，它类似的方式是 Embedding。这种方式并不能实现 is-a 这种定义上的抽象关系，因此 Golang 并没有传统意义上的多态。
注意下面代码中的绿色粗体注释，把 Student 当做 Human 会报错。
package main import "fmt" func main(){ var h Human s := Student{Grade: 1, Major: "English", Human: Human{Name: "Jason", Age: 12, Being: Being{IsLive: true}}} fmt.Println("student:", s) fmt.Println("student:", s.Name, ", isLive:", s.IsLive, ", age:", s.Age, ", grade:", s.Grade, ", major:", s.Major) //h = s // cannot use s (type Student) as type Human in assignment fmt.Println(h) //Heal(s) // cannot use s (type Student) as type Being in argument to Heal Heal(s.Human.Being) // true s.Drink() s.Eat() } type Car struct { Color string SeatCount int } type Being struct { IsLive bool } type Human struct { Being Name string Age int } func (h Human) Eat(){ fmt.Println("human eating...") h.Drink() } func (h Human) Drink(){ fmt.Println("human drinking...") } func (h Human) Move(){ fmt.Println("human moving...") } type Student struct { Human Grade int Major string } func (s Student) Drink(){ fmt.Println("student drinking...") } type Teacher struct { Human School string Major string Grade int Salary int } func (s Teacher) Drink(){ fmt.Println("teacher drinking...") } type IEat interface { Eat() } type IMove interface { Move() } type IDrink interface { Drink() } func Heal(b Being){ fmt.Println(b.IsLive) } 输出结果：
student: {{{true} Jason 12} 1 English} student: Jason , isLive: true , age: 12 , grade: 1 , major: English {{false} 0} true student drinking... human eating... human drinking... 这里有一点需要注意，Student 实现了 Drink 方法，覆盖了 Human 的 Drink，但是没有实现 Eat 方法。因此，Student 在调用 Eat 方法时，调用的是 Human 的 Eat()；而 Human 的 Eat() 调用了 Human 的 Drink()，于是我们看到结果中输出的是 human drinking&hellip; 。这既不同于 Java 类语言的行为，也不同于 prototype 链式继承的行为，Golang 叫做 Embedding，这像是一种寄生关系：Human 寄生在 Student 中，但仍保持一定程度的独立。
Golang 的接口
我们从接口产生的原因来考虑。
代码处理的是各种数据。对于强类型语言来说，非常希望一批数据都是单一类型的，这样它们的行为完全一致。但世界是复杂的，很多时候数据可能包含不同的类型，却有一个或多个共同点。这些共同点就是抽象的基础。单一继承关系解决了 is-a 也就是定义问题，因此可以把子类当做父类来对待。但对于父类不同但又具有某些共同行为的数据，单一继承就不能解决了。单一继承构造的是树状结构，而现实世界中更常见的是网状结构。
于是有了接口。接口是在某一个方面的抽象，也可以看做具有某些相同行为的事物的标签。
但不同于继承，接口是松散的结构，它不和定义绑定。从这一点上来说，Duck Type 相比传统的 extends 是更加松耦合的方式，可以同时从多个维度对数据进行抽象，找出它们的共同点，使用同一套逻辑来处理。
Java 中的接口方式是先声明后实现的强制模式，比如，你要告诉大家你会英语，并且要会听说读写，你才具有英语这项技能。
interface IEnglishSpeaker { ListenEnglish() ReadEnglish() SpeakEnglish() WriteEnglish() } Golang 不同，你不需要声明你会英语，只要你会听说读写了，你就会英语了。也就是实现决定了概念：如果一个人在学校（有 School、Grade、Class 这些属性），还会学习（有 Study() 方法），那么这个人就是个学生。
Duck Type 更符合人类对现实世界的认知过程：我们总是通过认识不同的个体来进行总结归纳，然后抽象出概念和定义。这基本上就是在软件开发的前期工作，抽象建模。
相比较而言， Java 的方式是先定义了关系（接口），然后去实现，这更像是从上帝视角先规划概念产生定义，然后进行造物。
因为 interface 和 object 之间的松耦合，Golang 有 type assertion 这样的方式来判断一个接口是不是某个类型：
value, b := interface.(Type)，value 是 Type 的默认实例；b 是 bool 类型，表明断言是否成立。
// 接上面的例子 v1, b := interface{}(s).(Car) fmt.Println(v1, b) v2, b := interface{}(s).(Being) fmt.Println(v2, b) v3, b := interface{}(s).(Human) fmt.Println(v3, b) v4, b := interface{}(s).(Student) fmt.Println(v4, b) v5, b := interface{}(s).(IDrink) fmt.Println(v5, b) v6, b := interface{}(s).(IEat) fmt.Println(v6, b) v7, b := interface{}(s).(IMove) fmt.Println(v7, b) v8, b := interface{}(s).(int) fmt.Println(v8, b) 输出结果：
{ 0} false {false} false {{false} 0} false {{{true} Jason 12} 1 English} true {{{true} Jason 12} 1 English} true {{{true} Jason 12} 1 English} true &lt;nil> false 0 false 上面的代码中，使用空接口 interface{} 对 s 进行了类型转换，因为 s 是 struct，不是 interface，而类型断言表达式要求点号左边必须为接口。
常用的方式应该是类似泛型的使用方式：
s1 := Student{Grade: 1, Major: "English", Human: Human{Name: "Jason", Age: 12, Being: Being{IsLive: true}}} s2 := Student{Grade: 1, Major: "English", Human: Human{Name: "Tom", Age: 13, Being: Being{IsLive: true}}} s3 := Student{Grade: 1, Major: "English", Human: Human{Name: "Mike", Age: 14, Being: Being{IsLive: true}}} t1 := Teacher{Grade: 1, Major: "English", Salary: 2000, Human: Human{Name: "Michael", Age: 34, Being: Being{IsLive: true}}} t2 := Teacher{Grade: 1, Major: "English", Salary: 3000, Human: Human{Name: "Tony", Age: 31, Being: Being{IsLive: true}}} t3 := Teacher{Grade: 1, Major: "English", Salary: 4000, Human: Human{Name: "Ivy", Age: 40, Being: Being{IsLive: true}}} drinkers := []IDrink{s1, s2, s3, t1, t2, t3} for _, v := range drinkers { switch t := v.(type) { case Student: fmt.Println(t.Name, "is a Student, he/she needs more homework.") case Teacher: fmt.Println(t.Name, "is a Teacher, he/she needs more jobs.") default: fmt.Println("Invalid Human being:", t) } } 输出结果：
Jason is a Student, he/she needs more homework. Tom is a Student, he/she needs more homework. Mike is a Student, he/she needs more homework. Michael is a Teacher, he/she needs more jobs. Tony is a Teacher, he/she needs more jobs. Ivy is a Teacher, he/she needs more jobs. 这段代码中使用了 Type Switch，这种 switch 判断的目标是类型。
Golang：接口为重
了解了 Golang 的 OOP 相关的基本知识后，难免会有疑问，为什么 Golang 要用这种 “非主流” 的方式呢？
Java 之父 James Gosling 在某次会议上有过这样一次问答：
I once attended a Java user group meeting where James Gosling (Java’s inventor) was the featured speaker.
During the memorable Q&amp;A session, someone asked him: “If you could do Java over again, what would you change?” “I’d leave out classes,” he replied. After the laughter died down, he explained that the real problem wasn’t classes per se, but rather implementation inheritance (the extends relationship). Interface inheritance (the implements relationship) is preferable. You should avoid implementation inheritance whenever possible.
大意是：
问：如果你重新做 Java，有什么是你想改变的？
答：我会把类（class）丢掉。真正的问题不在于类本身，而在于基于实现的继承（the extends relationship）。基于接口的继承（the implements relationship）是更好的选择，你应该在任何可能的时候避免使用实现继承。
我的理解是：实现之间应该少用继承式的强关联，多用接口这种弱关联。接口已经可以在很多方面替代继承的作用，比如多态和泛型。而且接口的关系松散、随意，可以有更高的自由度、更多的抽象角度。
以继承为特点的 OOP 只是编程世界的一种抽象方式，在 Golang 的世界里没有继承，只有组合和接口，这看起来更符合 Gosling 的设想。借用那位老人的话：黑猫白猫，捉住老鼠就是好猫。让我来继续探索吧。
注：刚刚学习 Golang 不久，后面可能会发现也许某些理解是错误的。随时修正。</content></entry><entry><title>ShardingSphere分库分表实战</title><url>https://codingroam.github.io/post/shardingsphere%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%AE%9E%E6%88%98/</url><categories><category>MySQL</category><category>ShardingSphere</category><category>中间件</category></categories><tags><tag>MySQL</tag><tag>ShardingSphere</tag><tag>中间件</tag><tag>项目实战</tag></tags><content type="html"> ShardingSphere分库分表实战
一. 项目需求 我们做项目的时候，数据量比较大,单表千万级别的,需要分库分表，于是在网上搜索这方面的开源框架,最常见的就是mycat,sharding-sphere,最终我选择后者,用它来做分库分表比较容易上手。
二. 简介sharding-sphere 官网地址: https://shardingsphere.apache.org/
ShardingSphere是一套开源的分布式数据库中间件解决方案组成的生态圈，它由Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar（计划中）这3款相互独立的产品组成。 他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景。 ShardingSphere定位为关系型数据库中间件，旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 它与NoSQL和NewSQL是并存而非互斥的关系。NoSQL和NewSQL作为新技术探索的前沿，放眼未来，拥抱变化，是非常值得推荐的。反之，也可以用另一种思路看待问题，放眼未来，关注不变的东西，进而抓住事物本质。 关系型数据库当今依然占有巨大市场，是各个公司核心业务的基石，未来也难于撼动，我们目前阶段更加关注在原有基础上的增量，而非颠覆 sharding-jdbc 定位为轻量级Java框架，在Java的JDBC层提供的额外服务。 它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架。 适用于任何基于Java的ORM框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template或直接使用JDBC。 基于任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP等。 支持任意实现JDBC规范的数据库。目前支持MySQL，Oracle，SQLServer和PostgreSQL 三. 项目实战 本项目基于 Spring Boot 2.1.5 使用sharding-sphere + Mybatis-Plus 实现分库分表
pom.xml引入依赖 &lt;?xml version="1.0" encoding="UTF-8"?> &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;parent> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-parent&lt;/artifactId> &lt;version>2.1.5.RELEASE&lt;/version> &lt;relativePath/> &lt;/parent> &lt;groupId>com.xd&lt;/groupId> &lt;artifactId>spring-boot-sharding-table&lt;/artifactId> &lt;version>0.0.1-SNAPSHOT&lt;/version> &lt;name>spring-boot-sharding-table&lt;/name> &lt;description>基于 Spring Boot 2.1.5 使用sharding-sphere + Mybatis-Plus 实现分库分表&lt;/description> &lt;properties> &lt;java.version>1.8&lt;/java.version> &lt;/properties> &lt;dependencies> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-test&lt;/artifactId> &lt;scope>test&lt;/scope> &lt;/dependency> &lt;!--mysql--> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;scope>runtime&lt;/scope> &lt;/dependency> &lt;!--Mybatis-Plus--> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>3.1.1&lt;/version> &lt;/dependency> &lt;!--shardingsphere start--> &lt;!-- for spring boot --> &lt;dependency> &lt;groupId>io.shardingsphere&lt;/groupId> &lt;artifactId>sharding-jdbc-spring-boot-starter&lt;/artifactId> &lt;version>3.1.0&lt;/version> &lt;/dependency> &lt;!-- for spring namespace --> &lt;dependency> &lt;groupId>io.shardingsphere&lt;/groupId> &lt;artifactId>sharding-jdbc-spring-namespace&lt;/artifactId> &lt;version>3.1.0&lt;/version> &lt;/dependency> &lt;!--shardingsphere end--> &lt;!--lombok--> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;version>1.18.8&lt;/version> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;plugins> &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> 创建数据库和表 ds0 ├── user_0 └── user_1 ds1 ├── user_0 └── user_1 既然是分库分表 库结构与表结构一定是一致的
数据库: ds0
CREATE DATABASE IF NOT EXISTS `ds0` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci */; USE `ds0`; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for user_0 -- ---------------------------- DROP TABLE IF EXISTS `user_0`; CREATE TABLE `user_0` ( `id` int(11) NOT NULL, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; -- ---------------------------- -- Table structure for user_1 -- ---------------------------- DROP TABLE IF EXISTS `user_1`; CREATE TABLE `user_1` ( `id` int(11) NOT NULL, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; SET FOREIGN_KEY_CHECKS = 1; 数据库: ds1
CREATE DATABASE IF NOT EXISTS `ds1` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci */; USE `ds1`; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for user_0 -- ---------------------------- DROP TABLE IF EXISTS `user_0`; CREATE TABLE `user_0` ( `id` int(11) NOT NULL, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; -- ---------------------------- -- Table structure for user_1 -- ---------------------------- DROP TABLE IF EXISTS `user_1`; CREATE TABLE `user_1` ( `id` int(11) NOT NULL, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; SET FOREIGN_KEY_CHECKS = 1; application.properties (重点)基本是在这个文件配置的 # 数据源 ds0,ds1 sharding.jdbc.datasource.names=ds0,ds1 # 第一个数据库 sharding.jdbc.datasource.ds0.type=com.zaxxer.hikari.HikariDataSource sharding.jdbc.datasource.ds0.driver-class-name=com.mysql.cj.jdbc.Driver sharding.jdbc.datasource.ds0.jdbc-url=jdbc:mysql://localhost:3306/ds0?characterEncoding=utf-8&amp;&amp;serverTimezone=GMT%2B8 sharding.jdbc.datasource.ds0.username=root sharding.jdbc.datasource.ds0.password=root # 第二个数据库 sharding.jdbc.datasource.ds1.type=com.zaxxer.hikari.HikariDataSource sharding.jdbc.datasource.ds1.driver-class-name=com.mysql.cj.jdbc.Driver sharding.jdbc.datasource.ds1.jdbc-url=jdbc:mysql://localhost:3306/ds1?characterEncoding=utf-8&amp;&amp;serverTimezone=GMT%2B8 sharding.jdbc.datasource.ds1.username=root sharding.jdbc.datasource.ds1.password=root # 水平拆分的数据库（表） 配置分库 + 分表策略 行表达式分片策略 # 分库策略 sharding.jdbc.config.sharding.default-database-strategy.inline.sharding-column=id sharding.jdbc.config.sharding.default-database-strategy.inline.algorithm-expression=ds$->{id % 2} # 分表策略 其中user为逻辑表 分表主要取决于age行 sharding.jdbc.config.sharding.tables.user.actual-data-nodes=ds$->{0..1}.user_$->{0..1} sharding.jdbc.config.sharding.tables.user.table-strategy.inline.sharding-column=age # 分片算法表达式 sharding.jdbc.config.sharding.tables.user.table-strategy.inline.algorithm-expression=user_$->{age % 2} # 主键 UUID 18位数 如果是分布式还要进行一个设置 防止主键重复 #sharding.jdbc.config.sharding.tables.user.key-generator-column-name=id # 打印执行的数据库以及语句 sharding.jdbc.config.props..sql.show=true spring.main.allow-bean-definition-overriding=true 我这次使用配置文件方式实现分库以及分表 以上配置说明:
逻辑表 user
水平拆分的数据库（表）的相同逻辑和数据结构表的总称。例：用户数据根据主键尾数拆分为2张表，分别是user0到user1，他们的逻辑表名为user。
真实表
在分片的数据库中真实存在的物理表。即上个示例中的user0到user1
分片算法:
Hint分片算法
对应HintShardingAlgorithm，用于处理使用Hint行分片的场景。需要配合HintShardingStrategy使用。
分片策略:
行表达式分片策略 对应InlineShardingStrategy。使用Groovy的表达式，提供对SQL语句中的=和IN的分片操作支持，只支持单分片键。对于简单的分片算法，可以通过简单的配置使用，从而避免繁琐的Java代码开发，如: user$->{id % 2} 表示user表根据id模2，而分成2张表，表名称为user0到user_1。
自增主键生成策略
通过在客户端生成自增主键替换以数据库原生自增主键的方式，做到分布式主键无重复。 采用UUID.randomUUID()的方式产生分布式主键。或者 SNOWFLAKE
实体类 package com.zhang.shardingtable.entity; import com.baomidou.mybatisplus.annotation.TableName; import com.baomidou.mybatisplus.extension.activerecord.Model; import groovy.transform.EqualsAndHashCode; import lombok.Data; import lombok.experimental.Accessors; /** * @Classname User * @Description 用户实体类 * @Author * @Date 2019-06-28 17:24 * @Version 1.0 */ @Data @EqualsAndHashCode(callSuper = true) @Accessors(chain = true) @TableName("user") public class User extends Model&lt;User> { /** * 主键Id */ private int id; /** * 名称 */ private String name; /** * 年龄 */ private int age; } dao层 package com.zhang.shardingtable.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.zhang.shardingtable.entity.User; /** * user dao层 * @author lihaodong */ public interface UserMapper extends BaseMapper&lt;User> { } service层以及实现类 UserService
package com.zhang.shardingtable.service; import com.baomidou.mybatisplus.extension.service.IService; import com.zhang.shardingtable.entity.User; import java.util.List; /** * @Classname UserService * @Description 用户服务类 * @Author * @Date 2019-06-28 17:31 * @Version 1.0 */ public interface UserService extends IService&lt;User> { /** * 保存用户信息 * @param entity * @return */ @Override boolean save(User entity); /** * 查询全部用户信息 * @return */ List&lt;User> getUserList(); } UserServiceImpl
package com.zhang.shardingtable.service.Impl; import com.baomidou.mybatisplus.core.toolkit.Wrappers; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import com.zhang.shardingtable.entity.User; import com.zhang.shardingtable.mapper.UserMapper; import com.zhang.shardingtable.service.UserService; import org.springframework.stereotype.Service; import java.util.List; /** * @Classname UserServiceImpl * @Description 用户服务实现类 * @Author * @Date 2019-06-28 17:32 * @Version 1.0 */ @Service public class UserServiceImpl extends ServiceImpl&lt;UserMapper, User> implements UserService { @Override public boolean save(User entity) { return super.save(entity); } @Override public List&lt;User> getUserList() { return baseMapper.selectList(Wrappers.&lt;User>lambdaQuery()); } } 控制类 package com.zhang.shardingtable.controller; import com.zhang.shardingtable.entity.User; import com.zhang.shardingtable.service.UserService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import java.util.List; /** * @Classname UserController * @Description 用户测试控制类 * @Author * @Date 2019-06-28 17:36 * @Version 1.0 */ @RestController public class UserController { @Autowired private UserService userService; @GetMapping("/select") public List&lt;User> select() { return userService.getUserList(); } @GetMapping("/insert") public Boolean insert(User user) { return userService.save(user); } } 四. 测试
启动项目 打开浏览器 分别访问:
http://localhost:8080/insert?id=1&amp;name=lhd&amp;age=12 http://localhost:8080/insert?id=2&amp;name=lhd&amp;age=13 http://localhost:8080/insert?id=3&amp;name=lhd&amp;age=14 http://localhost:8080/insert?id=4&amp;name=lhd&amp;age=15
可以在控制台看到如下展示，表示插入成功了
根据分片算法和分片策略 不同的id以及age取模落入不同的库表 达到了分库分表的结果
有的人说 查询的话 该怎么做呢 其实也帮我们做好了 打开浏览器 访问: http://localhost:8080/select
分别从ds0数据库两张表和ds1两张表查询结果 然后汇总结果返回</content></entry><entry><title>ShardingSphere基本介绍及核心概念</title><url>https://codingroam.github.io/post/shardingsphere%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/</url><categories><category>MySQL</category><category>ShardingSphere</category><category>分库分表</category></categories><tags><tag>MySQL</tag><tag>ShardingSphere</tag><tag>分库分表</tag><tag>中间件</tag><tag>Learning</tag></tags><content type="html"> ShardingSphere基本介绍及核心概念
背景描述 最初设计的系统只用了单机数据库 随着用户的不断增多，考虑到系统的高可用和越来越多的用户请求，开始使用数据库主从架构 当用户量级和业务进一步提升后，写请求越来越多，这时需要开始使用了分库分表 遇到的问题 用户请求量太大 单服务器TPS、内存、IO都是有上限的，需要将请求打散分布到多个服务器 单库数据量太大 单个数据库处理能力有限；单库所在服务器的磁盘空间有限；单库上的操作IO有瓶颈 单表数据量太大 查询、插入、更新操作都会变慢，在加字段、加索引、机器迁移都会产生高负载，影响服务 如何解决 垂直拆分 垂直分库 微服务架构时，业务切割得足够独立，数据也会按照业务切分，保证业务数据隔离，大大提升了数据库的吞吐能力 垂直分表 表中字段太多且包含大字段的时候，在查询时对数据库的IO、内存会受到影响，同时更新数据时，产生的binlog文件会很大，MySQL在主从同步时也会有延迟的风险 水平拆分(数据分片) 水平分表 针对数据量巨大的单张表（比如订单表），按照规则把一张表的数据切分到多张表里面去。但是这些表还是在同一个库中，所以库级别的数据库操作还是有IO瓶颈。 水平分库 将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，只是表中数据集合不同。 水平分库分表能够有效的缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈 水平分库规则 不跨库、不跨表，保证同一类的数据都在同一个服务器上面。 数据在切分之前，需要考虑如何高效的进行数据获取，如果每次查询都要跨越多个节点，就需要谨慎使用。 水平分表规则 - RANGE - 时间：按照年、月、日去切分。例如order_2020、order_202005、order_20200501 - 地域：按照省或市去切分。例如order_beijing、order_shanghai、order_chengdu - 大小：从0到1000000一个表。例如1000001-2000000放一个表，每100万放一个表 - HASH - 用户ID取模 不同的业务使用的切分规则是不一样，就上面提到的切分规则，举例如下： - 站内信 用户维度：用户只能看到发送给自己的消息，其他用户是不可见的，这种情况下是按照用户ID hash分库，在用户查看历史记录翻页查询时，所有的查询请求都在同一个库内 - 用户表 范围法：以用户ID为划分依据，将数据水平切分到两个数据库实例，如：1到1000W在一张表，1000W到2000W在一张表，这种情况会出现单表的负载较高 按照用户ID HASH尽量保证用户数据均衡分到数据库中 如果在登录场景下，用户输入手机号和验证码进行登录，这种情况下，登录时是不是需要扫描所有分库的信息？ 最终方案：用户信息采用ID做切分处理，同时存储用户ID和手机号的映射的关系表（新增一个关系表），关系表采用手机号进行切分。可以通过关系表根据手机号查询到对应的ID，再定位用户信息。
- 流水表 时间维度：可以根据每天新增的流水来判断，选择按照年份分库，还是按照月份分库，甚至也可以按照日期分库 - 订单表 在线招聘网站中，求职者（下面统称C端用户）投递企业（下面统称B端用户）的职位产生的记录称之为订单表。在线上的业务场景中，C端用户看自己的投递记录，每次的投递到了哪个状态，B端用户查看自己收到的简历，对于合适的简历会进行下一步沟通，同一个公司内的员工可以协作处理简历。 如何能同时满足C端和B端对数据查询，不进行跨库处理？ 最终方案：为了同时满足两端用户的业务场景，采用空间换时间，将一次的投递记录存为两份，C端的投递记录以用户ID为分片键，B端收到的简历按照公司ID为分片键 - 主键选择 - UUID：本地生成，不依赖数据库，缺点就是作为主键性能太差 - SNOWFLAKE：百度UidGenerator、美团Leaf、基于SNOWFLAKE算法实现 - 数据一致性 - 强一致性：XA协议 - 最终一致性：TCC、saga、Seata - 数据库扩容 - 成倍增加数据节点，实现平滑扩容 - 成倍扩容以后，表中的部分数据请求已被路由到其他节点上面，可以清理掉 - 业务层改造 - 基于代理层方式：Mycat、Sharding-Proxy、MySQL Proxy - 基于应用层方式：Sharding-jdbc - 分库后面临的问题 - 事务问题：一次投递需要插入两条记录，且分布在不同的服务器上，数据需要保障一致性。 - 跨库跨表的join问题，可以通过下面几种方式进行处理： - 全局表（字典表）：基础数据/配置数据，所有库都拷贝一份 - 字段冗余：可以使用字段冗余就不用join查询了 - 系统层组装：可以在业务层分别查询出来，然后组装起来，逻辑较复杂 - 额外的数据管理负担和数据运算压力：数据库扩容、维护成本变高 官网地址
Apache ShardingSphere是一款开源的分布式数据库中间件组成的生态圈。它由Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar（规划中）这3款相互独立的产品组成。 他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景。
ShardingSphere项目状态如下，目前已经更新到了5.0版本，但是本次学习任然使用4.1版本： ShardingSphere定位为关系型数据库中间件，旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 Sharding-JDBC：被定位为轻量级Java框架，在Java的JDBC层提供的额外服务，以jar包形式使用。 Sharding-Proxy：被定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 Sharding-Sidecar：被定位为Kubernetes或Mesos的云原生数据库代理，以DaemonSet的形式代理所有对数据库的访问。 Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar三者区别如下：
ShardingSphere-JDBC 采用无中心化架构，适用于 Java 开发的高性能的轻量级 OLTP 应用；ShardingSphere-Proxy 提供静态入口以及异构语言的支持，适用于 OLAP 应用以及对分片数据库进行管理和运维的场景。
Apache ShardingSphere 是多接入端共同组成的生态圈。 通过混合使用 ShardingSphere-JDBC 和 ShardingSphere-Proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，使得架构师更加自由地调整适合与当前业务的最佳系统架构。
ShardingSphere安装包下载：https://shardingsphere.apache.org/document/current/cn/downloads/ 使用Git下载工程： https://github.com/apache/incubator-shardingsphere.git
Sharding-JDBC定位为轻量级Java框架，在Java的JDBC层提供的额外服务。 它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架的使用。
适用于任何基于Java的ORM框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template或直接使用JDBC。 基于任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP等。 支持任意实现JDBC规范的数据库。目前支持MySQL，Oracle，SQLServer和PostgreSQL。 Sharding-JDBC主要功能： 数据分片 - 分库、分表 - 读写分离 - 分片策略 - 分布式主键 - 分布式事务 - 标准化的事务接口 - XA强一致性事务 - 柔性事务 - 数据库治理 - 配置动态化 - 编排和治理 - 数据脱敏 - 可视化链路追踪 Sharding-JDBC 内部结构： 图中黄色部分表示的是Sharding-JDBC的入口API，采用工厂方法的形式提供。 目前有ShardingDataSourceFactory和MasterSlaveDataSourceFactory两个工厂类。 - ShardingDataSourceFactory支持分库分表、读写分离操作 - MasterSlaveDataSourceFactory支持读写分离操作 - 图中蓝色部分表示的是Sharding-JDBC的配置对象，提供灵活多变的配置方式。ShardingRuleConfiguration是分库分表配置的核心和入口，它可以包含多个TableRuleConfiguration和MasterSlaveRuleConfiguration。 - TableRuleConfiguration封装的是表的分片配置信息，有5种配置形式对应不同的Configuration类型。 - MasterSlaveRuleConfiguration封装的是读写分离配置信息。 - 图中红色部分表示的是内部对象，由Sharding-JDBC内部使用，应用开发者无需关注。Sharding-JDBC通过ShardingRuleConfiguration和MasterSlaveRuleConfiguration生成真正供ShardingDataSource和MasterSlaveDataSource使用的规则对象。ShardingDataSource和MasterSlaveDataSource实现了DataSource接口，是JDBC的完整实现方案。 Sharding-JDBC初始化流程： 根据配置的信息生成Configuration对象 通过Factory会将Configuration对象转化为Rule对象 通过Factory会将Rule对象与DataSource对象封装 Sharding-JDBC使用DataSource进行分库分表和读写分离操作 Sharding-JDBC 使用过程： 引入maven依赖 &lt;dependency> &lt;groupId>org.apache.shardingsphere&lt;/groupId> &lt;artifactId>sharding-jdbc-core&lt;/artifactId> &lt;version>${latest.release.version}&lt;/version> &lt;/dependency> 注意: 请将${latest.release.version}更改为实际的版本号。
规则配置 Sharding-JDBC可以通过Java，YAML，Spring命名空间和Spring Boot Starter四种方式配置，开发者可根据场景选择适合的配置方式。 创建DataSource 通过ShardingDataSourceFactory工厂和规则配置对象获取ShardingDataSource，然后即可通过DataSource选择使用原生JDBC开发，或者使用JPA, MyBatis等ORM工具。 DataSource dataSource = ShardingDataSourceFactory.createDataSource(dataSourceMap,shardingRuleConfig, props); 官网地址
表概念 真实表 数据库中真实存在的物理表。例如b_order0、b_order1 逻辑表 在分片之后，同一类表结构的名称（总成）。例如b_order。 数据节点 在分片之后，由数据源和数据表组成。例如ds0.b_order1 绑定表 指的是分片规则一致的关系表（主表、子表），例如b_order和b_order_item，均按照order_id分片，则此两个表互为绑定表关系。绑定表之间的多表关联查询不会出现笛卡尔积关联，可以提升关联查询效率。 b_order：b_order0、b_order1 b_order_item：b_order_item0、b_order_item1 select * from b_order o join b_order_item i on(o.order_id=i.order_id)where o.order_id in (10,11); 在不配置绑定表关系时，假设分片键order_id将数值10路由至第0片，将数值11路由至第1片，那么路由后的SQL应该为4条，它们呈现为笛卡尔积： SELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11);
SELECT i.* FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11);
SELECT i.* FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11);
SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); 在配置绑定表关系后，路由的SQL应该为2条： SELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11);
SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11);
广播表 在使用中，有些表没必要做分片，例如字典表、省份信息等，因为他们数据量不大，而且这种表可能需要与海量数据的表进行关联查询。广播表会在不同的数据节点上进行存储，存储的表结构和数据完全相同。 分片概念 分片键 用于分片的数据库字段，是将数据库(表)水平拆分的关键字段。例：将订单表中的订单主键的尾数取模分片，则订单主键为分片字段。 SQL中如果无分片字段，将执行全路由，性能较差。 除了对单分片字段的支持，ShardingSphere也支持根据多个字段进行分片。
分片算法（ShardingAlgorithm） 由于分片算法和业务实现紧密相关，因此并未提供内置分片算法，而是通过分片策略将各种场景提炼出来，提供更高层级的抽象，并提供接口让应用开发者自行实现分片算法。目前提供4种分片算法。
精确分片算法PreciseShardingAlgorithm 用于处理使用单一键作为分片键的=与IN进行分片的场景。 范围分片算法RangeShardingAlgorithm 用于处理使用单一键作为分片键的BETWEEN AND、>、&lt;、>=、&lt;=进行分片的场景。 复合分片算法ComplexKeysShardingAlgorithm 用于处理使用多键作为分片键进行分片的场景，多个分片键的逻辑较复杂，需要应用开发者自行处理其中的复杂度。 Hint分片算法HintShardingAlgorithm 用于处理使用Hint行分片的场景。对于分片字段非SQL决定，而由其他外置条件决定的场景，可使用SQL Hint灵活的注入分片字段。例：内部系统，按照员工登录主键分库，而数据库中并无此字段。SQL Hint支持通过Java API和SQL注释两种方式使用。 分片策略 包含分片键和分片算法，由于分片算法的独立性，将其独立抽离。真正可用于分片操作的是分片键 + 分片算法，也就是分片策略。目前提供5种分片策略。
标准分片策略 对应StandardShardingStrategy。提供对SQL语句中的=, >, &lt;, >=, &lt;=, IN和BETWEEN AND的分片操作支持。StandardShardingStrategy只支持单分片键，提供PreciseShardingAlgorithm和RangeShardingAlgorithm两个分片算法。PreciseShardingAlgorithm是必选的，用于处理=和IN的分片。RangeShardingAlgorithm是可选的，用于处理BETWEEN AND, >, &lt;, >=, &lt;=分片，如果不配置RangeShardingAlgorithm，SQL中的BETWEEN AND将按照全库路由处理。 复合分片策略 对应ComplexShardingStrategy。复合分片策略。提供对SQL语句中的=, >, &lt;, >=, &lt;=, IN和BETWEEN AND的分片操作支持。ComplexShardingStrategy支持多分片键，由于多分片键之间的关系复杂，因此并未进行过多的封装，而是直接将分片键值组合以及分片操作符透传至分片算法，完全由应用开发者实现，提供最大的灵活度。 行表达式分片策略 对应InlineShardingStrategy。使用Groovy的表达式，提供对SQL语句中的=和IN的分片操作支持，只支持单分片键。对于简单的分片算法，可以通过简单的配置使用，从而避免繁琐的Java代码开发，如: t_user_$->{u_id % 8} 表示t_user表根据u_id模8，而分成8张表，表名称为t_user_0到t_user_7。 Hint分片策略 对应HintShardingStrategy。通过Hint指定分片值而非从SQL中提取分片值的方式进行分片的策略。 不分片策略 对应NoneShardingStrategy。不分片的策略。 分片策略配置 对于分片策略存有数据源分片策略和表分片策略两种维度，两种策略的API完全相同。
数据源分片策略 用于配置数据被分配的目标数据源。 表分片策略 用于配置数据被分配的目标表，由于表存在与数据源内，所以表分片策略是依赖数据源分片 策略结果的。 流程剖析 ShardingSphere 3个产品的数据分片功能主要流程是完全一致的，如下图所示。 SQL解析 SQL解析分为词法解析和语法解析。 先通过词法解析器将SQL拆分为一个个不可再分的单词。再使用语法解析器对SQL进行理解，并最终提炼出解析上下文。 Sharding-JDBC采用不同的解析器对SQL进行解析，解析器类型如下： - MySQL解析器 - Oracle解析器 - SQLServer解析器 - PostgreSQL解析器 - 默认SQL解析器 - 查询优化 负责合并和优化分片条件，如OR等。 - SQL路由 根据解析上下文匹配用户配置的分片策略，并生成路由路径。目前支持分片路由和广播路由。 - SQL改写 将SQL改写为在真实数据库中可以正确执行的语句。SQL改写分为正确性改写和优化改写。 - SQL执行 通过多线程执行器异步执行SQL。 - 结果归并 将多个执行结果集归并以便于通过统一的JDBC接口输出。结果归并包括流式归并、内存归并和使 用装饰者模式的追加归并这几种方式。 SQL使用规范 SQL使用规范 - 支持项 - 路由至单数据节点时，目前MySQL数据库100%全兼容，其他数据库完善中。 - 路由至多数据节点时，全面支持DQL、DML、DDL、DCL、TCL。支持分页、去重、排序、分组、聚合、关联查询（不支持跨库关联）。以下用最为复杂的查询为例： ​
- SELECT主语句 SELECT select_expr [, select_expr ...] FROM table_reference [, table_reference ...] [WHERE predicates] [GROUP BY {col_name | position} [ASC | DESC], ...] [ORDER BY {col_name | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] 不支持项 - 路由至多数据节点 - 不支持CASE WHEN、HAVING、UNION (ALL)，有限支持子查询。 除了分页子查询的支持之外(详情请参考分页)，也支持同等模式的子查询。无论嵌套多少层，ShardingSphere都可以解析至第一个包含数据表的子查询，一旦在下层嵌套中再次找到包含数据表的子查询将直接抛出解析异常。
例如，以下子查询可以支持：
SELECT COUNT(*) FROM (SELECT * FROM t_order o) 以下子查询不支持：
SELECT COUNT(*) FROM (SELECT * FROM t_order o WHERE o.id IN (SELECT id FROM t_order WHERE status = ?)) 简单来说，通过子查询进行非功能需求，在大部分情况下是可以支持的。比如分页、统计总数等；而通过子查询实现业务查询当前并不能支持。
由于归并的限制，子查询中包含聚合函数目前无法支持。 不支持包含schema的SQL。因为ShardingSphere的理念是像使用一个数据源一样使用多数据源，因此对SQL的访问都是在同一个逻辑schema之上。 当分片键处于运算表达式或函数中的SQL时，将采用全路由的形式获取结果。 例如下面SQL，create_time为分片键： SELECT * FROM t_order WHERE to_date(create_time, 'yyyy-mm-dd') = '2019-01-01'; 由于ShardingSphere只能通过SQL字面提取用于分片的值，因此当分片键处于运算表达式或函数中时，ShardingSphere无法提前获取分片键位于数据库中的值，从而无法计算出真正的分片值。
当出现此类分片键处于运算表达式或函数中的SQL时，ShardingSphere将采用全路由的形式获取结果。 不支持的SQL示例：
分页查询 完全支持MySQL和Oracle的分页查询，SQLServer由于分页查询较为复杂，仅部分支持.
性能瓶颈： 查询偏移量过大的分页会导致数据库获取数据性能低下，以MySQL为例： SELECT * FROM t_order ORDER BY id LIMIT 1000000, 10 这句SQL会使得MySQL在无法利用索引的情况下跳过1000000条记录后，再获取10条记录，其性能可想而知。 而在分库分表的情况下（假设分为2个库），为了保证数据的正确性，SQL会改写为：
SELECT * FROM t_order ORDER BY id LIMIT 0, 1000010 即将偏移量前的记录全部取出，并仅获取排序后的最后10条记录。这会在数据库本身就执行很慢的情况下，进一步加剧性能瓶颈。 因为原SQL仅需要传输10条记录至客户端，而改写之后的SQL则会传输1,000,010 * 2的记录至客户端。
ShardingSphere的优化 ShardingSphere进行了2个方面的优化。 - 首先，采用流式处理 + 归并排序的方式来避免内存的过量占用。由于SQL改写不可避免的占用了额外的带宽，但并不会导致内存暴涨。 与直觉不同，大多数人认为ShardingSphere会将1,000,010 * 2记录全部加载至内存，进而占用大量内存而导致内存溢出。 但由于每个结果集的记录是有序的，因此ShardingSphere每次比较仅获取各个分片的当前结果集记录，驻留在内存中的记录仅为当前路由到的分片的结果集的当前游标指向而已。 对于本身即有序的待排序对象，归并排序的时间复杂度仅为O(n)，性能损耗很小。 - 其次，ShardingSphere对仅落至单分片的查询进行进一步优化。 落至单分片查询的请求并不需要改写SQL也可以保证记录的正确性，因此在此种情况下，ShardingSphere并未进行SQL改写，从而达到节省带宽的目的。 - 分页方案优化 由于LIMIT并不能通过索引查询数据，因此如果可以保证ID的连续性，通过ID进行分页是比较好的解决方案： SELECT * FROM t_order WHERE id > 100000 AND id &lt;= 100010 ORDER BY id 或通过记录上次查询结果的最后一条记录的ID进行下一页的查询：
SELECT * FROM t_order WHERE id > 100000 LIMIT 10 行表达式 Inline行表达式 InlineShardingStrategy：采用Inline行表达式进行分片的配置。 Inline是可以简化数据节点和分片算法配置信息。主要是解决配置简化、配置一体化。 语法格式： 行表达式的使用非常直观，只需要在配置中使用e x p r e s s i o n 或 { expression }或 expression或->{ expression }标识行表达式即可。例如：
${ ['online', 'offline']}_table${ 1..3} 最终会解析为：
online_table1, online_table2, online_table3, offline_table1, offline_table2, offline_table3 配置数据节点: 对于均匀分布的数据节点，如果数据结构如下：
db0 ├── t_order0 └── t_order1 db1 ├── t_order0 └── t_order1 用行表达式可以简化为：
db${ 0..1}.t_order${ 0..1} 或者
db$->{ 0..1}.t_order$->{ 0..1} 对于自定义的数据节点，如果数据结构如下：
db0 ├── t_order0 └── t_order1 db1 ├── t_order2 ├── t_order3 └── t_order4 用行表达式可以简化为：
db0.t_order${ 0..1},db1.t_order${ 2..4} 或者
db0.t_order$->{ 0..1},db1.t_order$->{ 2..4} 分片算法配置 对于只有一个分片键的使用=和IN进行分片的SQL，可以使用行表达式代替编码方式配置。 行表达式内部的表达式本质上是一段Groovy代码，可以根据分片键进行计算的方式，返回相应的真实数据源或真实表名称。 例如：分为10个库，尾数为0的路由到后缀为0的数据源， 尾数为1的路由到后缀为1的数据源，以此类推。用于表示分片算法的行表达为： ds${id % 10} 或者
ds$->{id % 10} 结果为：ds0、ds1、ds2… ds9
分布式主键 ShardingSphere不仅提供了内置的分布式主键生成器，例如UUID、SNOWFLAKE，还抽离出分布式主键生成器的接口，方便用户自行实现自定义的自增主键生成器。
内置主键生成器：
UUID 采用UUID.randomUUID()的方式产生分布式主键。 SNOWFLAKE 在分片规则配置模块可配置每个表的主键生成策略，默认使用雪花算法，生成64bit的长整型数据，详细规则参考下面的官网地址。 https://shardingsphere.apache.org/document/legacy/4.x/document/cn/features/sharding/other-features/key-generator/
自定义主键生成器：
自定义主键类，实现ShardingKeyGenerator接口 按SPI规范配置自定义主键类 在Apache ShardingSphere中，很多功能实现类的加载方式是通过SPI注入的方式完成的。注意：在resources目录下新建META-INF文件夹，再新建services文件夹，然后新建文件的名字为org.apache.shardingsphere.spi.keygen.ShardingKeyGenerator，打开文件，复制自定义主键类全路径到文件中保存。 自定义主键类应用配置 #对应主键字段名 spring.shardingsphere.sharding.tables.t_book.key-generator.column=id #对应主键类getType返回内容 spring.shardingsphere.sharding.tables.t_book.keygenerator.type=SELFKEY</content></entry><entry><title>Canal基础操作</title><url>https://codingroam.github.io/post/%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3canal%E7%9C%8B%E8%BF%99%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/</url><categories><category>Canal</category><category>中间件</category></categories><tags><tag>中间件</tag><tag>Canal</tag></tags><content type="html"> 回顾Canal基础操作，canal server端配置和client基础代码
我们都知道一个系统最重要的是数据，数据是保存在数据库里。但是很多时候不单止要保存在数据库中，还要同步保存到Elastic Search、HBase、Redis等等。
这时我注意到阿里开源的框架Canal，他可以很方便地同步数据库的增量数据到其他的存储应用。所以在这里总结一下，分享给各位读者参考~
我们先看官网的介绍
canal，译意为水道/管道/沟渠，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费。
这句介绍有几个关键字：增量日志，增量数据订阅和消费。
这里我们可以简单地把canal理解为一个用来同步增量数据的一个工具。
接下来我们看一张官网提供的示意图：
canal的工作原理就是把自己伪装成MySQL slave，模拟MySQL slave的交互协议向MySQL Mater发送 dump协议，MySQL mater收到canal发送过来的dump请求，开始推送binary log给canal，然后canal解析binary log，再发送到存储目的地，比如MySQL，Kafka，Elastic Search等等。
以下参考 canal官网
。
与其问canal能做什么，不如说数据同步有什么作用。
但是canal的数据同步不是全量的，而是增量。基于binary log增量订阅和消费，canal可以做：
数据库镜像 数据库实时备份 索引构建和实时维护 业务cache(缓存)刷新 带业务逻辑的增量数据处理 3.1 首先有一个MySQL服务器 当前的 canal 支持源端 MySQL 版本包括 5.1.x , 5.5.x , 5.6.x , 5.7.x , 8.0.x
我的Linux服务器安装的MySQL服务器是5.7版本。
MySQL的安装这里就不演示了，比较简单，网上也有很多教程。
然后在MySQL中需要创建一个用户，并授权：
-- 使用命令登录：mysql -u root -p -- 创建用户 用户名：canal 密码：Canal@123456 create user 'canal'@'%' identified by 'Canal@123456'; -- 授权 *.*表示所有库 grant SELECT, REPLICATION SLAVE, REPLICATION CLIENT on *.* to 'canal'@'%' identified by 'Canal@123456'; 下一步在MySQL配置文件my.cnf设置如下信息：
[mysqld] # 打开binlog log-bin=mysql-bin # 选择ROW(行)模式 binlog-format=ROW # 配置MySQL replaction需要定义，不要和canal的slaveId重复 server_id=1 改了配置文件之后，重启MySQL，使用命令查看是否打开binlog模式：
查看binlog日志文件列表：
查看当前正在写入的binlog文件：
MySQL服务器这边就搞定了，很简单。
3.2 安装canal 去官网下载页面进行下载：https://github.com/alibaba/canal/releases
我这里下载的是1.1.4的版本：
解压canal.deployer-1.1.4.tar.gz，我们可以看到里面有四个文件夹：
接着打开配置文件conf/example/instance.properties，配置信息如下：
## mysql serverId , v1.0.26+ will autoGen ## v1.0.26版本后会自动生成slaveId，所以可以不用配置 # canal.instance.mysql.slaveId=0 # 数据库地址 canal.instance.master.address=127.0.0.1:3306 # binlog日志名称 canal.instance.master.journal.name=mysql-bin.000001 # mysql主库链接时起始的binlog偏移量 canal.instance.master.position=154 # mysql主库链接时起始的binlog的时间戳 canal.instance.master.timestamp= canal.instance.master.gtid= # username/password # 在MySQL服务器授权的账号密码 canal.instance.dbUsername=canal canal.instance.dbPassword=Canal@123456 # 字符集 canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false # table regex .*\\..*表示监听所有表 也可以写具体的表名，用，隔开 canal.instance.filter.regex=.*\\..* # mysql 数据解析表的黑名单，多个表用，隔开 canal.instance.filter.black.regex= 我这里用的是win10系统，所以在bin目录下找到startup.bat启动：
启动就报错，坑呀：
要修改一下启动的脚本startup.bat：
然后再启动脚本：
这就启动成功了。
首先引入maven依赖：
&lt;dependency> &lt;groupId>com.alibaba.otter&lt;/groupId> &lt;artifactId>canal.client&lt;/artifactId> &lt;version>1.1.4&lt;/version> &lt;/dependency> 然后创建一个canal项目，使用SpringBoot构建，如图所示：
在CannalClient类使用Spring Bean的生命周期函数afterPropertiesSet()：
@Component public class CannalClient implements InitializingBean { private final static int BATCH_SIZE = 1000; @Override public void afterPropertiesSet() throws Exception { // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress("127.0.0.1", 11111), "example", "", ""); try { //打开连接 connector.connect(); //订阅数据库表,全部表 connector.subscribe(".*\\..*"); //回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有ack的地方开始拿 connector.rollback(); while (true) { // 获取指定数量的数据 Message message = connector.getWithoutAck(BATCH_SIZE); //获取批量ID long batchId = message.getId(); //获取批量的数量 int size = message.getEntries().size(); //如果没有数据 if (batchId == -1 || size == 0) { try { //线程休眠2秒 Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } } else { //如果有数据,处理数据 printEntry(message.getEntries()); } //进行 batch id 的确认。确认之后，小于等于此 batchId 的 Message 都会被确认。 connector.ack(batchId); } } catch (Exception e) { e.printStackTrace(); } finally { connector.disconnect(); } } /** * 打印canal server解析binlog获得的实体类信息 */ private static void printEntry(List&lt;Entry> entrys) { for (Entry entry : entrys) { if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) { //开启/关闭事务的实体类型，跳过 continue; } //RowChange对象，包含了一行数据变化的所有特征 //比如isDdl 是否是ddl变更操作 sql 具体的ddl sql beforeColumns afterColumns 变更前后的数据字段等等 RowChange rowChage; try { rowChage = RowChange.parseFrom(entry.getStoreValue()); } catch (Exception e) { throw new RuntimeException("ERROR ## parser of eromanga-event has an error , data:" + entry.toString(), e); } //获取操作类型：insert/update/delete类型 EventType eventType = rowChage.getEventType(); //打印Header信息 System.out.println(String.format("================》; binlog[%s:%s] , name[%s,%s] , eventType : %s", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); //判断是否是DDL语句 if (rowChage.getIsDdl()) { System.out.println("================》;isDdl: true,sql:" + rowChage.getSql()); } //获取RowChange对象里的每一行数据，打印出来 for (RowData rowData : rowChage.getRowDatasList()) { //如果是删除语句 if (eventType == EventType.DELETE) { printColumn(rowData.getBeforeColumnsList()); //如果是新增语句 } else if (eventType == EventType.INSERT) { printColumn(rowData.getAfterColumnsList()); //如果是更新的语句 } else { //变更前的数据 System.out.println("------->; before"); printColumn(rowData.getBeforeColumnsList()); //变更后的数据 System.out.println("------->; after"); printColumn(rowData.getAfterColumnsList()); } } } } private static void printColumn(List&lt;Column> columns) { for (Column column : columns) { System.out.println(column.getName() + " : " + column.getValue() + " update=" + column.getUpdated()); } } } 以上就完成了Java客户端的代码。这里不做具体的处理，仅仅是打印，先有个直观的感受。
最后我们开始测试，首先启动MySQL、Canal Server，还有刚刚写的Spring Boot项目。然后创建表：
CREATE TABLE `tb_commodity_info` ( `id` varchar(32) NOT NULL, `commodity_name` varchar(512) DEFAULT NULL COMMENT '商品名称', `commodity_price` varchar(36) DEFAULT '0' COMMENT '商品价格', `number` int(10) DEFAULT '0' COMMENT '商品数量', `description` varchar(2048) DEFAULT '' COMMENT '商品描述', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='商品信息表'; 然后我们在控制台就可以看到如下信息：
如果新增一条数据到表中：
INSERT INTO tb_commodity_info VALUES('3e71a81fd80711eaaed600163e046cc3','叉烧包','3.99',3,'又大又香的叉烧包，老人小孩都喜欢'); 控制台可以看到如下信息：
canal的好处在于对业务代码没有侵入，因为是基于监听binlog日志去进行同步数据的。实时性也能做到准实时，其实是很多企业一种比较常见的数据同步的方案。
通过上面的学习之后，我们应该都明白canal是什么，它的原理，还有用法。实际上这仅仅只是入门，因为实际项目中我们不是这样玩的…
实际项目我们是配置MQ模式，配合RocketMQ或者Kafka，canal会把数据发送到MQ的topic中，然后通过消息队列的消费者进行处理。
Canal的部署也是支持集群的，需要配合ZooKeeper进行集群管理。
Canal还有一个简单的Web管理界面。
下一篇就讲一下集群部署Canal，配合使用Kafka，同步数据到Redis。
参考资料：Canal官网</content></entry><entry><title>redis单机、主从、哨兵、集群模式</title><url>https://codingroam.github.io/post/redis%E5%8D%95%E6%9C%BA%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/</url><categories><category>Redis</category><category>中间件</category></categories><tags><tag>Redis</tag><tag>中间件</tag><tag>项目实战</tag></tags><content type="html"> redis单机、主从、哨兵、集群模式
1、基本介绍 单机模式是最简单的，redis启动后，业务调用即可。
优点：
部署简单 没有备用节点，成本低 单机不需要同步数据，单个机器的性能高 缺点：
可靠性低，单节点宕机后，redis就无法继续使用 单机的高性能和存储能力，都无法满足双十一这种高性能、高缓存场景。 2、安装 大多数企业都是基于Linux服务器来部署项目，而且Redis官方也没有提供Windows版本的安装包
官网下载redis： https://redis.io/download/
如果说国外的网站太慢，可以在中国友人翻译的redis完整，进行下载，但是并没有最新版本：http://www.redis.cn/ 解压：tar -xzf redis-6.2.6.tar.gz 运行并且编译make &amp;&amp; make install默认的安装路径是在 /usr/local/bin目录 redis-cli：是redis提供的命令行客户端
redis-server：是redis的服务端启动脚本
redis-sentinel：是redis的哨兵启动脚本
进入解压的目录下备份一份redis.conf配置文件。并且进行修改配置redis-conf 文件 # 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0 bind 0.0.0.0 # 守护进程，修改为yes后即可后台运行 daemonize yes # 密码，设置后访问Redis必须输入密码 requirepass 123321 # 监听的端口 port 6379 # 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录 dir . # 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15 databases 1 # 设置redis能够使用的最大内存 maxmemory 512mb # 日志文件，默认为空，不记录日志，可以指定日志文件名 logfile "redis.log" 在解压的目录下进行启动：redis-server redis.conf 查看redis是否启动成功ps -ef |grep redis 启动开机自动启动redis： 建一个系统服务文件：vi /etc/systemd/system/redis.service [Unit] Description=redis-server After=network.target [Service] Type=forking #ExecStart= 服务地址 和配置文件 ExecStart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf PrivateTmp=true [Install] WantedBy=multi-user.target 然后重载系统服务：systemctl daemon-reload
# 启动 systemctl start redis # 停止 systemctl stop redis # 重启 systemctl restart redis # 查看状态 systemctl status redis 3、SpringBoot集成Redis 单机模式 1.创建demo 基于spring Initializr来创建一个springBoot项目 我的springBoot版本是：2.2.5.RELEASE 其中我勾选了Spring Data Redis和Lombok 引入依赖 &lt;!--redis 的依赖--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId> &lt;!-- 1.x 的版本默认采用的连接池技术是 Jedis， 2.0 以上版本默认连接池是 Lettuce, 如果采用 Jedis，需要排除 Lettuce 的依赖。 --> &lt;/dependency> &lt;!-- 连接池依赖--> &lt;dependency> &lt;groupId>org.apache.commons&lt;/groupId> &lt;artifactId>commons-pool2&lt;/artifactId> &lt;/dependency> 配置application.yml配置文件 spring: redis: # Redis服务器地址 host: 192.168.0.121 # Redis服务器端口 port: 6379 # Redis服务器密码 password: root # 选择哪个库，默认0库 database: 0 # 连接超时时间 timeout: 10000ms lettuce: pool: max-active: 8 # 最大连接 max-idle: 8 #最大空闲连接 min-idle: 0 # 最小空闲连接 max-wait: 100 # 连接等待时间 2.创建test测试 @SpringBootTest public class RedisDemoLettuce { @Autowired private StringRedisTemplate stringRedisTemplate; @Test public void testString(){ ValueOperations&lt;String, String> ops = stringRedisTemplate.opsForValue(); ops.set("name", "zhangsan"); System.out.println(ops.get("name")); //zhangsan } @Test public void testHash(){ HashOperations&lt;String, String, String> opsHash = stringRedisTemplate.opsForHash(); opsHash.put("user:001","name","zhangsan"); opsHash.put("user:001","age","13"); String name = opsHash.get("user:001", "name"); String age = opsHash.get("user:001", "age"); System.out.println(name+age); //zhangsan 13 } } 1、基本介绍 主从模式：将一台Redis服务器的数据，复制到其他的Redis服务器。数据的复制是单向的，只能由主节点到从节点。主节点负责读写、从节点只负责读操作。
为什么要开启读写分离？
在大多redis应用环境中，读操作比较多，而写操作比较少。 单节点Redis的并发能力是有上限的，要进一步提高redis的并发能力，就需要搭建主从集群，实现读写分离。 优点：
高可用的基石：如果主节点宕机，从节点可以继续使用 宕机前的备份数据 进行读工作 相对单节点，提高了读的能力。分担了主节点的读压力 缺点：
数据冗余问题，因为主从节点中的数据，都是相同数据 数据也存在存储能力 主节点宕机后，需要人工干预，进行主从切换 2、搭建 在一台虚拟机上进行测试，一共三个节点，一主两从。
创建了3个文件夹，存放配置信息 mkdir tmp-redis 7001 7002 7003 准备配置文件： # 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0 bind 0.0.0.0 # 守护进程，修改为yes后即可后台运行 daemonize yes # 密码，设置后访问Redis必须输入密码 requirepass 123321 # 监听的端口 port 6379 # 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15 databases 1 # 设置redis能够使用的最大内存 maxmemory 512mb 将redis.conf配置信息，移动到是三个配置文件中 echo 7001 7002 7003 | xargs -t -n 1 cp redis-6.2.4/redis.conf 修改三个配置文件中的配置： 第一：修改每个实例的端口、工作目录 修改每个文件夹内的配置文件，将端口分别修改为7001、7002、7003，将rdb文件保存位置都修改为自己所在目录（在/tmp目录执行下列命令）：
sed -i -e 's/6379/7001/g' -e 's/dir .\//dir \/tmp\/7001\//g' 7001/redis.conf sed -i -e 's/6379/7002/g' -e 's/dir .\//dir \/tmp\/7002\//g' 7002/redis.conf sed -i -e 's/6379/7003/g' -e 's/dir .\//dir \/tmp\/7003\//g' 7003/redis.conf 第二：修改每个实例的声明IP 虚拟机本身有多个IP，为了避免将来混乱，我们需要在redis.conf文件中指定每一个实例的绑定ip信息，格式如下：
# redis实例的声明 IP replica-announce-ip 192.168.150.101 # 一键修改 printf '%s\n' 7001 7002 7003 | xargs -I{ } -t sed -i '1a replica-announce-ip 192.168.75.111' { }/redis.conf 开启主从关系 有临时和永久两种模式：
修改配置文件（永久生效） - 在redis.conf中添加一行配置：slaveof - 使用redis-cli客户端连接到redis服务，执行slaveof命令（重启后失效）： slaveof 注意：在5.0以后新增命令replicaof，与salveof效果一致。
开启主从服务： # 第1个 redis-server 7001/redis.conf # 第2个 redis-server 7002/redis.conf # 第3个 redis-server 7003/redis.conf 测试 主能读写、从只能读 可以发现，只有在7001这个master节点上可以执行写操作，7002和7003这两个slave节点只能执行读操作. 3、介绍数据同步： 1.全量同步 全量同步：主从第一次连接时会执行全量同步，将master节点所有的数据拷贝给slave节点。
简述全量同步的流程？
slave节点请求增量同步 master节点判断replid，发现不一致，拒绝增量同步 master将完整内存数据生成RDB，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log中的命 令发送给slave slave执行接收到的命令，保持与master之间的同步 2. 增量同步 增量同步的过程：
slave请求增量同步 master检查replid是否一致？ - 不一致：全量同步 - 一致：去repl_baklog中获取offset后的数据 - 发送offset后的命令 - 执行命令 3.优化Redis主从就集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 4.全量同步和增量同步 简述全量同步和增量同步区别？
全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？
slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？
slave节点断开又恢复，并且在repl_baklog中能找到offset时 1、介绍哨兵 哨兵Sentinel也是一个集群，哨兵节点是特殊的redis节点，不存储数据。 访问redis集群的数据都是通过哨兵集群的，哨兵监控整个redis集群。 Redis的Sentinel最小配置是 一主一从。 哨兵基于主从模式，主从有的优点，哨兵全部都有。
哨兵的作用：
监控：Sentinel会不断检查您的master和slave是否按预期工作 主备切换：如果master故障，sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主，旧的master成为slave 通知：sentinel充当Reids客户端的服务发现来源，当集群发生故障转移时，会将最新消息推送给Redis的客户端。 哨兵的优点：
主从可以自动切换，系统更健壮，可用性更高。 监控 通知 哨兵的缺点：
不能解决海量数据存储问题。不支持数据扩容 高并发写操作的问题 2、 搭建哨兵集群 这里我们搭建一个三节点形成的Sentinel集群，来监管之前的Redis主从集群
创建三个文件夹，存放哨兵的配置信息 mkdir s1 s2 s3 然后我们在s1目录创建一个sentinel.conf文件，添加下面的内容： port 27001 sentinel announce-ip 192.168.150.101 sentinel monitor mymaster 192.168.150.101 7001 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 60000 dir "/tmp/s1" 解读：
port 27001：是当前sentinel实例的端口 sentinel monitor mymaster 192.168.150.101 7001 2：指定主节点信息 - mymaster：主节点名称，自定义，任意写 - 192.168.150.101 7001：主节点的ip和端口 - 2：选举master时的quorum值 然后将s1/sentinel.conf文件拷贝到s2、s3两个目录中（在/tmp目录执行下列命令）：echo s2 s3 | xargs -t -n 1 cp s1/sentinel.conf 修改s2、s3两个文件夹内的配置文件，将端口分别修改为27002、27003： sed -i -e 's/27001/27002/g' -e 's/s1/s2/g' s2/sentinel.conf sed -i -e 's/27001/27003/g' -e 's/s1/s3/g' s3/sentinel.conf 启动 # 第1个 redis-sentinel s1/sentinel.conf # 第2个 redis-sentinel s2/sentinel.conf # 第3个 redis-sentinel s3/sentinel.conf 3、测试主备切换： 尝试让master节点7001宕机，查看sentinel日志： 可以看到哨兵日志：
默认30秒后，才会更新日志，才会认为下线了：
主观下线7001 客观下线，以及有2个了。（3各节点设的2个主观认为下线，则下线） 哨兵先进行主节点，哨兵27003当成了领导者。因为去做故障恢复一个主节点去做就行了。 进行选举新的主节点。找到领导者。 策略是：谁先发现的宕机，谁就去选择主节点：27003先发现的，那么7003选择的是7002是主节点。 slaveof-no noe slave …7002 说明将7002选为主节点 进行广播告诉27003说7002是master了，将7001作为从节点。 6. 将7001切换成slave节点 4、 SpringBoot集成Redis 哨兵模式 在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。
添加配置 加入依赖： &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId> &lt;/dependency> 加入application.yml spring: redis: sentinel: master: mymaster # 指定master名称 nodes: # 指定redis-sentinel集群信息 - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 配置主从读写分离 @Bean public LettuceClientConfigurationBuilderCustomizer configurationBuilderCustomizer(){ return configBuilder -> configBuilder.readFrom(ReadFrom.REPLICA_PREFERRED); } 这里的ReadFrom是配置Redis的读取策略，包括下面选择：
MASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master Redis3.0之前的集群模式，用ShardedJedis实现分片是客户端实现分片，客户端自己计算数据的key应该在哪个机器上存储，这个方法的好处是降低了服务器集群的复杂性，客户端自己分片，服务器节点之间是没有联系的，缺点也很明显，就是客户端要实时的知道当前所有节点的信息，因为当增加一个节点时得动态的重新分片啊。
Redis3.0之后的集群模式，用Cluster是用服务端节点实现的数据分片，即客户端随意与集群中的任何节点通信，节点负责计算某个key在哪个机器上，把计算结果反馈给客户端，客户端再去指定的节点存储查询数据，这是一个重定向的过程。 很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。使用哈希槽的好处就在于可以方便的添加 或 移除节点，当添加或移除节点时，只需要移动对应槽和数据移动到对应节点就行。 1、介绍分片集群模式（3.0之后） 集群中有多个master，每个master保存不同数据 每个master都可以有多个slave节点 master之间通过ping监测彼此健康状态 客户端请求可以访问集群任意节点，最终都会被转发到正确节点 优点：
redis cluster的性能和高可用性均优于哨兵模式 采用多主多从。达到读写高性能 存储海量数据 主从切换 读写分离 缺点：
部署繁琐，运维量大，成本高 2、搭建 集群配置：三个master节点、三个slave节点
将每个节点都有自己的redis.conf 配置：举例如下： port 6379 # 开启集群功能 cluster-enabled yes # 集群的配置文件名称，不需要我们创建，由redis自己维护 cluster-config-file /tmp/6379/nodes.conf # 节点心跳失败的超时时间 cluster-node-timeout 5000 # 持久化文件存放目录 dir /tmp/6379 # 绑定地址 bind 0.0.0.0 # 让redis后台运行 daemonize yes # 注册的实例ip replica-announce-ip 192.168.150.101 # 保护模式 protected-mode no # 数据库数量 databases 1 # 日志 logfile /tmp/6379/run.log 创建6个新目录，将reids.conf复制到不同目录。 echo 7001 7002 7003 8001 8002 8003 | xargs -t -n 1 cp redis.conf 修改每个目录下的redis.conf，将其中的6379修改为与所在目录一致： 会将配置文件中的6379 分别改为 7001 7002 7003 8001 8002 8003
# 进入/tmp目录 cd /tmp # 修改配置文件 printf '%s\n' 7001 7002 7003 8001 8002 8003 | xargs -I{ } -t sed -i 's/6379/{ }/g' { }/redis.conf 一键启动服务： # 一键启动所有服务 printf '%s\n' 7001 7002 7003 8001 8002 8003 | xargs -I{ } -t redis-server { }/redis.conf 通过ps查看状态： ps -ef | grep redis 如果要关闭所有进程，可以执行命令： ps -ef | grep redis | awk '{ print $2}' | xargs kill 或者（推荐这种方式）： printf '%s\n' 7001 7002 7003 8001 8002 8003 | xargs -I{ } -t redis-cli -p { } shutdown 创建集群 在Redis5.0之前创建集群比较麻烦，5.0之后集群管理命令都集成到了redis-cli中。 我们使用的是Redis6.2.4版本，集群管理以及集成到了redis-cli中，格式如下： Redis5.0以后:
redis-cli --cluster create --cluster-replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003 命令说明：
redis-cli &ndash;cluster或者./redis-trib.rb：代表集群操作命令 create：代表是创建集群 &ndash;replicas 1或者&ndash;cluster-replicas 1 ：指定集群中每个master的副本个数为1，此时节点总数 ÷ (replicas + 1) 得到的就是master的数量。因此节点列表中的前n个就是master，其它节点都是slave节点，随机分配到不同master 通过命令可以查看集群状态： redis-cli -p 7001 cluster nodes 测试： 尝试连接7001节点，存储一个数据： 集群操作时，需要给redis-cli加上-c参数才可以： redis-cli -c -p 7001 # 连接 redis-cli -c -p 7001 # 存储数据 set num 123 # 读取数据 get num # 再次存储 set a 1 3、散列插槽概念 Redis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到： 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： • key中包含"{}"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分 • key中不包含“{}”，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。
4、 RedisTemplate访问分片集群 RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致：
引入redis的starter依赖 配置分片集群地址 配置读写分离 与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下： spring: redis: cluster: nodes: # 指定分片集群的每一个节点信息 - 192.168.150.101:7001 - 192.168.150.101:7002 - 192.168.150.101:7003 - 192.168.150.101:8001 - 192.168.150.101:8002 - 192.168.150.101:8003</content></entry><entry><title>redis的实战项目_02缓存、缓存更新策略、穿透、雪崩、击穿、缓存工具封装</title><url>https://codingroam.github.io/post/redis%E7%9A%84%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE02_%E7%BC%93%E5%AD%98%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5%E7%A9%BF%E9%80%8F%E9%9B%AA%E5%B4%A9%E5%87%BB%E7%A9%BF%E7%BC%93%E5%AD%98%E5%B7%A5%E5%85%B7%E5%B0%81%E8%A3%85/</url><categories><category>Redis</category><category>中间件</category></categories><tags><tag>Redis</tag><tag>中间件</tag></tags><content type="html"> redis的实战项目02_缓存、缓存更新策略、穿透、雪崩、击穿、缓存工具封装
缓存就是数据交换的缓冲区（称作Cache [ kæʃ ] ），是存贮数据的临时地方，一般读写性能较高。
例如：web应用
浏览器：【浏览器可以作为缓存：例如页面静态资源】
Tomcat：添加应用层缓存【map、redis】
数据库：数据库缓存【索引】
缓存的作用： 减低后端的负载 请求进入Tomcat后，之前是访问数据库，由于数据库要做磁盘读写，相对效率较低。数据压力较大。 加入缓存后，从缓存中拿到数据，返回前端。 提高读写效率，降低相应时间 数据库读写相应时间长。 缓存redis，读写延迟是微妙级别，可以应对更高的并发问题 缓存的成本： 数据一致性成本 数据库和缓存的数据一致性 代码维护成本 运维成本 搭建集群，需要增加人工成本 最最普通添加缓存案例： 前端穿过来一个id，根据id查询商铺信息。
------controller---- /** * 根据id查询商铺信息 * @param id 商铺id * @return 商铺详情数据 */ @GetMapping("/{id}") public Result queryShopById(@PathVariable("id") Long id) { Result shop = shopService.selectById(id); return shop; } ------------serviceImpl------- @Resource private StringRedisTemplate stringRedisTemplate; @Override public Result selectById(Long id) { //1. 通过id 到redis查询 String shopKey = CACHE_SHOP_KEY+id; String shopJson = stringRedisTemplate.opsForValue().get(shopKey); //2. 判断有没有查询到. 不是空的--》有数据则返回结果 if (StrUtil.isNotBlank(shopJson)){ // 3. 如果查到了则直接返回结果，说明查到了JSON，需要转换为实体类 Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); } //4. 如果没有查到，到数据查询 Shop shop = getById(id); // 5. 数据库没有查询到，返回错误信息 if (shop == null){ return Result.fail("店铺不存在"); } //6. 数据库查询到了，则写入redis String shopStr = JSONUtil.toJsonStr(shop); stringRedisTemplate.opsForValue().set(shopKey,shopStr); //7. 返回数据结果 return Result.ok(shop); } 非常好用的工具包：例如下面格式转换就是用的该工具包
&lt;!--hutool--> &lt;dependency> &lt;groupId>cn.hutool&lt;/groupId> &lt;artifactId>hutool-all&lt;/artifactId> &lt;version>5.7.17&lt;/version> &lt;/dependency> 1、场景的缓存更新策略 1. 内存淘汰： 说明： redis是基于内存来存储的，内存数据有限。原本是redis解决内存不足的问题。redis自身的淘汰机制。当内存不足时，就会触发该机制，根据机制将一部分数据淘汰掉。
一致性：差 是因为，当数据库信息发生了改变，缓存的数据并没有变。当用户请求时，还是会查到旧的数据。
维护成本：无。配置即可
2.超时剔除 说明：添加在缓存中的时间。到期后自动删除。下次查询时，再次更新 一致性：一般 维护成本：低
3. 主动更新 自己编写业务逻辑，修改数据库的同时，修改缓存。
4. 业务场景： 低一致性需求：使用内存淘汰机制。例如店铺类型的查询缓存 高一致性需求：使用主动更新机制，并以超时剔除作为兜底方案。例如店铺详情查询的缓存
2、主动更新的策略的实现： 1.主动更新需要考虑的3点问题： 主动更新：缓存调用者，在更新数据库的同时更新缓存。
1、删除缓存还是更新缓存？
更新缓存：每次更新数据库都更新缓存。无效的写操作较多，不推荐。 例如：更新了一百次数据库，从而也就更新了一百次缓存。如果一次都没有查询缓存。那么无效写缓存操作较多。
推荐：删除缓存：更新数据库时，让缓存删除，查询时在更新缓存。 例如：更新了一百次数据库，从而也就更新了一百次缓存。 当有用户查询该用户时，才会更新缓存。
2、如何保证缓存与数据库的操作的同时成功或失败？
单体系统，将缓存与数据库操作放在一个事务 分布式系统：利用TCC等分布式事务方案
3、先操作缓存还是先操作数据库？ 先删除缓存，在操作数据库 推荐：先操作数据库，在删除缓存。
在先操作数据库，在删除缓存时，导致缓存和数据库不一致问题： 1.当线程1来查询数据库时，恰好缓存失效了。失效的同时，就会到数据库查询数据。比如id=10 2.在微妙级写入缓存时，这时： 3.线程2插入了进来，要更新数据库变为20，并且删除空缓存【是空的，在查询时就已经失效了】 4.这时将数据库查询到的旧数据放到了缓存当中。id=10
2. 具体实现-案例 给查询商铺的缓存添加超时剔除和主动更新的策略 修改ShopController中的业务逻辑，满足下面的需求： ① 根据id查询店铺时，如果缓存未命中，则查询数据库，将数据库结果写入缓存，并设置超时时间 ② 根据id修改店铺时，先修改数据库，再删除缓存 3. 测试： 当执行了更新数据库后，发现缓存中以及没有了数据。 当再次查看商品信息时， 第一：数据库和缓存都有了全新数据 第二：缓存中增加了超时剔除策略。 1、介绍： 缓存穿透：用户查询的数据，在数据库和缓存中都不存在。从而当用户请求过来，如果不加入措施的情况下，会直接访问数据库。 如果不断的发起请求，那么会给数据库带来巨大压力
6个解决方法： 缓存null值 布隆过滤 增加id复杂度，避免被猜测id的规律 做好数据的基础格式校验 加强用户权限校验 做好热点参数的限流 2、解决方法一：缓存空对象 说明： 当用户请求数据时，redis中没有，就会访问数据库，数据库也没有，就会返回空值，先写入redis缓存中，然后返回客户端。 当客户再次访问该数据时，就直接从redis中拿到空值。从而避免访问数据库。
优点： 简单、维护方便
缺点： 1.额外的内存消耗。redis缓存了一些没有用的值。 【解决方法：设置有效期】
2.可能造成短期的不一致。 比如：请求了某一个id，请求时不存在，当把空值写入缓存时，这时将该id，添加到了数据库。从而缓存和数据库中的值不一致。 【解决办法：添加数据库时，覆盖redis缓存】 3、解决方法二：布隆过滤 说明： 在客户端与redis之间，加入一层布隆过滤器。 当用户请求过来以后，先到布隆过滤器，根据计算是否存在该key，如果存在则访问redis。如果不存在拒绝访问。
布隆过滤器原理：是一个bit数组，存放的是二进制位。判断数据库中的数据是否存在时，并不是把数据放到了布隆过滤器，而是通过hash算法，计算出hash值，再将hash值转换为二进制位，保存到布隆过滤器中。
优点： 空间占用小
缺点： 实现复杂、存在误判可能 不存在是真的不存在，存在不一定存在。
4、案例实现：缓存穿透-缓存空对象 需要改动的是： 1.当请求是id在缓存和数据库中都不存在的值时，数据库返回null，写入redis中。并返回null 2.用户从redis缓存中拿数据时，判断是否为空，是空则直接结束。不是空，则返回信息。
-------service代码------------ @Service public class ShopServiceImpl extends ServiceImpl&lt;ShopMapper, Shop> implements IShopService { @Resource private StringRedisTemplate stringRedisTemplate; @Override public Result selectById(Long id) { //1. 通过id 到redis查询 String shopKey = CACHE_SHOP_KEY+id; String shopJson = stringRedisTemplate.opsForValue().get(shopKey); //2. 判断有没有查询到. 不是空的--》有数据则返回结果 // isNOtBlank只有一种情况，真正有数据才能为true。 比如空字符串是false if (StrUtil.isNotBlank(shopJson)){ // 3. 如果查到了则直接返回结果，说明查到了JSON，需要转换为实体类 Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); } // 缓存穿透后，将空字符串保存在缓存中，再次查询时，将缓存返回空数据 if (shopJson == null){ return Result.fail("店铺信息不存在！！"); } //4. 如果没有查到，到数据查询 Shop shop = getById(id); // 5. 数据库没有查询到，返回错误信息 if (shop == null){ // 前端要查id=shopKey， 如果数据库没有，则向缓存中保存“”空字符串，并过期时间2分钟。 stringRedisTemplate.opsForValue().set(shopKey,"",CACHE_NULL_TTL,TimeUnit.MINUTES); return Result.fail("店铺不存在"); } //6. 数据库查询到了，则写入redis String shopStr = JSONUtil.toJsonStr(shop); // 设置超时剔除，CACHE_SHOP_TTL=30L分钟后自动从缓存中删除 stringRedisTemplate.opsForValue().set(shopKey,shopStr,CACHE_SHOP_TTL, TimeUnit.MINUTES); //7. 返回数据结果 return Result.ok(shop); } 解释：isNotBlank方法：
ps：此消息【店铺信息不存在】是缓存到了浏览器页面。所以第二次访问该id也进行了提示。 缓存雪崩：是在指同一时间段大量的缓存key同时失效或者是redis服务宕机，导致大量请求到达数据库，从而给数据库带来巨大的压力。
解决雪崩的4个方法： 同时失效解决方法：给不同的key的TTL添加一个随机值。比如1-5分钟。 比如说可能做缓存的预热，那么批量将数据导入到缓存中。如果给所有数据设置的是同一个TTL值，那么所有的数据将会同一时间过期。造成雪崩。
从而：在缓存预热，批量导入数据时，可以给TTL后面添加一个随机数。这样这些数据，就可以在时间内过期，避免同一时间内失效。
redis宕机解决方法：利用redis集群提高服务的可用性 reids哨兵机制：可以实现对服务的监控。当某个主节点宕机后，哨兵会从从节点中选出一个节点成为主节点。主从还可以实现数据同步，保证数据不会丢失。从而提高服务的可用性、数据安全性。
给缓存业务添加降级限流策略 当出现人们无法抗拒的事故，比如整个集群宕机了。那么可以给服务添加一种降级限流策略。比如sentinel快速失败、拒绝服务。从而避免对数据库的访问。
这样牺牲了部分服务，但是能保护整个数据库的服务。
给业务添加多级缓存 浏览器缓存【缓存静态数据】
反向代理Nginx【做本地缓存】
reids缓存
JVM内部建立缓存
数据库
缓存雪崩是大量key同时过期，导致大量访问到数据库，从而给数据库带来巨大压力。 击穿是部分key过期，导致给数数据库带来巨大压力。
缓存击穿也叫热点key问题，就是一个被高并发访问并且缓存重建业务较复杂（可以理解为查询语句比较耗时的SQL语句）的key突然失效了，无数的请求访问会在瞬间给数据库带来巨大压力。
热点key：就是访问的非常多的某个key。比如秒杀商品。 缓存重建：某个key的缓存时间到期了。就会从新去数据库进行查询，写入redis中。 有些查询数据的时间比较长，比如设计多个表、做运算。从而达到毫秒级别。从而redis中一直都没有缓存。在查询数据库中的这段时间内，就会有无数个请求该key，这些都在redis中未命中，都会去访问数据库。
例如： 当一个热点key失效了，这时有100个请求访问该key。
在缓存中查询未命中，要进行缓存重建 查询数据库，重建缓存数据。【这段时间比较耗时】 查询数据库结束后，写入缓存中 那么：：：当第一个请求在查询数据这段时间内，突然来了100个请求该key，那么请求缓存也是未命中，从而只要是在第一个请求写入缓存之前，来访问该key的请求，就都会去数据库查询。重复以上操作，并且重复查询数据库N次，重复更新缓存N次。 1、解决方法一：互斥锁 无数个请求都去做重建缓存，利用锁的机制。只允许一个请求去查询数据库。
当请求发现缓存未命中。 去获取互斥锁， 只有获取互斥锁成功的请求，才能去数据库进行查询数据。 当写入缓存后 释放锁 在第三步，没有获取互斥锁的请求， 1.会睡眠一会儿 2.然后在重试访问缓存，如果缓存失败 3.再去获取锁，锁也失败，在进行睡眠。 直到第一个请求写入缓存成功后，请求2才能在缓存中拿到数据。
缺点：互相等待，从而性能差 例如构建缓存时间比较久，比如200毫秒。在这一段时间内，涌入的所有线程只能做等待。
2、解决方法二：逻辑过期 缓存击穿的原因是：热点key缓存时间过期了。所以导致未命中，从而重建缓存。 既然如此，那么当在redis中存入数据时，不设置TTL过期时间。 而是：在插入数据时不加入真正的TTL过期时间，加一个字段为过期时间。并不是真正过期时间，而是某个字段内容是过期时间。
线程一：查询缓存发现逻辑时间过期，获取互斥锁成功后， 开启线程二，线程二进行缓存重建。释放互斥锁 线程一直接返回逻辑过期的数据。 当线程三来访问缓存时，发现缓存时间过期、并且互斥锁也获取失败，那么也会返回逻辑过期数据。 直到线程二，缓存重建成功，并且释放互斥锁后。就可以拿到正常缓存数据了。 3、互斥锁和逻辑过期对比 互斥锁可以保证数据一致性，当数据过期后，会一直等待锁释放比较耗时，但是拿到最新数据。 逻辑过期可以性能好，不需要等待释放锁。但是拿到的是过期后的数据。
要求一致性：互斥锁 要求性能：逻辑过期 4、代码实现互斥锁：【案例】 1.代码流程： 根据id查询数据库 redis缓存查询数据 判断是否命中 如果命中，则返回数据 如果没有命中，尝试获取互斥锁 判断是否拿到锁， 如果没有拿到锁，进入休眠，在重新去redis缓存中查询数据。（可以做递归处理，调用本方法） 如果拿到锁，那么进行缓存重建。去数据库查询数据 查询到的数据写入redis中 释放锁 返回数据。 2.对于锁的介绍： setnx命令：给某个key赋值，当这个key不存在的时候才能写入该key。 也就是说key存在，就不执行。 释放锁删除、获取锁就是赋值。 在利用setnx时，往往加入一个有效期TTL，10秒钟。往往一个SQL查询业务，在1秒内完成。从而避免某些原因锁得不到释放。 3.具体代码： ------service层代码：---------- /** * controller 调用的就是该类。 * 根据id，返回数据。 * queryWithMutex：【缓存穿透-空字符串返回】 + 【缓存击穿-互斥锁】 * @param id * @return */ @Override public Result selectById(Long id) { //1.缓存穿透，直接调用下面方法【queryWithPassThrough】即可 //Shop shop = queryWithPassThrough(id); //2. 缓存击穿：利用互斥锁 Shop shop = queryWithMutex(id); // 在方法内，由于返回值是Shop类型，所以返回直接设为了null。 // 那么在这里做一下处理，当返回空时，后端友好提示一下返回结果。 if (shop ==null) { return Result.fail("店铺不存在"); } //7. 返回数据结果 return Result.ok(shop); } /** * mutex：互斥锁的意思 * @param id * @return */ public Shop queryWithMutex(Long id){ //1. 通过id 到redis查询 String shopKey = CACHE_SHOP_KEY+id; String shopJson = stringRedisTemplate.opsForValue().get(shopKey); //2. 判断有没有查询到. 不是空的--》有数据则返回结果 // isNOtBlank只有一种情况，真正有数据才能为true。 比如空字符串是false if (StrUtil.isNotBlank(shopJson)){ // 3. 如果查到了则直接返回结果，说明查到了JSON，需要转换为实体类 return JSONUtil.toBean(shopJson, Shop.class); } // 缓存穿透后，将空字符串保存在缓存中，再次查询时，将缓存返回空数据 if (shopJson != null){ return null; } // 应该给锁设置try，即使抛异常，也必须要要释放锁。 String lockKey= null; Shop shop = null; try { //4. 如果缓存没有查询到，进入缓存重建 // 4.1 获取互斥锁 lockKey = "lock:shop:"+id; boolean isLock = tryLock(lockKey); // 4.2 判断互斥锁是否获取成功 if (!isLock){ // 4.3 如果没有拿到互斥锁，休眠等待+ 然后从新去缓存中查数据 Thread.sleep(50); return queryWithMutex(id); } // 4.4 如果拿到了互斥锁，去数据进行查询数据 shop = getById(id); // 模拟提高缓存重建时间。提高查询数据的时间 Thread.sleep(200); // 5. 判断有没有在数据库中，取到数据。 if (shop == null){ // 缓存穿透问题：如果没有拿到数据： 前端要查id=shopKey， 如果数据库没有，则向缓存中保存“”空字符串，并过期时间2分钟。 stringRedisTemplate.opsForValue().set(shopKey,"",CACHE_NULL_TTL,TimeUnit.MINUTES); return null ; } //6. 数据库查询到了，则写入redis String shopStr = JSONUtil.toJsonStr(shop); // 设置超时剔除，CACHE_SHOP_TTL=30L分钟后自动从缓存中删除 stringRedisTemplate.opsForValue().set(shopKey,shopStr,CACHE_SHOP_TTL, TimeUnit.MINUTES); } catch (InterruptedException e) { //是打断的异常 throw new RuntimeException(e); } finally { // 7.释放互斥锁 unlock(lockKey); } //8. 返回数据结果 return shop; } 4.测试缓存击穿、并发访问 热点key需要满足两个方面： 1.高并发。用Apache JMeter工具来模拟高并发。 2.缓存重建时间比较久。重建时间越久，发生线程并发安全问题就越高。也就是说缓存中并没有数据。 目前测试阶段：为了提高缓存重建时间，在查询数据时，添加睡眠时间： 测试要求： 测试结果：当reids缓存失效时，1000个请求来访问该接口时，只有一个请求线程拿到了锁，并访问了一次数据库。剩下的请求都在睡眠中。 当拿到锁的请求，进行了更新缓存，并且释放锁后。其他请求都在缓存中获取数据 5、代码实现逻辑过期【案例】 1.代码流程： 逻辑过期并不是真正的过期，要求在存储数据到redis的时候，不添加过期时间TTL，而是额外添加一个过期时间的字段。当调用该数据时，通过业务代码来判断该数据是否过期。
前端通过id查询数据 在redis查询缓存。 判断有没有命中。【其实理论上热点key是肯定能命中的，因为设置了永不过期】 如果没有命中，直接返回null 核心 ：命中后，该key在逻辑上有没有过期 如果没有过期，可用状态。直接返回数据即可 如果过期数据：并发请求争抢互斥锁。 判断获取互斥锁是否成功 如果没有拿到互斥锁，直接返回旧数据。说明缓存数据已经开始做缓存重建了。 如果拿到了互斥锁，**返回旧数据**，开启新线程进行缓存重建。 11. 新线程：查询数据库id，写到redis，并设置逻辑过期时间。 12. 新线程：释放锁。 2. 逻辑过期时间问题 再把数据写入到redis中，如何将逻辑过期时间添加到数据中呢？ 在写一个实体类：其中包括 过期时间字段、object类型的变量，用来作为所有数据的类型。 到时候存入逻辑时间过期的值时，将RedisData存入即可。
@Data public class RedisData { //逻辑过期时间 private LocalDateTime expireTime; //data：就是要存入到redis中的数据。也就是shop实例 private Object data; } 3.缓存预热，将热点key加入到redis中 ------------test--- @SpringBootTest class HmDianPingApplicationTests { @Resource private ShopServiceImpl shopService; @Test public void saveShop(){ shopService.save2ShopRedis(1L,10L); } } -----------只写在了service层代码 /** * 释放锁 * @parm key */ private void unlock(String key){ stringRedisTemplate.delete(key); } /** * 模拟缓存预热功能，将一个热点key，设为逻辑过期时间，写入到redis缓存中。 * @param id * @param expireSeconds */ public void save2ShopRedis(Long id,Long expireSeconds){ //商铺的实体类 Shop shop = getById(id); // 用来封装商铺实体类和逻辑过期时间 RedisData redisData = new RedisData(); // 设置数据 redisData.setData(shop); //设置逻辑过期时间 redisData.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); //存入redis中 stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY+id,JSONUtil.toJsonStr(redisData)); } 4. 所有代码： 以下代码包括：逻辑过期、加锁、释放锁、缓存预热 其实整体代码还包括：逻辑时间类【其中包括object数据、过期时间成员变量】
--------sevice层： /** * 根据id，返回数据。 * queryWithMutex：【缓存穿透-空字符串返回】 + 【缓存击穿-互斥锁】 * @param id * @return */ @Override public Result selectById(Long id) { // 3. 缓存击穿 逻辑过期。 不考虑缓存穿透，所有的值都存在。 Shop shop = queryWithLogicalExpire(id); /* 在方法内，由于返回值是Shop类型，所以返回直接设为了null。 那么在这里做一下处理，当返回空时，后端友好提示一下返回结果。 */ if (shop ==null) { return Result.fail("店铺不存在"); } // . 返回数据结果 return Result.ok(shop); } /** * 用逻辑过期---代码实现； * @param id * @return */ public Shop queryWithLogicalExpire(Long id){ //1. 通过id 到redis查询 String shopKey = CACHE_SHOP_KEY+id; String shopJson = stringRedisTemplate.opsForValue().get(shopKey); //2. 判断redis中是否命中 if (StrUtil.isBlank(shopJson)){ //3.如果 未命中，直接返回 return null; } // 4.如果查到了数据：判断是否过期。那么就需要先把JSON反序列化为对象 /*得到的bean是RedisData*/ RedisData redisData = JSONUtil.toBean(shopJson, RedisData.class); JSONObject data = (JSONObject)redisData.getData(); //得到了数据，并转为了bean Shop shop = JSONUtil.toBean(data, Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if (expireTime.isAfter(LocalDateTime.now())){ // 5.1如果没过期，直接返回数据 return shop; } //5.2 数据已经过期，进行缓存重建 。 //6.1 尝试获取互斥锁. String lockKey =LOCK_SHOP_KEY+id; boolean isLock = tryLock(lockKey); // 6.2 判断是否拿到了互斥锁 if (isLock){ // 6.3 成功，开启独立线程，实现缓存重建（去数据库读取数据，并更新数据，释放线程锁） CACHE_REBUILD_EXECUTOR.submit(()->{ //为了方便测试，添加商品数据过期时间为20秒。 try { this.save2ShopRedis(id,20L); } catch (Exception e) { throw new RuntimeException(e); } finally { // 必须要释放锁 unlock(lockKey); } }); } // 6.4 获取互斥锁失败，返回过期的商品信息 return shop; } // 创建线程池： private static final ExecutorService CACHE_REBUILD_EXECUTOR= Executors.newFixedThreadPool(10); /** * 获取锁 * @param key * @return */ private boolean tryLock(String key){ // setnx: setIfAbsent 如果不存在，才会存的意思。 10秒内过期 Boolean aBoolean = stringRedisTemplate.opsForValue().setIfAbsent(key, "1", 10, TimeUnit.SECONDS); /** * 如果直接返回 return aboolean; 会进行拆箱，那么可能空指针 */ return BooleanUtil.isTrue(aBoolean); } /** * 释放锁 * @parm key */ private void unlock(String key){ stringRedisTemplate.delete(key); } /** * 模拟缓存预热功能，将一个热点key，设为逻辑过期时间，写入到redis缓存中。 * @param id * @param expireSeconds */ public void save2ShopRedis(Long id,Long expireSeconds) throws InterruptedException { //商铺的实体类 Shop shop = getById(id); // 缓存重建有一定的延迟时间。 Thread.sleep(200); // 用来封装商铺实体类和逻辑过期时间 RedisData redisData = new RedisData(); // 设置数据 redisData.setData(shop); //设置逻辑过期时间 redisData.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); //存入redis中 stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY+id,JSONUtil.toJsonStr(redisData)); } 5.测试： 缓存击穿的特点：
热点key逻辑过期。【提前将数据插入到缓存中，然后在进行测试】 高并发测试。1秒查询100次访问该热点key。 预先得知结论：当高并发访问该热点数据时，如果判断数据逻辑过期后，会有另外一个新线程进行更新数据，在更新数据前，所有的请求都是拿到旧数据。 为了能模拟上述情况：先将数据缓存到redis中，并等待逻辑过期。 然后只把数据库中的信息进行修改。 然后在看测试回馈结果，就会得出上述结论。从而达到测试目的。 可以看到前面部分数据查询出来的是旧数据： 后面部分是新数据。 基于StringRedisTemplate封装一个缓存工具类，满足下列需求： ✓ 方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间 ✓ 方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓存 击穿问题 ✓ 方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题 ✓ 方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题
1、工具类的代码： package com.hmdp.utils; import cn.hutool.core.util.BooleanUtil; import cn.hutool.core.util.StrUtil; import cn.hutool.json.JSONObject; import cn.hutool.json.JSONUtil; import lombok.extern.slf4j.Slf4j; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.time.LocalDateTime; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.TimeUnit; import java.util.function.Function; import static com.hmdp.utils.RedisConstants.CACHE_NULL_TTL; import static com.hmdp.utils.RedisConstants.LOCK_SHOP_KEY; @Slf4j @Component public class CacheClient { private final StringRedisTemplate stringRedisTemplate; private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public CacheClient(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public void set(String key, Object value, Long time, TimeUnit unit) { stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit); } public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) { // 设置逻辑过期 RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time))); // 写入Redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); } public &lt;R,ID> R queryWithPassThrough( String keyPrefix, ID id, Class&lt;R> type, Function&lt;ID, R> dbFallback, Long time, TimeUnit unit){ String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(json)) { // 3.存在，直接返回 return JSONUtil.toBean(json, type); } // 判断命中的是否是空值 if (json != null) { // 返回一个错误信息 return null; } // 4.不存在，根据id查询数据库 R r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, "", CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); return r; } public &lt;R, ID> R queryWithLogicalExpire( String keyPrefix, ID id, Class&lt;R> type, Function&lt;ID, R> dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); R r = JSONUtil.toBean((JSONObject) redisData.getData(), type); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return r; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ // 6.3.成功，开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -> { try { // 查询数据库 R newR = dbFallback.apply(id); // 重建缓存 this.setWithLogicalExpire(key, newR, time, unit); } catch (Exception e) { throw new RuntimeException(e); }finally { // 释放锁 unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return r; } public &lt;R, ID> R queryWithMutex( String keyPrefix, ID id, Class&lt;R> type, Function&lt;ID, R> dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 3.存在，直接返回 return JSONUtil.toBean(shopJson, type); } // 判断命中的是否是空值 if (shopJson != null) { // 返回一个错误信息 return null; } // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; R r = null; try { boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) { // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit); } // 4.4.获取锁成功，根据id查询数据库 r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, "", CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); } catch (InterruptedException e) { throw new RuntimeException(e); }finally { // 7.释放锁 unlock(lockKey); } // 8.返回 return r; } private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, "1", 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemplate.delete(key); } } 2、如何使用工具类？使用案例： 缓存穿透：参数（key的前缀，key的id，实体的类型，查询数据库的函数，过期时间，和单位）
-----使用案例----- ------service层代码：------- @Resource private CacheClient cacheClient; //该方法就是controller层调用的方法 @Override public Result queryById(Long id) { 参数（key的前缀，key的id，实体的类型，查询数据库的函数，过期时间，和单位） // 解决缓存穿透 Shop shop = cacheClient .queryWithPassThrough(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES); // 互斥锁解决缓存击穿 // Shop shop = cacheClient // .queryWithMutex(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES); // 逻辑过期解决缓存击穿 // Shop shop = cacheClient // .queryWithLogicalExpire(CACHE_SHOP_KEY, id, Shop.class, this::getById, 20L, TimeUnit.SECONDS); if (shop == null) { return Result.fail("店铺不存在！"); } // 7.返回 return Result.ok(shop); }</content></entry><entry><title>redis的实战项目01_模拟短信登录业务</title><url>https://codingroam.github.io/post/redis%E7%9A%84%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE01_%E6%A8%A1%E6%8B%9F%E7%9F%AD%E4%BF%A1%E7%99%BB%E5%BD%95%E4%B8%9A%E5%8A%A1/</url><categories><category>Redis</category><category>中间件</category></categories><tags><tag>Redis</tag><tag>中间件</tag><tag>项目实战</tag></tags><content type="html"> 黑马redis的实战项目模拟短信登录业务
1. 数据库： ⚫ tb_user：用户表 ⚫ tb_user_info：用户详情表 ⚫ tb_shop：商户信息表 ⚫ tb_shop_type：商户类型表 ⚫ tb_blog：用户日记表（达人探店日记） ⚫ tb_follow：用户关注表 ⚫ tb_voucher：优惠券表 ⚫ tb_voucher_order：优惠券的订单表
2. 单体项目介绍： 案例是：前后端分离的单体项目，后端部署在Tomcat，前端部署在Nginx服务器上。 当移动端、客户端、app端发起Nginx请求，得到静态资源。页面再通过Nginx，向服务端发起请求，进行查询数据。查询后，返回给前端，前端在进行渲染。 开启Nginx: D:\Program Files\Nginx_Test\nginx-1.18.0> start .\nginx.exe
短信登录包括：
1、发送短信验证码 1.理论流程 用户通过手机进行获取验证码，进行登录。 1.那么用户会根据手机号请求一个验证码 2.服务端接收到手机号后，进行验证手机号。 2.1校验失败，重新写手机号 2.2校验成功，服务端生成验证码 3.保存验证码到session 4.发送验证码 请求路径：api是表示当前的请求，是一个发向Tomcat服务的请求，会过滤掉。真正的请求路径是：/user/code 请求参数：phone=888888
2.代码操作： controller层： @PostMapping("code") public Result sendCode(@RequestParam("phone") String phone, HttpSession session) { /** * 发送短信验证码并保存验证码 * session：需要把验证码保存到session */ return userService.sendCode(phone,session); } ----------------------sever层----------------------- package com.hmdp.service.impl; import cn.hutool.core.util.RandomUtil; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import com.hmdp.dto.Result; import com.hmdp.entity.User; import com.hmdp.mapper.UserMapper; import com.hmdp.service.IUserService; import com.hmdp.utils.RegexUtils; import lombok.extern.slf4j.Slf4j; import org.springframework.stereotype.Service; import javax.servlet.http.HttpSession; /** * &lt;p> * 服务实现类 * &lt;/p> * */ @Slf4j @Service public class UserServiceImpl extends ServiceImpl&lt;UserMapper, User> implements IUserService { @Override public Result sendCode(String phone, HttpSession session) { //1. 验证手机号 // RegexUtils 是一个工具类，直接将前端传过来的电话，传过去，判断手机格式字是否正常 // 如果返回false 是手机号格式正常。 返回true，是手机号错误。 if (RegexUtils.isPhoneInvalid(phone)){ //2. 验证不通过返回错误提示 log.debug("验证码错误....."); return Result.fail("手机号错误，请重新填写"); } //3. 验证通过，生成验证码 // RandomUtil 是一个工具包。该方法生成6位随机数 String code = RandomUtil.randomNumbers(6); //4. 保存验证码到session session.setAttribute("code",code); //5. 发送验证码 log.debug("验证码发送成功,{}",code); return Result.ok(); } } ---------手机号正则--------------- public static final String PHONE_REGEX = "^1([38][0-9]|4[579]|5[0-3,5-9]|6[6]|7[0135678]|9[89])\\d{8}$"; 2、短信验证码登录、注册 1.理论流程 用户收到了验证码，就可以进行登录或者注册了。 1.用户将手机号和验证码进行提交。 2.服务端进行校验验证码。从session中取出验证码进行比较 2.1比较失败，拒绝登录 2.2比较成功，说明验证码填写的是正确的，但是手机号不能确定是否正确。 3.根据手机号查询用户信息。用户是否存在 3.1如果不存在说明该用户第一次登录，那么为该用户注册成新的用户。然后在进行登录。 3.2该用户以及存在了，可也进行登录 4.登录-》保存用户信息。保存到session中。 2.代码操作： -----------controller --------- /** * 登录功能 * @RequestBody注解: 前端穿过来的是JSON格式，就必须用requestBody注解。并且实体类来接受 * @param loginForm 登录参数，包含手机号、验证码；或者手机号、密码. * session:登录的时候需要把用户信息存到session，验证码校验也需要用到session */ @PostMapping("/login") public Result login(@RequestBody LoginFormDTO loginForm, HttpSession session){ // 实现登录功能 return userService.login(loginForm,session); } ------------service------------------------- public class UserServiceImpl extends ServiceImpl&lt;UserMapper, User> implements IUserService /** * 进行登录功能 * @param loginForm 前端传来的用户信息 * @param session * @return */ @Override public Result login(LoginFormDTO loginForm, HttpSession session) { //1. 判断手机号格式是否正确 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)){ //2.如果不正确则直接返回错误 log.debug("手机号:{}错误",phone); Result.fail("手机号格式错误"); } //3. 判断验证码是否一致，session中对比 //从session中获取验证码，也就是后端生成的验证码 Object cacheCode = session.getAttribute("code"); //前端传过来的验证码 String code = loginForm.getCode(); //如果验证码为空，或者不一致，则返回验证码错误 if (code ==null || cacheCode.toString().equals(code) ){ log.debug("验证码:{}错误",code); Result.fail("验证码错误"); } //4. 用户是否存在 select * from t_user where phone=? // 查询表t_user，当“表中字段”中有等于phone的信息，进行查询处理，查询一条。返回的是一个user实体类 User user = query().eq("phone", phone).one(); //5 如果用户不存在则创建用户，保存到数据库 if (user == null){ user = createUser(phone); } //5. 如果用户存在，保存到session session.setAttribute("user",user); return Result.ok(); } /** * 创建用户，添加到数据库中 * @param phone * @return */ private User createUser(String phone) { User user = new User(); user.setPhone(phone); user.setNickName(USER_NICK_NAME_PREFIX+RandomUtil.randomString(10)); //将用户信息插入到 t_user表中 save(user); return user; } 3、校验登录状态 1.理论流程 用户登录成功后，在进行访问其他页面的的时候，需要进行校验登录状态。 1.基于session进行校验。 基于cookie中的session的id，拿到session。
之前将用户信息保存到了session。session是基于cookie,每个session都有一个id，保存在浏览器的cookie当中。
当用户来访问其他页面时，那么就会携带着自己的cookie，而cookie有session的id。那么就可以基于session中的id，拿到session。从而在session中获取用户信息。
2.判断session有无用户 2.1如果没有，则拦截请求 2.2如果有，则缓存用户信息，将用户信息保存到ThreadLocal。这样其他业务就可以直接从ThreadLoad中获取用户信息。
ThreadLoad是线程域对象，在业务当中，每一个请求到达微服务，都会是一个独立的线程。
ThreadLoad会将数据保存到每一个线程的内部，在线程内部创建一个map来保存。
每一个请求来了以后，都有独立的存储空间，请求之间相互没有干扰。
3.放行。
后续其他请求，就可以从ThreadLoad中取出自己的用户信息
前端在请求接口时，会携带cookie。而登录的凭证就是session的id，而该id是存在cookie其中。 那么服务端根据这个session的id得到session。 从session中获取用户信息。 在根据该用户信息去数据库判断，有无该用户， 如果没有则拦截。 如果有，则保护信息到ThreadLoad，在进行放行；
在springmvc中，拦截器可以在所有controller执行前，进行执行。
那么所有的请求，先经过拦截器，在进行访问各个controller。
拦截器：实现用户登录的校验。
–校验后：
那么后续的业务当中【比如说订单、点赞、评论】，都是需要用户的信息
所有说各个controller也需要的到用户信息。
那么就需要将拦截器拦截后得到的信息，传给各个controller。 需要注意的是：线程安全
–解解方法：ThreadLoad
当拦截器拦截了用户信息后，将用户信息保存到ThreadLoad。
ThreadLoad是一个线程域对象，每一个进入Tomcat的请求都是一个独立的线程。
ThreadLoad就会在线程内开一个内存空间，进行保存对应的用户。这样每个用户相互不干扰
– controller需要用到用户信息时，在从ThreadLoad中取出用户
2.代码操作： -----------------拦截器：---------- package com.hmdp.utils; import com.hmdp.entity.User; import org.springframework.web.servlet.HandlerInterceptor; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import javax.servlet.http.HttpSession; /** * 用户登录拦截器 * @Author Hu */ public class LoginInterceptor implements HandlerInterceptor { /** * pre：前置拦截。 进入controller执行，进行校验。 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1. 获取session对象 HttpSession session = request.getSession(); // 2. 从session中获取用户信息 Object user = session.getAttribute("user"); //3. 判断用户是否存在 if (user == null ){ //4. 如果不存在，则拦截 response.setStatus(401); return false; } //5. 如果存在则保存到ThreadLoad UserHolder.saveUser((User) user); //6. 放行 return true; } /** * 渲染之后，返回用户之前。 用户执行完毕后，销毁对应的用户信息。 * 避免用户泄露 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { UserHolder.removeUser(); } } -----------配置拦截器的configuration----------- package com.hmdp.config; import com.hmdp.utils.LoginInterceptor; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.config.annotation.InterceptorRegistration; import org.springframework.web.servlet.config.annotation.InterceptorRegistry; import org.springframework.web.servlet.config.annotation.WebMvcConfigurer; /** * 配置拦截器 * 可以让LoginInterceptor拦截器生效。 * */ @Configuration public class MvcConfiguration implements WebMvcConfigurer { /** * 添加拦截器 * @param registry */ @Override public void addInterceptors(InterceptorRegistry registry) { //添加拦截器 InterceptorRegistration interceptorRegistration = registry.addInterceptor(new LoginInterceptor()); // 添加配置可以放行哪些路径 interceptorRegistration.excludePathPatterns( "/shop/**", "/voucher/**", "/shop-type/**", "/upload/**", "/blog/hot", "/user/code", "/user/login" ); } } ----------------controller类进行返回用户信息-------- @GetMapping("/me") public Result me(){ // 获取当前登录的用户并返回 return Result.ok(UserHolder.getUser()); } ----ThreadLoad类的信息------该信息是隐藏了用户信息累的实体类 package com.hmdp.utils; import com.hmdp.dto.UserDTO; /** * 创建ThreadLoad */ public class UserHolder { private static final ThreadLocal&lt;UserDTO> tl = new ThreadLocal&lt;>(); //添加到线程里面 public static void saveUser(UserDTO user){ tl.set(user); } // 获取用户信息 public static UserDTO getUser(){ return tl.get(); } // 移除信息 public static void removeUser(){ tl.remove(); } } 3.隐藏用户敏感信息 登录校验功能，返回的信息比较多。比如密码这些重要信息不应该进行返回到前端。 为什么前端收到了所有的用户信息？ 从后往前推：
controller接口：校验成功后，将用户信息返回给前端，返回的是User完整的信息。而user信息是从UserHolder中得到的 拦截器： 从session中获取的用户信息，就是完整的用户信息。直接通过UserHolder的save方法，将所有的用户信息，添加到了拦截器。 那么谁给session存的信息呢？是登录业务时：从数据库中查询了所有的用户信息。直接放到了session中。 解决方法： 4. 也就是上的第三步：将数据库查询出来的用户信息，放到新pojo类中，在放到session中。 5. 也就是第二步：获取session中的就是UserDTo了。不在是整个user用户信息了
改动的代码： session共享问题：多台Tomcat并不共享session共享空间，当请求切换到不同Tomcat服务时导致数据丢失的问题。
存储到ATomcat服务器时，在其他的Tomcat服务器中是看不到的。
1、发送验证码。如何将验证码存入到redis中？ redis的数据类型：用String类型 验证码就是6位数字。 Redis的Key：手机号 确保每一个手机号来验证时，保存的key都是不一样的。那么就用每一个手机号来作为redis的key。 当登录时，取验证码时，将前端传过来的手机号作为key，来获取验证码，进行比较登录成功。 Redis的value：就是验证码 代码操作： 2、验证码登录。如何将用户信息保存到redis 数据类型：hash 保存的是用户信息，用户对象。 redis的key：随机token为key存储用户数据。比如用UUID来生成。 key的要求：key唯一 redis的value：保存整个用户信息 String类型：把java对象序列化为Jason字符串，保存在redis。看起来简单，把整个JSON数据变成一个字符串了。占用内存多，除了保存自身外，还有JSON串的格式，比如冒号、大括号
hash类型：value是一个hashMap类型。把java中每一个字段保存在redis。每个字段是独立的，可以针对单个字段做CRUD。占用更少的内存。只需要保存数据本身就可以了。
代码操作： -----用户登录service层---------- /** * 进行登录功能 * @param loginForm 前端传来的用户信息 * @param session * @return */ @Override public Result login(LoginFormDTO loginForm, HttpSession session) { //1. 判断手机号格式是否正确 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)){ //2.如果不正确则直接返回错误 log.debug("手机号:{}错误",phone); Result.fail("手机号格式错误"); } /*//3. 判断验证码是否一致，session中对比 //从session中获取验证码，也就是上面方法获取验证码时，后端生成的验证码 Object cacheCode = session.getAttribute("code");*/ // 3. 从redis中获取验证码 String cacheCode = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY+phone); String code = loginForm.getCode(); // 4. 如果验证码为空，或者不一致，则返回验证码错误 if (code ==null || !cacheCode.equals(code) ){ log.debug("验证码:{}的错误",code); return Result.fail("验证码错误"); } //5. 用户是否存在 select * from t_user where phone=? /*查询表t_user，当“表中字段”中有等于phone的信息，进行查询处理，查询一条。返回的是一个user实体类*/ User user = query().eq("phone", phone).one(); //6 如果用户不存在则创建用户，保存到数据库 if (user == null){ // 6.如果用户不存在则创建用户，保存到数据库 user = createUser(phone); } /*//7. 如果用户存在，保存到session。 // 通过hutool类中的对象，将用户的部分信息放到session中。 session.setAttribute("user",BeanUtil.copyProperties(user, UserDTO.class)); */ // 7.如果用户存在，以hash的形式保存到redis中 // 7.1生成token，作为登录令牌 String token = UUID.randomUUID().toString(true); //7.2 将User对象转为Hash存储. UserDTO是用户的部分信息，发送给前端即可。 UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); /** * beanToMap是将user实体类转为HashMap类型 * 而StringRedisTemplate&lt;string,string>都是string类型。而userDTO中的id是Long类型，所以报错了 * 允许对key/value进行自定义 * 对字段值的修改器，修改字段值 */ Map&lt;String, Object> userMap = BeanUtil.beanToMap(userDTO,new HashMap&lt;>(), CopyOptions.create() .setIgnoreNullValue(true) // 忽略空的值 .setFieldValueEditor((fieldName,fieldVaule)->fieldVaule.toString())); //7.3 存储用户信息，保存到redis中。 key：token, key:value是用户信息 String tokenKey = LOGIN_USER_KEY+ token; stringRedisTemplate.opsForHash().putAll(tokenKey,userMap); //7.4 为该key，设置有效期 stringRedisTemplate.expire(tokenKey,LOGIN_USER_TTL, TimeUnit.MINUTES); // 8 将token，返回给前端 return Result.ok(token); } /** * 创建用户，添加到数据库中 * @param phone * @return */ private User createUser(String phone) { User user = new User(); user.setPhone(phone); user.setNickName(USER_NICK_NAME_PREFIX+RandomUtil.randomString(10)); //将用户信息插入到 t_user表中 save(user); return user; } 3、校验登录功能。获取用户信息中的token【hash的key】 将redis中token返回给前端。访问接口时就会携带这token来访问接口，服务端就会根据token，获取用户信息。在进行判断是否存在，能否访问接口等等校验功能，以及业务代码功能。 代码操作： -------interceptor拦截器代码：------------- package com.hmdp.utils; import cn.hutool.core.bean.BeanUtil; import cn.hutool.core.util.StrUtil; import com.hmdp.dto.UserDTO; import com.hmdp.entity.User; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.web.servlet.HandlerInterceptor; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import javax.servlet.http.HttpSession; import java.util.Map; import java.util.concurrent.TimeUnit; /** * 用户登录拦截器 * @Author Hu */ public class LoginInterceptor implements HandlerInterceptor { /** * pre：前置拦截。 进入controller执行，进行校验。 * @param request * @param response * @param handler * @return * @throws Exception */ private StringRedisTemplate stringRedisTemplate; public LoginInterceptor(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1. 获取session对象 String token = request.getHeader("authorization"); if (StrUtil.isBlank(token)){ return false; } // 2. 基于token，获取用户信息 String redisKey = RedisConstants.LOGIN_USER_KEY+token; Map&lt;Object, Object> userMap = stringRedisTemplate.opsForHash().entries(redisKey); //3. 判断用户是否存在 if (userMap.isEmpty() ){ //4. 如果不存在，则拦截 response.setStatus(401); return false; } //5.将查询到的Hash类型数据，转换为userDTO对象 UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 6.将userDTO对象，保存到ThreadLoad UserHolder.saveUser(userDTO); //7. 刷新token有效值 stringRedisTemplate.expire(redisKey,RedisConstants.LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.放行 return true; } /** * 渲染之后，返回用户之前。 用户执行完毕后，销毁对应的用户信息。 * 避免用户泄露 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { UserHolder.removeUser(); } } 在拦截器中：不能使用@Aotuwired、@Resurces注解，自动导入。 因为interceptor拦截器类是手动new出来的，并不是@Component、@Configuration等注解来构建的。也就是说不是由spring来构建的，就不能进行依赖注入。 所以说通过构造函数注入，谁来注入呢？那就是谁用了类，谁就来注入该类。而MVC的configuration拦截器用到了。从而mvcConfig来帮助redisTemplate来注入到spring中。
4、测试： 用户访问任何一个页面，都会进行刷新登录token的倒计时：
----------拦截所有请求，都进行刷新token---------- package com.hmdp.utils; import cn.hutool.core.bean.BeanUtil; import com.hmdp.dto.UserDTO; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.web.servlet.HandlerInterceptor; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.util.Map; import java.util.concurrent.TimeUnit; public class RefreshTokenInterceptor implements HandlerInterceptor { private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1. 获取token String token = request.getHeader("authorization"); //2. 根据token获取对象用户信息 String redisKey = RedisConstants.LOGIN_USER_KEY+token; Map&lt;Object, Object> userMap = stringRedisTemplate.opsForHash().entries(redisKey); //3.判断用户是否存在 if (userMap.isEmpty()){ return true; } // 4. 将查询到的userMap转为UserDTO UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 5. 存在，将用户信息存入ThreadLoad UserHolder.saveUser(userDTO); // 6. 刷新token有效期 stringRedisTemplate.expire(redisKey,RedisConstants.LOGIN_USER_TTL, TimeUnit.MINUTES); // 7.放行 return true; } /** * 渲染之后，返回用户之前。 用户执行完毕后，销毁对应的用户信息。 * 避免用户泄露 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { UserHolder.removeUser(); } } ---------拦截部分请求，刷洗token--------- package com.hmdp.utils; import org.springframework.web.servlet.HandlerInterceptor; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * 用户登录拦截器 * @Author Hu */ public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1. 判断是否拦截： 查看ThreadLoad是否有用户 if (UserHolder.getUser() == null){ response.setStatus(401); return false; } // 8.放行 return true; } } --------mvcConfig配置拦截器： package com.hmdp.config; import com.hmdp.utils.LoginInterceptor; import com.hmdp.utils.RefreshTokenInterceptor; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.web.servlet.config.annotation.InterceptorRegistration; import org.springframework.web.servlet.config.annotation.InterceptorRegistry; import org.springframework.web.servlet.config.annotation.WebMvcConfigurer; /** * 配置拦截器 * 可以让LoginInterceptor拦截器生效。 * */ @Configuration public class MvcConfiguration implements WebMvcConfigurer { /** * 添加拦截器 * @param registry */ @Autowired private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) { //添加登录拦截器。 拦截部分请求，优先级低 InterceptorRegistration interceptorRegistration = registry.addInterceptor(new LoginInterceptor()); // 添加配置可以放行哪些路径 interceptorRegistration.excludePathPatterns( "/shop/**", "/voucher/**", "/shop-type/**", "/upload/**", "/blog/hot", "/user/code", "/user/login" ).order(1); // 添加刷新拦截所有请求。 设置优先级高于 拦截部分请求 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns("/**").order(0); } }</content></entry><entry><title>redis的实战项目03_set点赞、Zset点赞排序</title><url>https://codingroam.github.io/post/redis%E7%9A%84%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE03_set%E7%82%B9%E8%B5%9Ezset%E7%82%B9%E8%B5%9E%E6%8E%92%E5%BA%8F/</url><categories><category>Redis</category><category>中间件</category></categories><tags><tag>Redis</tag><tag>中间件</tag></tags><content type="html"> redis的实战项目03_set点赞、Zset点赞排序
redis的实战项目03_set点赞、Zset点赞排序 一、点赞 1、需求： 2、实现步骤描述： 3、代码业务： 二、点赞排行榜 1、需求： 2、实现步骤描述： 3、代码业务： 三、所有的业务层代码：用SortedSet进行实现点赞、排序点赞
在学习redis中的基本数据类型时，只知道增删改查，并体会不到数据类型的优点和缺点。
今天我们通过点赞和点赞排序，体验set和SortedSet的优点。
list：底层是链表，查找时需要根据链表遍历查找，所以o(n)
set：底层是哈希，所以查找速度非常快。
1、需求： 根据博客的id，用户为该博客进行点赞操作。 需求：
同一个用户只能点赞一次，再次点赞则取消点赞 如果当前用户已经点赞，那么点赞按钮高亮显示（前端实现高亮，判断字段Blog类的isLike属性 ） 2、实现步骤描述： 实现方法1 ：直接使用数据库，不加redis缓存 在数据库中建立一张表格，记录Blog的id，以及用户的id，那么点赞以后，在该表中进行记录一次。 当再次点赞时，去数据库中判断是否存在。 缺点：数据库性能不是很好，点赞判断比较多，对数据库中的压力比较大。 实现方法2：用redis做缓存 其实判断用户没有点赞过，其实是记录当前博客被哪些用户点赞过。 key是 博客id，value是点赞过的用户id，value记录哪些用户为该博客id点过赞。 也就是需要一个集合，所有点过赞的id，全部放进去。 当需要再次点赞时，判断该用户id，在集合中是否存在。 一个用户只能点赞一次。那么就是说该集合中用户id不能重复。 对redis的数据类型要求：set集合能满足以下要求： 1.必须是一个集合 2.必须满足元素唯一性。 当用户点击 点赞按钮业务流程：
获取用户的信息 判断该用户是否已经点过赞了。也就是说用户在redis中是否存在 存在，说明已经点过赞了 3. 在数据库中文字id对应的Like字段-1 【点赞数量减一】 3. 从reids中删除用户的信息 【说明该用户取消点赞了】 6. 不存在，说明还没有点赞 6. 在数据库中文字id对应的Like字段 +1 6. 在redis中增加用户的信息 【说明该用户已经点赞了】 3、代码业务： 给Blog类中添加一个isLike字段，表示是否被当前用户点赞 /** * 是否点赞过了 * @TableField(exist = false) ：表示数据库中并没有该字段 */ @TableField(exist = false) private Boolean isLike; 修改点赞功能，利用Redis的Set集合判断是否点赞过【is member有无该用户id】，未点赞过则点赞数+1【添加用户id】，已点赞过的则点赞数-1 @Resource private IUserService userService; @Resource private StringRedisTemplate stringRedisTemplate; /** * 为该文章id，文章点赞功能 * @param id ：文章id * @return */ @Override public Result likeBlog(Long id) { // 1. 获取用户的信息 UserDTO user = UserHolder.getUser(); Long userId = user.getId(); // 2. 判断该用户在redis中是否存在。是否已经点赞了 String key = "bolg:liked:" + id; Boolean member = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); if (BooleanUtil.isTrue(member)){ // 3. 存在说明，已经点赞了 // 3.1 从数据库中的like字段中-1 boolean isSuccess = update().setSql("liked = liked-1").eq("id", id).update(); // 3.2 从redis中删除用户信息 if (isSuccess) { stringRedisTemplate.opsForSet().remove(key,userId.toString()); } }else{ // 4. 不存在，说明还没有点赞 // 4.1 在数据库中的Like字段中+1 boolean isSuccess = update().setSql("liked = liked+1").eq("id", id).update(); // 4.2 在redis中增加用户信息 if (isSuccess) { stringRedisTemplate.opsForSet().add(key, userId.toString()); } } return Result.ok(); } 修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段 /** * 根据id，获取博客 * @param id * @return */ @Override public Result queryBlogById(int id) { // 1. 根据博客id，获取博客 Blog blog = getById(id); if (blog == null){ return Result.fail("博客不存在"); } // 2. 查询blog有关的用户信息 queryBlogUser(blog); // 3. 查看当前用户是否点赞 isBlogLiked(blog); return Result.ok(blog); } private void isBlogLiked(Blog blog) { // 1. 获取当前用户信息 UserDTO user = UserHolder.getUser(); Long userid = user.getId(); // 根据当前用户去redis查询是否已经点赞了 String key = "bolg:liked:" + blog.getId(); Boolean member = stringRedisTemplate.opsForSet().isMember(key, userid.toString()); blog.setIsLike(BooleanUtil.isTrue(member)); } /** * 根据id，获取用户信息 * @param blog */ private void queryBlogUser(Blog blog) { Long userId = blog.getUserId(); User user = userService.getById(userId); blog.setName(user.getNickName()); blog.setIcon(user.getIcon()); } 修改分页查询Blog业务，判断当前登录的用户是否点赞过，赋值给isLike字段。 /** * 分页查询热门文章 * @param current * @return */ @Override public Result queryHotBlog(Integer current) { // 根据用户查询 Page&lt;Blog> page = query() .orderByDesc("liked") .page(new Page&lt;>(current, SystemConstants.MAX_PAGE_SIZE)); // 获取当前页数据 List&lt;Blog> records = page.getRecords(); // 查询用户 records.forEach(blog -> { this.queryBlogUser(blog); this.isBlogLiked(blog); }); // records.forEach(this::queryBlogUser); return Result.ok(records); } 1、需求： 把为该文章点过赞的人显示出来，比如只显示出前五个人 顺序是：根据时间排序，最先点赞的靠前 之前是单独完成点赞业务时，用的是redis中的set集合，因为set可以满足两点： 第一是value元素不唯一性，因为一个用户只允许为一个博客点赞一次。 第二是是一个集合，这样存多个元素。
而如今要实现的是点赞排行榜，那么set不支持排序，那么继续使用set作为点赞就不适合了，那么可以从集合中进行挑选：
list：是集合、支持排序、不支持唯一性、根据底层链表查找比较慢 set：是集合、不支持排序、支持唯一性、底层是hash表，查找速度非常快 SortedSet：是集合、支持排序、支持唯一性。 SortedSet： 添加元素是：ZADD 判断是否存在：Zscore：根据指定元素，获取对应的分数。 查询范围：zrange：从索引开始0到n
127.0.0.1:6379[3]> ZADD key01 888 huyelin 999 zhangsan 123 lisi (integer) 3 127.0.0.1:6379[3]> ZSCORE key01 huyelin "888" 127.0.0.1:6379[3]> ZSCORE key01 zhangsan "999" //查到的话返回的是元素的值 127.0.0.1:6379[3]> ZSCORE key01 jfklsajdf;lkj (nil) // 返回的是空 127.0.0.1:6379[3]> ZRANGE key01 0 2 lisi huyelin zhangsan 2、实现步骤描述： 从redis中获取，为该文章点赞的所有用户id 返回的是set，需要解析成list列表 根据用户id，查询出所有的信息，然后提出部分信息，交给实体类UserDTO传给前端 3、代码业务： /** * 获取点赞列表 * @param id ：文章id * @return */ @Override public Result queryLikes(int id) { // 1. 查询都有哪些用户点赞了。查询出前五条数据： String key = "bolg:liked:" +id; Set&lt;String> top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); //2.解析出其中的用户id List&lt;Long> ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); // 3. 根据用户的id，查询所有的用户信息，放入准备好的实体类【UserDTO】返回前端 String idstr = StrUtil.join(",",ids); // 将ids字符串，进行分割 System.out.println("查看字符串进行分割-------》"+idstr); List&lt;User> userList = userService.query().in("id", ids).last("ORDER BY FIELD(id," + idstr + ")").list(); List&lt;UserDTO> userDTOS = userList.stream() .map(user -> BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(userDTOS); } 其中准备好的实体类，返回前端的数据是：
@Data public class UserDTO { // 用户id private Long id; // 名字 private String nickName; //头像 private String icon; } package com.hmdp.service.impl; import cn.hutool.core.bean.BeanUtil; import cn.hutool.core.util.BooleanUtil; import cn.hutool.core.util.StrUtil; import com.baomidou.mybatisplus.extension.plugins.pagination.Page; import com.hmdp.dto.Result; import com.hmdp.dto.UserDTO; import com.hmdp.entity.Blog; import com.hmdp.entity.User; import com.hmdp.mapper.BlogMapper; import com.hmdp.service.IBlogService; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import com.hmdp.service.IUserService; import com.hmdp.utils.SystemConstants; import com.hmdp.utils.UserHolder; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Service; import javax.annotation.Resource; import java.util.List; import java.util.Set; import java.util.stream.Collectors; /** * &lt;p> * 服务实现类 * &lt;/p> * */ @Service public class BlogServiceImpl extends ServiceImpl&lt;BlogMapper, Blog> implements IBlogService { @Resource private IUserService userService; @Resource private StringRedisTemplate stringRedisTemplate; /** * 根据前端传过来的文字id，该用户为该文章点赞和取消赞的功能 * @param id ：文章id * @return */ @Override public Result likeBlog(Long id) { // 1. 获取用户的信息 UserDTO user = UserHolder.getUser(); Long userId = user.getId(); // 2. 判断该用户在redis中是否存在。是否已经点赞了 String key = "bolg:liked:" + id; // Boolean member = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); // 根据key和value，得到的是分数 Double member = stringRedisTemplate.opsForZSet().score(key,userId.toString()); if (member != null ){ // 3. 如果说分数不为空，说明有分数。说明 已经点赞了 // 3.1 从数据库中的like字段中-1 boolean isSuccess = update().setSql("liked = liked-1").eq("id", id).update(); // 3.2 从redis中删除用户信息 if (isSuccess) { // stringRedisTemplate.opsForSet().remove(key,userId.toString()); // 删除该用户信息 stringRedisTemplate.opsForZSet().remove(key,userId.toString()); } }else{ // 4. 不存在，说明还没有点赞 // 4.1 在数据库中的Like字段中+1 boolean isSuccess = update().setSql("liked = liked+1").eq("id", id).update(); // 4.2 在redis中增加用户信息 if (isSuccess) { stringRedisTemplate.opsForZSet().add(key,userId.toString(),System.currentTimeMillis()); } } return Result.ok(); } /** * 分页查询热门文章 * @param current * @return */ @Override public Result queryHotBlog(Integer current) { // 根据用户查询 Page&lt;Blog> page = query() .orderByDesc("liked") .page(new Page&lt;>(current, SystemConstants.MAX_PAGE_SIZE)); // 获取当前页数据 List&lt;Blog> records = page.getRecords(); // 查询用户 records.forEach(blog -> { //根据id，获取用户信息 this.queryBlogUser(blog); // 查看当前用户是否点赞 this.isBlogLiked(blog); }); // records.forEach(this::queryBlogUser); return Result.ok(records); } /** * 根据博客id，获取博客 * @param id * @return */ @Override public Result queryBlogById(int id) { // 1. 根据博客id，获取博客 Blog blog = getById(id); if (blog == null){ return Result.fail("博客不存在"); } // 2. 查询blog有关的用户信息 queryBlogUser(blog); // 3. 查看当前用户是否点赞 isBlogLiked(blog); return Result.ok(blog); } /** * 根据当前用户查询是否已经点赞 * @param blog */ private void isBlogLiked(Blog blog) { // 1. 获取当前用户信息 UserDTO user = UserHolder.getUser(); // 判断，如果当前用户没有登录，不需要再去查是否点赞，直接返回空即可 if (user ==null){ return; } Long userid = user.getId(); // 根据当前用户去redis查询是否已经点赞了 String key = "bolg:liked:" + blog.getId(); // Boolean member = stringRedisTemplate.opsForSet().isMember(key, userid.toString()); Double member = stringRedisTemplate.opsForZSet().score(key,userid.toString()); if (member != null){ blog.setIsLike(true); }else { blog.setIsLike(false); } } /** * 根据id，获取用户信息 * @param blog */ private void queryBlogUser(Blog blog) { Long userId = blog.getUserId(); User user = userService.getById(userId); blog.setName(user.getNickName()); blog.setIcon(user.getIcon()); } /** * 获取点赞列表 * @param id ：文章id * @return */ @Override public Result queryLikes(int id) { // 1. 查询都有哪些用户点赞了。查询出前五条数据： String key = "bolg:liked:" +id; Set&lt;String> top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); //2.解析出其中的用户id List&lt;Long> ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); // 3. 根据用户的id，查询所有的用户信息，放入准备好的实体类【UserDTO】返回前端 String idstr = StrUtil.join(",",ids); // 将ids字符串，进行分割 System.out.println("查看字符串进行分割-------》"+idstr); List&lt;User> userList = userService.query().in("id", ids).last("ORDER BY FIELD(id," + idstr + ")").list(); List&lt;UserDTO> userDTOS = userList.stream() .map(user -> BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(userDTOS); } }</content></entry><entry><title>Redis实现分布式锁的六种方式。最佳实践：原子性获取锁+LUA脚本释放锁</title><url>https://codingroam.github.io/post/redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%85%AD%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E5%8E%9F%E5%AD%90%E6%80%A7%E8%8E%B7%E5%8F%96%E9%94%81+lua%E8%84%9A%E6%9C%AC%E9%87%8A%E6%94%BE%E9%94%81/</url><categories><category>Redis</category><category>中间件</category></categories><tags><tag>Redis</tag><tag>中间件</tag><tag>项目实战</tag></tags><content type="html"> Redis实现分布式锁的六种方式。最佳实践：原子性获取锁+LUA脚本释放锁
经过反复学习和实践，总体概括一些Redis实现分布式锁迭代版本。
第四版本结合LUA脚本实现释放锁： // 比较线程标示与锁中的标示是否一致 if(redis.call('get', KEYS[1]) == ARGV[1]) then // 释放锁 del key return redis.call('del', KEYS[1]) end return 0` 第五种：Redisson是开源的框架，在redis基础上实现的分布式工具的集合。而分布式锁只是Redisson的一个子集。（下面有详细解释）
lock.tryLock(获取锁等待时间，锁自动释放时间); // 释放锁 lock.unlock(); 第六种：MultiLock联锁 解决的是主从集群一致性问题。（下面有详细解释） 我想说的是：用Redis实现分布式锁有多个方法，而本文介绍了第四种、第五种、第六种。他们能解决不同方面的问题。
而最重要的是：使用最多的是第四种方案，下文中有关于加锁、释放锁的的实际代码。
1、分布式锁的概念 分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。
多进程可见：多个JVM都能看得到。比如redis、mysql等，那么所有的JVM进程都能看得到 互斥锁：只允许一个进程能拿到锁 高可用：大多数情况下，获取锁都能获取成功 高性能：本身加锁后，线程变成了串行执行，从而会影响性能。所以获取锁的步骤上应该高性能 安全性：获取锁应该考虑异常的情况。获取锁后宕机怎么办？ 死锁怎么办 2、mysql、Redis、Zookeeper对比 分布式锁的核心是实现多进程之间互斥，而满足条件的并且常见的有三种：mysql、redis、zookeeper
mysq： 数据库都具有事务的机制，在执行事务操作时，mysql会自动分配一个互斥锁。所以说在事务之间是互斥的，只有一个人能去执行。在业务执行前，去数据库中申请一个互斥锁，然后再去执行业务，当业务执行结束后，提交事务，锁也就释放了。当业务抛出了异常时，会自动触发回滚，锁也就释放了 redis： 利用setnx互斥命令。 获取锁就在reids中setnx一条数据，如果没有该key，那么添加成功，随之获取锁成功，反之一样。当删除该key，就是释放锁成功。 1、最佳实践分布式锁：set key value nx ex 超时释放：在获取锁时加入过期时间。 可以避免服务宕机，然后死锁 127.0.0.1:6379[1]> set lock thread ex 8 nx
OK
127.0.0.1:6379[1]> ttl lock
(integer) 6
127.0.0.1:6379[1]> ttl lock
(integer) 4
127.0.0.1:6379[1]> ttl lock
(integer) 1
127.0.0.1:6379[1]> ttl lock
(integer) -2
释放锁就是删除key： del lock
2、实际开发中：实现redis分布式锁 添加释放锁需要判断是否是当前线程，避免锁误删操作。 添加LUA脚本解决多条命令原子性问题
1.定义接口，利用redis实现分布式锁功能 尝试获取锁：是因为采用的是非阻塞式。获取锁只是获取一次。要么成功要么失败。
public interface ILock { /** * 尝试获取锁 * @param timeoutSec = EX ：锁持有的超时时间，过期后自动释放 * @return true代表获取锁成功; false代表获取锁失败 */ boolean tryLock(long timeoutSec); /** * 释放锁 */ void unlock(); } 2. 实现接口，具体实现获取锁和释放锁 1.在获取锁时存入线程标识（可以用UUID表示）
2.在释放锁时先获取锁中的线程标识，判断是否与当前线程标识一致。从而避免误删别人的锁。
如果一致则释放锁 如果不一致则不释放锁 public class SimpleRedisLock implements ILock { private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } private static final String KEY_PREFIX = "lock:"; private static final String ID_PREFIX = UUID.randomUUID().toString(true) + "-"; private static final DefaultRedisScript&lt;Long> UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript&lt;>(); UNLOCK_SCRIPT.setLocation(new ClassPathResource("unlock.lua")); UNLOCK_SCRIPT.setResultType(Long.class); } @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } @Override public void unlock() { // 调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId()); } /*@Override public void unlock() { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁中的标示 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 判断标示是否一致 if(threadId.equals(id)) { // 释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } }*/ } 3.释放锁的lua脚本 -- 比较线程标示与锁中的标示是否一致 if(redis.call('get', KEYS[1]) == ARGV[1]) then -- 释放锁 del key return redis.call('del', KEYS[1]) end return 0 1、Redisson的介绍 Redisson是开源的框架，在redis基础上实现的分布式工具的集合。而分布式锁只是Redisson的一个子集。
每个Redis服务实例都能管理多达1TB的内存。 Redisson底层采用的是Netty 框架。支持Redis 2.8以上版本，支持Java1.6+以上版本 GitHub地址：https://github.com/redisson/redisson 下面这个图是来自官网，可以实现对锁的功能 1、配置环境 引入依赖 &lt;dependency> &lt;groupId>org.redisson&lt;/groupId> &lt;artifactId>redisson&lt;/artifactId> &lt;version>3.13.6&lt;/version> &lt;/dependency> 配置Redisson客户端： @Configuration public class RedissonConfig { // redis的工厂类，可以从中拿到各种工具 @Bean public RedissonClient redissonClient(){ // 配置类 Config config = new Config(); // 添加redis地址，这里添加了单点的地址，也可以使用config.useClusterServers()添加集群地址 config.useSingleServer().setAddress("redis://192.168.75.111:6379").setPassword("123321"); // 创建RedissonClient对象。创建客户端 return Redisson.create(config); } } 为什么不使用yml文件和start呢？ 添加配置可以使用yml文件，跟springBoot整合来实现，官网还提供了start。 因为会替代spring提供的redis的配置和实现。 建议使用Redisson时，自己进行配置bean，不和spring提供的redis配置进行掺和。
2、测试 @Resource private RedissonClient redissonClient; @Test void testRedisson() throws InterruptedException { // 获取锁（可重入），指定锁的名称 RLock lock = redissonClient.getLock("anyLock"); /* 尝试获取锁，参数分别是： 参数一：获取锁的最大等待时间（期间会重试） 参数二：锁自动释放时间，时间单位 1. 无参模式：非阻塞式 等待时间为-1，就是不等待。如果获取失败立即结束。 自动释放为30秒钟，超时30秒后才会释放 */ boolean isLock = lock.tryLock(1, 10, TimeUnit.SECONDS); // 判断释放获取成功 if(isLock){ try { System.out.println("执行业务"); }finally { // 释放锁 lock.unlock(); } } } 3、可重入锁的原理 1、什么是可重入锁？ 可重入锁：
指的是同一个线程，可以多次获得一把锁。 利用Hash结构记录线程id和重试次数。 利用watchDog延续锁时间。 利用信号量控制锁重试等待。 缺点：redis宕机引起锁失效问题
例如方法A调用方法B，在方法A中先去获得锁，然后执行业务去调用B，而B又要获取同一把锁。
而例如set key value nx time 就是不可重入锁，就会出现死锁的状态。例如：如果A获得锁后，去执行B，B如果也想获得锁，但是A并没有释放锁，所以说就会出现死锁状态。
2、获取锁和释放锁 需要Hash类型
key中记录锁的名称 field记录线程标识 value记录锁的重试次数。 获取锁和释放锁的流程：
创建锁的对象 在方法A中，获取锁，tryLock时记录锁的线程标识和重试次数为1 在方法B中，获取锁。如果是锁已经存在，并且是同一线程时，只需要在重试次数中加1。代表是第二次获取同一个锁。 在方法B或者方法A中，执行完业务，释放锁的逻辑是：需要把重试次数减1，并判断是否为0，如果为0则删除锁。 @SpringBootTest class RedissonTest { @Resource private RedissonClient redissonClient; private RLock lock; @BeforeEach void setUp() { lock = redissonClient.getLock("order"); } @Test void method1() throws InterruptedException { // 尝试获取锁 boolean isLock = lock.tryLock(1L, TimeUnit.SECONDS); if (!isLock) { log.error("获取锁失败 .... 1"); return; } try { log.info("获取锁成功 .... 1"); method2(); log.info("开始执行业务 ... 1"); } finally { log.warn("准备释放锁 .... 1"); lock.unlock(); } } void method2() { // 尝试获取锁 boolean isLock = lock.tryLock(); if (!isLock) { log.error("获取锁失败 .... 2"); return; } try { log.info("获取锁成功 .... 2"); log.info("开始执行业务 ... 2"); } finally { log.warn("准备释放锁 .... 2"); lock.unlock(); } } } 4. 获取锁和释放锁的lua脚本 获取锁和释放锁一定要采用Lua脚本，来确保获取和释放锁的原子性。
获取锁：
local key = KEYS[1]; -- 锁的key local threadId = ARGV[1]; -- 线程唯一标识 local releaseTime = ARGV[2]; -- 锁的自动释放时间 -- 判断是否存在 if(redis.call('exists', key) == 0) then -- 不存在, 获取锁 redis.call('hset', key, threadId, '1'); -- 设置有效期 redis.call('expire', key, releaseTime); return 1; -- 返回结果 end; -- 锁已经存在，判断threadId是否是自己 if(redis.call('hexists', key, threadId) == 1) then -- 不存在, 获取锁，重入次数+1 redis.call('hincrby', key, threadId, '1'); -- 设置有效期 redis.call('expire', key, releaseTime); return 1; -- 返回结果 end; return 0; -- 代码走到这里,说明获取锁的不是自己，获取锁失败 释放锁：
local key = KEYS[1]; -- 锁的key local threadId = ARGV[1]; -- 线程唯一标识 local releaseTime = ARGV[2]; -- 锁的自动释放时间 -- 判断当前锁是否还是被自己持有 if (redis.call('HEXISTS', key, threadId) == 0) then return nil; -- 如果已经不是自己，则直接返回 end; -- 是自己的锁，则重入次数-1 local count = redis.call('HINCRBY', key, threadId, -1); -- 判断是否重入次数是否已经为0 if (count > 0) then -- 大于0说明不能释放锁，重置有效期然后返回 redis.call('EXPIRE', key, releaseTime); return nil; else -- 等于0说明可以释放锁，直接删除 redis.call('DEL', key); return nil; end; 4、 Redisson的锁重试和WatchDog机制 1、什么可重试问题 可重试：利用信号量和PubSub【发布订阅】功能实现等待、唤醒、获取锁失败的重试机制。
第一次尝试获取锁失败以后，并不是立即失败，而是利用了redis的PubSub的机制，做一个等待，等待释放锁的消息。 而获取锁成功的线程，在释放锁中会发送一条释放锁的消息。从而会被正在等待的线程通过订阅机制捕获到。 当等到释放锁的消息后，就会重试机制。
不可重试： 获取锁只尝试一次就返回false。
boolean isLock = lock.tryLock(); tryLock()的参数： long waitTime：获取锁的最大等待时常。当第一次获取锁失败后，不会立即返回false，而是在规定的时间内进行重试，直到超时才会返回false。 long leaseTime：自动失效释放的时间 TimeUnit unit：时间单位 2、深入原码解释： 从获取锁这条命令开始往下执行： boolean isLock = lock.tryLock(1L, TimeUnit.SECONDS);
public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException { long time = unit.toMillis(waitTime); //将传进来的时间，转换为毫秒 long current = System.currentTimeMillis();//得到当前时间 long threadId = Thread.currentThread().getId(); //得到线程的id，就是将来锁的标识 Long ttl = this.tryAcquire(waitTime, leaseTime, unit, threadId); //尝试获取锁。 if (ttl == null) { return true; //如果获取成功，返回true } else { /* *获取失败，就要再次获取： */ time -= System.currentTimeMillis() - current; //判断是否超时重试时间 if (time &lt;= 0L) { this.acquireFailed(waitTime, unit, threadId); return false; } else { current = System.currentTimeMillis(); // 再次得到当前时间 /* 并没有立即去尝试。而是订阅subscribe 其他人释放锁的信号 在释放锁时有这样的语句，用来发布信号：redis.call('piblis',KEY[2],ARGV[1]);" */ RFuture&lt;RedissonLockEntry> subscribeFuture = this.subscribe(threadId); // 当且仅当future在指定的时间限制内完成时为True // 等待time（锁的剩余等待时间），如果等到锁的时间过期， // 还没有等到释放锁的信号，就会返回获取锁失败 if (!subscribeFuture.await(time, TimeUnit.MILLISECONDS)) { if (!subscribeFuture.cancel(false)) { subscribeFuture.onComplete((res, e) -> { if (e == null) { // 等待锁的重试超时时间，就取消订阅 this.unsubscribe(subscribeFuture, threadId); } }); } this.acquireFailed(waitTime, unit, threadId); return false; try { //计算剩余等待时间 time -= System.currentTimeMillis() - current; // 如果剩余等待时间小于0 if (time &lt;= 0L) { this.acquireFailed(waitTime, unit, threadId); boolean var20 = false; return var20; } else { boolean var16; // 如果剩余等待时间大于0 。 进入do while循环 do { //得到当前时间 long currentTime = System.currentTimeMillis(); // 第一次去重试 ttl = this.tryAcquire(waitTime, leaseTime, unit, threadId); if (ttl == null) { var16 = true; return var16; } // 如果获取失败，则看一下剩余时间 time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0L) { this.acquireFailed(waitTime, unit, threadId); var16 = false; return var16; } // 剩余时间如果还有 currentTime = System.currentTimeMillis(); // 采用信号量。在规定时间内，等待得到释放锁的信号量 // 如果ttl小于等待时间：说明在等待时锁就释放了，就等待ttl的时间 // 如果ttl大于等待时间：等待time的时间 if (ttl >= 0L &amp;&amp; ttl &lt; time) { ((RedissonLockEntry)subscribeFuture.getNow()).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); } else { ((RedissonLockEntry)subscribeFuture.getNow()).getLatch().tryAcquire(time, TimeUnit.MILLISECONDS); } time -= System.currentTimeMillis() - currentTime; // 如果等着ttl 到期后，time肯定还没有到期。那么就一直循环while，等待锁的释放信号 } while(time > 0L); this.acquireFailed(waitTime, unit, threadId); var16 = false; return var16; } } finally { this.unsubscribe(subscribeFuture, threadId); } } } } } 2、WatchDog机制 超时释放： 锁超时释放虽然可以避免死锁，但如果业务执行耗时较长，也会导致锁释放，存在安全隐患。
超时续约： 利用watchDog看门狗机制，每隔一段时间（releseTime/3），重置超时时间
private &lt;T> RFuture&lt;Long> tryAcquireAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId) { // if (leaseTime != -1L) { return this.tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } else { /* getConnectionManager：看门狗的时间，默认是30秒，去获取锁 */ RFuture&lt;Long> ttlRemainingFuture = this.tryLockInnerAsync(waitTime, this.commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); // 当future完成以后(剩余有效期，和异常) ttlRemainingFuture.onComplete((ttlRemaining, e) -> { if (e == null) { // 剩余有效期= null， 说明获取锁成功了 if (ttlRemaining == null) { // 任务调度 过期时间 ： 自动续期功能 this.scheduleExpirationRenewal(threadId); } } }); return ttlRemainingFuture; } } this.internalLockLeaseTime / 3L, TimeUnit.MILLISECONDS); 1、什么是主从一致性问题 如果redis提供了主从集群，主从同步存在延迟，假设主服务器宕机后，如果主服务器中的锁，没有同步，导致死锁
主节点：负责增删改
从节点：只负责读的问题
那么主节点会把数据同步到从节点中，但是同步时会存在延迟，即使延迟很短也是会存在。当获取锁后，主从数据还没有来及同步时，主节点宕机了。主备切换后，在新的master节点中，发现锁并不存在了。
2、Redisson如何解决一致性问题【MultiLock联锁】 原理：多个独立的redis节点，必须在所有节点都获取重入锁，才算获取锁成功 优点：所有锁中最安全的实现方法 缺点：运维成本高、实现复杂
既然主从关系是导致一致性问题的原因，那么Redisson取消主从，那么所有的节点都是独立的redisson节点，相互之间没有任何关系，都可以做读写操作。那么获取锁时，依次在多个节点中进行获取锁操作。 可用性问题： 即使某一个节点宕机后，那么其他节点都有锁的信息。 更高的可用性： 在每一个节点后面加入slave节点，做主从同步。
即使加入了主从同步，也不会出现安全问题。
假设某一台master宕机后，刚好并没有完成数据同步。那么slave变成了master主节点。没有锁标识。
有一个线程趁虚而入，想要获取锁，并不能获取成功。因为只有在每一个节点都拿到锁才能获取成功。
只要任意一个节点存活中，其他线程就不能拿到锁，就不会出现锁失效的问题。
优点：保留了主从 机制，确保了整个redis的高可用特性，避免了主从一致引发的锁失效问题。</content></entry><entry><title>批处理if 命令详解</title><url>https://codingroam.github.io/post/%E6%89%B9%E5%A4%84%E7%90%86if-%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</url><categories><category>bat脚本</category></categories><tags><tag>脚本</tag><tag>bat脚本</tag><tag>Learning</tag></tags><content type="html"> 批处理if 命令详解
一、语法规则 1、if命令5钟用法 if "字符串1"=="字符串2" command 语句 #判断两个字符串是否相等 if 数值1 equ 数值2 command 语句 #判断两个数值是否相等 if exist filename command 语句 #判断判断驱动器，文件或文件夹是否存在 if defined 变量 command 语句 #判断变量是否已经定义 if errorlevel 数值 command 语句 #判断上个命令的返回值 2、语法注意事项 如果if后面判断为真后，只处理单句命令的话，直接跟在后面就可以
例如
if exist d:\test.txt (echo D盘下有test.txt存在) else (echo D盘下不存在test.txt) 如果 判断为真后，需要执行多条命令的话，则需要用括号将多条命令扩起来。 注意： 括号“（” 要位于跟if 同一行，如果不在同一行的话执行时会报语法错误。
@echo off echo %1 if "%1"=="ker" ( echo kernel echo kernel2 echo kernel3 ) else ( echo dtbc echo dtbc1 echo dtbc2 echo dtbc3 ) 二、各种示例 1、判断文件是否存在 ##单语句直接可以这样写 if exist d:\test.txt (echo D盘下有test.txt存在) else (echo D盘下不存在test.txt) ##多条语句必须这样写 if exist d:\test.txt ( echo D盘下有test.txt存在 ) else ( echo D盘下不存在test.txt ) 判断文件存不存在，存在就执行，不存在则复制文件到目标再执行。可按需要改动里面的文件名，可为EXE、BAT、CMD、COM等执行文件。
判断+调用其他bat文件
@echo off if exist e:\123.bat goto st copy /y c:\123.bat e:\123.bat goto st :st start e:\123.bat exit 命令中首先判断当前目录中是否存在folder1，如果存在，打印“已经存在文件夹”；如果不存在就用md命令建立文件夹。
@echo off @title 批处理判断文件夹是否存在 if exist folder1 ( echo "已经存在文件夹" ) else ( md folder1 ) if not exist folder2 md folder2 pause 判断字符串是否相等 if "abc"=="xyz" (echo 字符串abc等于字符串xyz) else (echo 字符串abc不等于字符串xyz) 执行后会要求你输入两个字符串，然后批处理判断它俩是否相同。在判断字符串是否相等的时候，if是会区分大小写
@echo off set /p var1=请输入第一个比较的字符： set /p var2=请输入第二个比软的字符： if "%var1%"=="%var2%" (echo 输入的两个字符相同) else echo 输入的两个字符不相同 pause 判断两个数值是否相等 if 1 equ 2 (echo 1等于2) else (echo 1不等于2) 判断变量是否已经定义 if defined str (echo 变量str已经被赋值，其值为%str%) else (echo 变量str的值为空) 不同机器使用cmd拷贝文件，思路是，先建立链接，然后映射网络驱动器，然后就可以拷贝了
net use z: \\192.168.2.112\C$ "wyzwyw" /USER:Administrator copy 1.txt z: 套娃
@echo off @title copy sth to current filepath mode con lines=5 cols=40 SET SourceFile=123.txt SET GenFile1=456.txt if exist %SourceFile% ( if not exist %GenFile1% (copy %SourceFile% %GenFile1%) else (echo %GenFile1% is exist!) ) else ( echo %SourceFile% is not exist! ) echo Success pause</content></entry><entry><title>centos7网卡报错,ip地址丢失不见等问题解决方法</title><url>https://codingroam.github.io/post/centos7%E7%BD%91%E5%8D%A1%E6%8A%A5%E9%94%99ip%E5%9C%B0%E5%9D%80%E4%B8%A2%E5%A4%B1%E4%B8%8D%E8%A7%81%E7%AD%89%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url><categories><category>Linux</category><category>Centos</category></categories><tags><tag>Linux</tag><tag>Problem-Solving</tag></tags><content type="html"> centos7网卡报错,ip地址丢失不见等问题解决方法
问题出现: ip地址丢失不见,网卡报错,当重启网卡时 Job for network.service failed because the control process exited with error code. See &ldquo;systemctl status network.service&rdquo; and &ldquo;journalctl -xe&rdquo; for details. 按照提示输入systemctl status network.service查看报错如下 Failed to start LSB: Bring up/down networking.
解决办法: 系统自带的NetworkManager这个管理套件出错，关掉. 关掉方法: systemctl stop NetworkManager
systemctl disable NetworkManager
重新启动网络： systemctl start network.service</content></entry><entry><title>docker环境搭建redis集群并模拟扩缩容</title><url>https://codingroam.github.io/post/docker%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAredis%E9%9B%86%E7%BE%A4%E5%B9%B6%E6%A8%A1%E6%8B%9F%E6%89%A9%E7%BC%A9%E5%AE%B9/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Redis</tag><tag>Problem-Solving</tag></tags><content type="html"> docker环境搭建redis集群并模拟扩缩容
docker环境搭建redis集群并模拟扩缩容 在docker环境拉下最新的redis镜像
docker pull redis 有了redis镜像，开始搭建redis集群
1关闭防火墙 [root@vm001 redis]# firewall-cmd --zone=public --add-port=6381-6390/tcp --permanent success [root@vm001 redis]# firewall-cmd --reload success 2配置案例设计 master 6381-6383 slave 6384-6386 master1() - slave1() master2() - slave2() master3() - slave3() master-slave实例具体的映射关系，会由redis自己匹配
3 新建6个docker容器实例 # 这里redis 使用的是latest版本，所以最后没有带版本号 docker run -d --name redis-node-1 --net host --privileged=true -v /data/redis/share/redis-node-1:/data redis --cluster-enabled yes --appendonly yes --port 6381 docker run -d --name redis-node-2 --net host --privileged=true -v /data/redis/share/redis-node-2:/data redis --cluster-enabled yes --appendonly yes --port 6382 docker run -d --name redis-node-3 --net host --privileged=true -v /data/redis/share/redis-node-3:/data redis --cluster-enabled yes --appendonly yes --port 6383 docker run -d --name redis-node-4 --net host --privileged=true -v /data/redis/share/redis-node-4:/data redis --cluster-enabled yes --appendonly yes --port 6384 docker run -d --name redis-node-5 --net host --privileged=true -v /data/redis/share/redis-node-5:/data redis --cluster-enabled yes --appendonly yes --port 6385 docker run -d --name redis-node-6 --net host --privileged=true -v /data/redis/share/redis-node-6:/data redis --cluster-enabled yes --appendonly yes --port 6386 4 命令分步解析 docker run -------- 创建并运行docker容器实例 --name redis-node-4 -------- 容器名字 --net host -------- 使用宿主机的IP和端口，默认 --privileged=true -------- 获取宿主机root用户权限 -v /data/redis/share/redis-node-4:/data -------- 容器卷，宿主机地址:docker内部地址 redis -------- redis镜像和版本号，这里redis是latest版本，所以不显示 --cluster-enabled yes -------- 开启redis集群 --appendonly yes -------- 开启持久化 --port 6384 -------- redis端口号 5 进入容器redis-node-1并为6台机器构建集群关系 # 进入容器 docker exec -it redis-node-1 /bin/bash # 构建集群关系 redis-cli --cluster create 192.168.226.128:6381 192.168.226.128:6382 192.168.226.128:6383 192.168.226.128:6384 192.168.226.128:6385 192.168.226.128:6386 --cluster-replicas 1 >>> Performing hash slots allocation on 6 nodes... ## 槽位分配 Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 192.168.226.128:6385 to 192.168.226.128:6381 Adding replica 192.168.226.128:6386 to 192.168.226.128:6382 Adding replica 192.168.226.128:6384 to 192.168.226.128:6383 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master ### M: 指 master(主)， S: 指 slave(从) 三主三从 M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[0-5460] (5461 slots) master M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[5461-10922] (5462 slots) master M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[10923-16383] (5461 slots) master S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 replicates aa89bba715780e6bf30ecbb734580b9da68e394f S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 replicates 94714f899698acb6e3d3584474c8bbbc7677e882 ## 尝试自动分配槽位并请求确认，此时需要打 'yes' Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join . >>> Performing Cluster Check (using node 192.168.226.128:6381) M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... ## 告诉用户所有槽位都得到覆盖 [OK] All 16384 slots covered. 6 以6381作为切入点，查看集群状态 root@vm001:/data# redis-cli -p 6381 ## cluster info 127.0.0.1:6381> cluster info cluster_state:ok cluster_slots_assigned:16384 # 节点分配 cluster_slots_ok:16384 # 节点个数 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 # 集群内部总实例个数 cluster_size:3 cluster_current_epoch:6 # 集群内如当前运行实例个数 cluster_my_epoch:1 cluster_stats_messages_ping_sent:469 cluster_stats_messages_pong_sent:448 cluster_stats_messages_sent:917 cluster_stats_messages_ping_received:443 cluster_stats_messages_pong_received:469 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:917 root@vm001:/data# redis-cli -p 6381 ## cluster nodes 127.0.0.1:6381> cluster nodes # master 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383@16383 master - 0 1651595835135 3 connected 10923-16383 # master aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382@16382 master - 0 1651595837279 2 connected 5461-10922 # slave， 挂载在6383 master机上---根据id可得出 e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385@16385 slave 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 0 1651595832000 3 connected # slave，挂载在6382 master机上---根据id可得出 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384@16384 slave aa89bba715780e6bf30ecbb734580b9da68e394f 0 1651595836216 2 connected # slave，挂载在6381 master机上---根据id可得出 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386@16386 slave 94714f899698acb6e3d3584474c8bbbc7677e882 0 1651595834063 1 connected # master， 当前机器 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381@16381 myself,master - 0 1651595835000 1 connected 0-5460 映射关系：
master1(6381) - slave1(6386) master2(6382) - slave2(6384) master3(6383) - slave3(6385) 7 数据的读写储存 [root@vm001 admin]# docker exec -it redis-node-1 /bin/bash root@vm001:/data# redis-cli -p 6381 127.0.0.1:6381> keys * (empty array) 127.0.0.1:6381> set k1 v1 (error) MOVED 12706 192.168.226.128:6383 127.0.0.1:6381> set k2 v2 OK 127.0.0.1:6381> set k3 v3 OK 127.0.0.1:6381> set k4 v4 (error) MOVED 8455 192.168.226.128:6382 127.0.0.1:6381> exit 上面出现某些key不能设置的原因： 这是一个使用了哈希槽分区算法的redis集群，但是我们连接的是其中的某一台机器，如果设置的key不在它的槽位范围，那么会报错，无法设置值
为了防止路由失效加参数-c并新增两个key
redis-cli -p 6381 -c ## -c: cluster root@vm001:/data# redis-cli -p 6381 -c 127.0.0.1:6381> flushall OK 127.0.0.1:6381> keys * (empty array) ## 如果在6381其能力范围之外，则会跳转到集群中另外的机器6383上，以下同理 127.0.0.1:6381> set k1 v1 -> Redirected to slot [12706] located at 192.168.226.128:6383 OK ## 同上理 192.168.226.128:6383> set k2 v2 -> Redirected to slot [449] located at 192.168.226.128:6381 OK 192.168.226.128:6381> set k3 v3 OK ## 同上理 192.168.226.128:6381> set k4 v4 -> Redirected to slot [8455] located at 192.168.226.128:6382 OK 192.168.226.128:6382> 查看集群信息
redis-cli &ndash;cluster check 192.168.226.128:6381
通过任何一台机器都可以查询，可以看到： &mdash;-a. 目前机器有3主3从，以及相应的映射关系 &mdash;-b. 而且集群目前储存了5个key
root@vm001:/data# redis-cli --cluster check 192.168.226.128:6381 192.168.226.128:6381 (94714f89...) -> 2 keys | 5461 slots | 1 slaves. 192.168.226.128:6382 (aa89bba7...) -> 1 keys | 5462 slots | 1 slaves. 192.168.226.128:6383 (56f6c3cc...) -> 2 keys | 5461 slots | 1 slaves. [OK] 5 keys in 3 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.226.128:6381) M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 8 主从扩容 – 四主四从或更多 8.1 新建6387、6388两个节点 docker run -d --name redis-node-7 --net host --privileged=true -v /data/redis/share/redis-node-7:/data redis --cluster-enabled yes --appendonly yes --port 6387 docker run -d --name redis-node-8 --net host --privileged=true -v /data/redis/share/redis-node-8:/data redis --cluster-enabled yes --appendonly yes --port 6388 docker ps # 结果显示8个节点 8.2 进入6387容器实例内部 docker exec -it redis-node-7 /bin/bash 8.3 将新增的节点(空槽号)6387作为master节点加入集群 # redis-cli --cluster add-node 自己实际IP地址:6387 自己实际IP地址:6381 # 6387 就是将要作为master新增节点 # 6381 就是原来集群节点里面的领路人，相当于6387拜拜6381的码头从而找到组织加入集群 redis-cli --cluster add-node 192.168.226.128:6387 192.168.226.128:6381 8.4 检查集群情况第1次 redis-cli --cluster check 真实ip地址:6381 redis-cli --cluster check 192.168.226.128:6381 ############ 检查集群情况 root@vm001:/data# redis-cli --cluster check 192.168.226.128:6381 192.168.226.128:6381 (94714f89...) -> 2 keys | 5461 slots | 1 slaves. 192.168.226.128:6382 (aa89bba7...) -> 1 keys | 5462 slots | 1 slaves. ############## 新增 redis server 没有分配到槽 192.168.226.128:6387 (cd58f466...) -> 0 keys | 0 slots | 0 slaves. 192.168.226.128:6383 (56f6c3cc...) -> 2 keys | 5461 slots | 1 slaves. [OK] 5 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.226.128:6381) M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 ############ 新增 redis master M: cd58f46691ae515f63739726b0c70018cd765bc7 192.168.226.128:6387 slots: (0 slots) master S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 8.5 重新分派槽号 # redis-cli --cluster reshard IP地址:端口号 redis-cli --cluster reshard 192.168.226.128:6381 How many slots do you want to move (from 1 to 16384)? 4094 //16,376个槽位由4个master节点均分 What is the receiving node ID? cd58f46691ae515f63739726b0c70018cd765bc7//即为8.4查出来的node7 master节点id Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node #1: all 8.6 检查集群情况第2次 # redis-cli --cluster check 真实ip地址:6381 root@vm001:/data# redis-cli --cluster check 192.168.226.128:6381 192.168.226.128:6381 (94714f89...) -> 1 keys | 4096 slots | 1 slaves. 192.168.226.128:6382 (aa89bba7...) -> 1 keys | 4096 slots | 1 slaves. 192.168.226.128:6387 (cd58f466...) -> 1 keys | 4096 slots | 0 slaves. 192.168.226.128:6383 (56f6c3cc...) -> 2 keys | 4096 slots | 1 slaves. [OK] 5 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.226.128:6381) M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 ############### 新增 redis server 分配到槽号了 [0-1364],[5461-6826],[10923-12287] ############### 每个其他节点都分部分槽号给到6387 M: cd58f46691ae515f63739726b0c70018cd765bc7 192.168.226.128:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 8.7 为主节点6387分配从节点6388 # 命令：redis-cli --cluster add-node ip:新slave端口 ip:新master端口 --cluster-slave --cluster-master-id 新主机节点ID redis-cli --cluster add-node 192.168.226.128:6388 192.168.226.128:6387 --cluster-slave --cluster-master-id cd58f46691ae515f63739726b0c70018cd765bc7 ## cd58f46691ae515f63739726b0c70018cd765bc7 这个是6387的编号，按照自己实际情况 root@vm001:/data# redis-cli --cluster add-node 192.168.226.128:6388 192.168.226.128:6387 --cluster-slave --cluster-master-id cd58f46691ae515f63739726b0c70018cd765bc7 >>> Adding node 192.168.226.128:6388 to cluster 192.168.226.128:6387 >>> Performing Cluster Check (using node 192.168.226.128:6387) M: cd58f46691ae515f63739726b0c70018cd765bc7 192.168.226.128:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. >>> Send CLUSTER MEET to node 192.168.226.128:6388 to make it join the cluster. Waiting for the cluster to join >>> Configure node as replica of 192.168.226.128:6387. [OK] New node added correctly. 8.8 检查集群情况第3次 redis-cli --cluster check 192.168.226.128:6381 root@vm001:/data# redis-cli --cluster check 192.168.226.128:6381 192.168.226.128:6381 (94714f89...) -> 1 keys | 4096 slots | 1 slaves. 192.168.226.128:6382 (aa89bba7...) -> 1 keys | 4096 slots | 1 slaves. 192.168.226.128:6387 (cd58f466...) -> 1 keys | 4096 slots | 1 slaves. 192.168.226.128:6383 (56f6c3cc...) -> 2 keys | 4096 slots | 1 slaves. [OK] 5 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.226.128:6381) M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 M: cd58f46691ae515f63739726b0c70018cd765bc7 192.168.226.128:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master 1 additional replica(s) S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 S: 3e67538095c080957dbafbc596b8cb8a5f7a9702 192.168.226.128:6388 slots: (0 slots) slave replicates cd58f46691ae515f63739726b0c70018cd765bc7 S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 9 主从缩容 目的：6387和6388下线
9.1 检查集群情况1获得6388的节点ID redis-cli --cluster check 192.168.226.128:6381 S: 3e67538095c080957dbafbc596b8cb8a5f7a9702 192.168.226.128:6388 slots: (0 slots) slave replicates cd58f46691ae515f63739726b0c70018cd765bc7 9.2 从集群中将4号从节点6388删除（先删从节点） ## redis-cli --cluster del-node 192.168.226.128:6388 3e67538095c080957dbafbc596b8cb8a5f7a9702 root@vm001:/data# redis-cli --cluster del-node 192.168.226.128:6388 3e67538095c080957dbafbc596b8cb8a5f7a9702 >>> Removing node 3e67538095c080957dbafbc596b8cb8a5f7a9702 from cluster 192.168.226.128:6388 >>> Sending CLUSTER FORGET messages to the cluster... >>> Sending CLUSTER RESET SOFT to the deleted node. root@vm001:/data# redis-cli --cluster check 192.168.226.128:6382 ## 结果只有 7 台机器 9.3 将6387的槽号清空，重新分配 本例将清出来的槽号都给6381，4096个槽位都指给6381，它变成了8192个槽位，相当于全部都给6381了，不然要输入3次，一锅端 root@vm001:/data# redis-cli --cluster reshard 192.168.226.128:6381 >>> Performing Cluster Check (using node 192.168.226.128:6381) M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) M: aa89bba715780e6bf30ecbb734580b9da68e394f 192.168.226.128:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) S: 71d1bc72e53985d25570ccd1accea81c7989b008 192.168.226.128:6386 slots: (0 slots) slave replicates 94714f899698acb6e3d3584474c8bbbc7677e882 M: cd58f46691ae515f63739726b0c70018cd765bc7 192.168.226.128:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master S: e4c3904b51cfed5a8f9e6d2a4d83236a906fda33 192.168.226.128:6385 slots: (0 slots) slave replicates 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 S: 8bd2ff973fa218f9022262c0815a9e5bd3c192d4 192.168.226.128:6384 slots: (0 slots) slave replicates aa89bba715780e6bf30ecbb734580b9da68e394f M: 56f6c3cc46dc5fa3eac29ccfa01d755f346fa2c2 192.168.226.128:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. ######## 本次移动 6387 上面所有4096个槽 How many slots do you want to move (from 1 to 16384)? 4096 ######## 移动的槽由 6381 接收 What is the receiving node ID? 94714f899698acb6e3d3584474c8bbbc7677e882 Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. ######## 输入 6387 的ID Source node #1: cd58f46691ae515f63739726b0c70018cd765bc7 Source node #2: done ########## xxxxxxxxxxxxx 前面会刷新很多槽的信息 Do you want to proceed with the proposed reshard plan (yes/no)? yes ########## xxxxxxxxxxxxx 后面会刷新很多槽的信息 9.4 检查集群情况第二次 root@vm001:/data# redis-cli --cluster check 192.168.226.128:6382 # 6381 的槽变成8192 个 M: 94714f899698acb6e3d3584474c8bbbc7677e882 192.168.226.128:6381 slots:[0-6826],[10923-12287] (8192 slots) master 1 additional replica(s) # 6387 的槽变成0 个 M: cd58f46691ae515f63739726b0c70018cd765bc7 192.168.226.128:6387 slots: (0 slots) master 9.5 删除6387 服务 root@vm001:/data# redis-cli --cluster del-node 192.168.226.128:6387 cd58f46691ae515f63739726b0c70018cd765bc7 >>> Removing node cd58f46691ae515f63739726b0c70018cd765bc7 from cluster 192.168.226.128:6387 >>> Sending CLUSTER FORGET messages to the cluster... >>> Sending CLUSTER RESET SOFT to the deleted node. 9.6 检查集群情况第三次 root@vm001:/data# redis-cli --cluster check 192.168.226.128:6382 192.168.226.128:6382 (aa89bba7...) -> 1 keys | 4096 slots | 1 slaves. 192.168.226.128:6381 (94714f89...) -> 2 keys | 8192 slots | 1 slaves. 192.168.226.128:6383 (56f6c3cc...) -> 2 keys | 4096 slots | 1 slaves. [OK] 5 keys in 3 masters. ## 剩下三主三从了 10 命令总结 构建集群关系：
# 构建集群关系 redis-cli --cluster create 192.168.226.128:6381 192.168.226.128:6382 192.168.226.128:6383 192.168.226.128:6384 192.168.226.128:6385 192.168.226.128:6386 --cluster-replicas 1 检查集群情况
# redis-cli --cluster check 真实ip地址:6381 root@vm001:/data# redis-cli --cluster check 192.168.226.128:6381 添加节点
# redis-cli --cluster add-node 自己实际IP地址:6387 自己实际IP地址:6381 # 6387 就是将要作为master新增节点 # 6381 就是原来集群节点里面的领路人，相当于6387拜拜6381的码头从而找到组织加入集群 redis-cli --cluster add-node 192.168.226.128:6387 192.168.226.128:6381 分配槽号
# redis-cli --cluster reshard IP地址:端口号 redis-cli --cluster reshard 192.168.226.128:6381 删除节点
## redis-cli --cluster del-node ip:从机端口 从机6388节点ID redis-cli --cluster del-node 192.168.226.128:6388 3e67538095c080957dbafbc596b8cb8a5f7a9702</content></entry><entry><title>Docker命令大全</title><url>https://codingroam.github.io/post/docker%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag></tags><content type="html"> Docker命令大全</content></entry><entry><title>docker详解(尚硅谷阳哥)</title><url>https://codingroam.github.io/post/docker%E8%AF%A6%E8%A7%A3%E5%B0%9A%E7%A1%85%E8%B0%B7%E9%98%B3%E5%93%A5/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Learning</tag></tags><content type="html"> docker详解(尚硅谷阳哥)
一款产品从开发到上线，一般都会有开发环境，测试环境，运行环境。
如果有一个环境中某个软件或者依赖版本不同了，可能产品就会出现一些错误，甚至无法运行。比如开发人员在windows系统，但是最终要把项目部署到linux。如果存在不支持跨平台的软件，那项目肯定也无法部署成功。
这就产生了开发和运维人员之间的矛盾。开发人员在开发环境将代码跑通，但是到了上线的时候就崩了。还要重新检查操作系统，软件，依赖等版本，这大大降低了效率。造成了搭环境一两天，部署项目两分钟的事件。
docker的出现就能解决以上问题：
开发人员把环境配置好，将需要运行的程序包运行成功，然后把程序包和环境一起打包给运维人员，让运维人员部署就可以了。这大大提高了项目上线的效率。
Docker是基于Go语言实现的云开源项目。
Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或数据库应用等等）及其运行环境能够做到“一次镜像，处处运行”
Linux容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用打成镜像，通过镜像成为运行在Docker容器上面的实例，而 Docker容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机器上就可以一键部署好，大大简化了操作。
docker简介总结：
解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术。docker基于Linux内核，仅包含业务运行所需的runtime环境。
3.1虚拟机 虚拟机是可以在一种操作系统里面运行另一种操作系统，比如在Windows10系统里面运行Linux系统CentOS7。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。
虚拟机的缺点：
启动慢 资源占用多 冗余步骤多 3.2容器虚拟化技术 由于前面虚拟机存在某些缺点，Linux发展出了另一种虚拟化技术：
Linux容器(Linux Containers，缩写为 LXC)
Linux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。
Linux 容器不是模拟一个完整的操作系统而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。
=
3.3两者对比 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程； 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。 一：更快速的应用交付和部署
传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测试验证时间。
二：更便捷的升级和扩缩容
随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成一块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。
三：更简单的系统运维
应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度一致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。
四：更高效的计算资源利用
Docker是内核级虚拟化，其不像传统的虚拟化技术一样需要额外的Hypervisor支持，所以在一台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。
docker借鉴了标准集装箱的概念。标准集装箱是将货物运往世界各地，docker将这个模型运用到自己的设计当中，唯一不同的是：集装箱运送货物，而docker运输软件。
一：镜像（Image）
Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。
二：容器（Container）
Docker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。
它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。
可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。
容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。
三：仓库（Repository）
仓库（Repository）是集中存放镜像文件的场所。仓库(Repository)和仓库注册服务器（Registry）是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。
仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是 Docker Hub(https://hub.docker.com/
)，存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云 等 Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 容器，是一个运行时环境，就是我们前面说到的集装箱。
Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。
Docker运行的基本流程为:
用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。 Docker Daemon作为Docker架构中的主体部分，首先提供Docker Server的功能使其可以接受Docker Client的请求。 Docker Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。 Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动Graph driver将下载镜像以Graph的形式存储。 当需要为Docker创建网络环境时，通过网络管理驱动Network driver创建并配置Docker容器网络环境。 当需要限制Docker容器运行资源或执行用户指令等操作时，则通过Exec driver来完成。 Libcontainer是一项独立的容器管理包，Network driver以及Exec driver都是通过Libcontainer来实现具体对容器进行的操作。 Docker官网
Docker并非是一个通用的容器工具,它依赖于已存在并运行的Linux内核环境。
Docker实质上是在已经运行的Linux下制造了一个隔离的文件环境，因此它执行的效率几乎等同于所部署的Linux主机。
因此，Docker必须部署在Linux内核的系统上。如果其他系统想部署Docker就必须安装一个虚拟 Linux环境。
要求系统为64位、Linux系统内核版本为 3.8以上
查看自己虚拟机的内核：
开始安装：
一：搭建gcc环境（gcc是编程语言译器）
yum -y install gcc yum -y install gcc-c++ 二：安装需要的软件包
yum install -y yum-utils 三：安装镜像仓库
官网上的是
但是因为docker的服务器是在国外，所以有时候从仓库中下载镜像的时候会连接被拒绝或者连接超时的情况，所以可以使用阿里云镜像仓库
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 四：更新yum软件包索引
yum makecache fast 五：安装docker引擎
yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin 六：启动docker
systemctl start docker 查看docker服务
查看docker版本信息
docker version docker run hello-world 为了提高镜像的拉取、发布的速度，可以配置阿里云镜像加速
查看加速器地址
在CentOS下配置镜像加速器
mkdir -p /etc/docker tee /etc/docker/daemon.json &lt;&lt;-'EOF' { "registry-mirrors": ["https://8pfzlx7j.mirror.aliyuncs.com"] } EOF systemctl daemon-reload systemctl restart docker 输出这段提示以后，hello world就会停止运行，容器自动终止。
run干了什么？
(1)docker有着比虚拟机更少的抽象层
由于docker不需要Hypervisor(虚拟机监视器)实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。
(2)docker利用的是宿主机的内核,而不需要加载操作系统OS内核
当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。进而避免引寻、加载操作系统内核返回等比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载OS,返回新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返回过程,因此新建一个docker容器只需要几秒钟。
13.1帮助启动类命令 启动docker： systemctl start docker
停止docker： systemctl stop docker
- 重启docker： systemctl restart docker
- 查看docker状态： systemctl status docker
- 开机启动： systemctl enable docker
- 查看docker概要信息： docker info
- 查看docker总体帮助文档： docker --help
- 查看docker命令帮助文档： docker 具体命令 --help
13.2镜像命令 列出主机上的所有镜像：docker images
各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像的标签版本号 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 OPTIONS说明： -a :列出本地所有的镜像（含历史映像层） -q :只显示镜像ID。 搜索某个镜像： search 镜像名`
各个选项说明: NAME：镜像名称 DESCRIPTION：镜像说明 STARS：点赞数量 OFFICAL：是否是官方的 AUTOMATED：是否是自动构建的 OPTIONS说明： --limit : 只列出N个镜像，默认25个 docker search --limit 5 redis 下载镜像：
docker pull 镜像名 不指定版本，默认是最新版本
docker pull 镜像名 [:TAG] 指定镜像版本
查看镜像/容器/数据卷所占的空间 docker system df 各个选项说明: TYPE：类型（镜像、容器、数据卷） TOTAL：总数 SIZE：大小 RECLAIMABLE：伸缩性 删除镜像
docker rmi 镜像名/镜像ID
强制删除镜像
docker rmi -f 镜像名/镜像ID
删除多个镜像
docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除docker引擎中的全部镜像
docker rmi -f $(docker images -qa)
删除带版本的镜像
查看私有仓库指定镜像所有版本
curl -XGET http://私有仓库主机ip:端口/v2/镜像名称/tags/list
面试题：谈谈docker虚悬镜像是什么？
仓库名、标签都是的镜像，俗称虚悬镜像dangling image
13.3容器命令 docker命令中/bin/bash的作用是：
docker中必须要保持一个进程的运行，要不然整个容器启动后就会马上kill itself，这个/bin/bash就表示启动容器后启动bash。
有镜像才能创建容器， 这是根本前提(下载一个CentOS或者ubuntu镜像演示)
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-2VIyzu0k-1667213295628)(Docker.assets/1658129060820-e3bbad90-56b9-44d7-971b-305557ab2ed9.png)]
新建/启动容器
docker run [OPTIONS] IMAGE [COMMAND] [ARG...]
启动交互式容器
docker run -it IMAGE [COMMAND] [ARG...] 新建指定名字的容器
docker run --name=容器名 IMAGE [COMMAND] [ARG...]
为容器开启守护进程
docker run -d IMAGE [COMMAND] [ARG...]
OPTIONS说明（常用）：有些是一个减号，有些是两个减号 --name="容器新名字"：为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i同时使用；也即启动交互式容器(前台有伪终端，等待交互)； -P: 随机端口映射，大写P -p: 指定端口映射，小写p 启动交互式容器（-it表示启动带命令行交互的伪终端）
新建指定的名字的容器
启动守护式容器(让容器在后台运行)
在大部分的场景下，我们希望 docker 的服务是在后台运行的， 我们可以过 -d 指定容器的后台运行模式
在后台开启一个容器后，再查询当前运行中的容器，发现并没有。
很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程。
容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的。
这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如service nginx start。但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用，这样的容器后台启动后，会立即自杀因为他觉得他没事可做了。所以，最佳的解决方案是将你要运行的程序以前台进程的形式运行，常见就是命令行模式，表示我还有交互操作，别中断.
前台启动redis服务
后台启动redis服务
列出当前正在运行的所有容器：docker ps [OPTIONS]
OPTIONS说明（常用）： -a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 创建一个新的虚拟机实例，查看当前容器的运行情况
退出容器，容器停止：
exit
退出容器，容器不停止
快捷键ctrl+p+q
- 启动已停止运行的容器：docker start 容器ID或者容器名
- 重启容器：docker restart 容器ID或者容器名
- 停止容器：docker stop 容器ID或者容器名
- 强制停止容器：docker kill 容器ID或容器名
- 删除已停止的容器：
docker rm 容器ID
强制删除正在运行的容器
docker rm -f 容器名/容器ID
一次性删除多个容器实例
docker rm -f $(docker ps -a -q)
docker ps -a -q | xargs docker rm
- 查看容器日志：docker logs 容器ID
查看redis的日志信息
查看容器内运行的进程：docker top 容器ID
可以看到redis容器中当前只有一个进程：redis-sercer
查看容器内部的细节：docker inspect 容器ID
进入正在运行的容器并以命令行交互：
方式1（推荐方式）：
docker exec -it 容器ID bashShell
方式2：
docker attach 容器ID
方式1：
方式2：
从容器内的文件拷贝到主机上：docker cp 容器ID:容器路径 主机路径
将容器下/opt目录下的test.txt文件cp到主机上
将容器以压缩包的形式导出到当前路径下：docker export 容器ID > 压缩文件名.tar
从tar包中的内容创建一个新的文件系统再导入为镜像：
cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本
使用刚刚解压的镜像创建一个容器，查看容器中是否有test.txt文件。
14.1镜像的概念 是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。
只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。
14.2分层的镜像 以我们的pull为例，在下载的过程中我们可以看到docker的镜像是在一层一层的下载
14.3镜像的底层原理(联合文件系统) UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。
特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录
14.4镜像加载原理 docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS(联合文件系统)。
bootfs(引导文件系统)主要包含bootloader和kernel(Linux内核),bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是引导文件系统bootfs。这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。
rootfs (root file system) ，在bootfs之上。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。
平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？
对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。
14.5为什么docker镜像使用分层结构 镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。
比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；
同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。
Docker镜像层都是只读的，容器层是可写的。当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。
所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。
Docker中的镜像分层，支持通过扩展现有镜像，创建新的镜像。类似Java继承于一个Base基础类，自己再按需扩展。
新镜像是从 base 镜像一层一层叠加生成的。每安装一个软件，就在现有镜像的基础上增加一层
14.6docker镜像commit操作案例 docker commit 提交容器副本使之成为一个新的镜像：
docker commit -m="提交的描述信息" -a="作者" 容器ID 要创建的目标镜像名:[标签名]
测试：在tomcat安装vim
安装vim
apt-get update apt-get -y install vim 14.7在阿里云推送/拉取镜像 阿里云镜像仓库提供的对应的命令：
测试：
删除本地镜像，从阿里云仓库中拉取
14.8阿里云私有库 Dockerhub、阿里云这样的公共镜像仓库可能不太方便，涉及机密的公司不可能提供镜像给公网，所以需要创建一个本地私人仓库供给团队使用，基于公司内部项目构建镜像。
Docker Registry是官方提供的工具，可以用于构建私有镜像仓库
14.9创建私有仓库 一：下载镜像docker registry
docker pull registry 二：运行私有库Registry
docker run -d -p 5000:5000 -v /docker-xha/tomcat-vim/:/opt/registry --privileged=true registry 三：创建一个新镜像，tomcat安装ifconfig命令
docker commit -m="提交的描述信息" -a="作者" 容器ID 要创建的目标镜像名:[标签名] docker commit -m="ifconfig cmd add" -a="xha" 749a08193a53 tomcat-ifconfig:1.1 已经提交的镜像：
四：验证提交的镜像能够使用ifconfig
五：查看私服库上有什么镜像
curl -XGET http://主机ip:5000/v2/_catalog 当前私服库上没有任何镜像
六：将新镜像修改符合私服规范的Tag
docker tag tomcat-ifconfig:1.1 192.168.26.135:5000/tomcat-ifconfig:1.1 七：修改进程守护配置文件，设置docker允许推动镜像
"insecure-registries": ["192.168.26.135:5000"] 重启docker服务
八：重启私有仓库
docker run -d -p 5000:5000 -v /命名空间/仓库名称/:/opt/registry --privileged=true registry 九：将镜像推送到私服库
十：查看当前私服库中的镜像
curl -XGET http://主机ip:5000/v2/_catalog 十一：拉取私服库中的镜像
docker pull 主机ip:端口号/镜像名:版本号 查看docker中的所有镜像
利用此镜像新建容器，并测试ifconfig命令
15.1容器数据卷概述 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性：数据卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷。
数据卷会将docker容器内的数据保存进宿主机的磁盘中，运行一个带有容器卷存储功能的容器实例。
15.2容器数据卷的特点 数据卷可在容器之间共享或重用数据 容器和宿主机之间数据共享 卷中的更改可以直接实时生效 数据卷中的更改不会包含在镜像的更新中 数据卷的生命周期一直持续到没有容器使用它为止 15.3为容器添加数据卷 docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 选项解释：
查看数据卷是否挂载成功：
dokcer inspect 容器id 容器和宿主机之间数据共享：
在容器中编辑文件：
在宿主机的同一路径下查看文件：
在宿主机中编辑文件：
查看容器中查看文件：
实现了容器和宿主机之间的数据共享
即使容器停止了，在宿主机操作数据卷，等到容器重新启动了也能实现数据共享
15.4容器卷ro和rw的读写规则 一：ro即read only，容器只能读
docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro --name:容器名 镜像名 创建的实例：
使用主机进行文件的修改和读取，并在容器中进行读写操作。
15.5容器卷之间的继承 第一个容器1完成和宿主机的映射
容器2继承容器1的卷规则
docker run -it --privileged=true --volumes-from 父类 --name:容器名 镜像名 16.1Tomcat 说明：由于较新版的tomcat需要删除/usr/local/tomcat目录下的webapps文件，并将webapps.dist重命名为webapp。所以推荐下载旧版的tomcat。
步骤：
docker hub上面查找tomcat镜像（这里采用8.0.53版本） 从docker hub上拉取tomcat镜像到本地 docker images查看是否有拉取到的tomcat 使用tomcat镜像创建容器实例 报错就重启一下docker服务
docker run -d -p 8080:8080 --name t1 tomcat:8.0.53 tomcat服务启动成功
16.2MySQL 一：简单版本
docker hub上面查找mysql镜像(这里采用8.0.19版本) 使用mysql8.0.19镜像创建容器(也叫运行镜像) docker run -p 主机端口号:容器端口 -e MYSQL_ROOT_PASSWORD=密码 -d mysql:版本 进入mysql 4.测试：查看数据库，创建数据库
6.查看docker容器下mysql的字符集编码
show variables like 'character%' 5.在本机采用可视化工具连接测试
二：实战版本
创建mysql容器实例 为了防止数据丢失和误删问题，采用数据卷的形式实现数据备份：
docker run -d -p 3306:3306 --privileged=true \ -v /opt/mysql/log:/var/log/mysql \ -v /opt/mysql/data:/var/lib/mysql \ -v /opt/mysql/conf:/etc/mysql/conf.d \ -e MYSQL_ROOT_PASSWORD=xu.123456 \ --name mysql \ mysql:8.0.19 在主机的配置文件目录下新建mysql的配置文件my.cnf 设置mysql字符编码为utf-8
[client] default_character_set=utf8 [mysqld] collation_server = utf8_general_ci character_set_server = utf8 重启mysql容器实例 docker restart 容器ID 重新启动容器，重新查看mysql的字符集编码 show variables like &lsquo;character%&rsquo; 16.3Redis 拉取redis镜像 获取redis的配置文件 redis配置文件官网 Redis configuration | Redis
创建存放redis配置文件的目录，创建redis.conf文件，写入配置信息 修改配置文件内容 1. 添加redis密码（requirepass） 1. 修改bind为0.0.0.0（任何机器都能够访问） 1. 为了避免和docker中的-d参数冲突，将后台启动设置为no（daemonize no） 1. 关闭保护模式(protected-mode no) 1. 开启AOF持久化(appendonly yes) 7. 使用redis镜像创建容器实例 docker run -d -p 6379:6379 \ --name redis --privileged=true \ -v /opt/redis/redis.conf:/etc/redis/redis.conf \ -v /opt/redis/data:/data \ redis:6.0.8 8. 进入容器，并启动redis客户端 退出容器，redis容器依旧运行。 首先安装好MySQL镜像 创建主节点MySQL实例对象，主机端口号为3307，容器内端口号为3306 docker run -p 3307:3306 \ --name mysql-master \ -v /opt/mysql-master/log:/var/log/mysql \ -v /opt/mysql-master/data:/var/lib/mysql \ -v /opt/mysql-master/conf:/etc/mysql/conf.d \ -e MYSQL_ROOT_PASSWORD=root \ -d mysql:8.0.19 进入/opt/mysql-master/conf目录下新建my.cnf [mysqld] ## 设置server_id，同一局域网中需要唯一 server_id=101 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能 log-bin=mall-mysql-bin ## 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 修改完配置后重启master容器实例 docker restart mysql-master 进入主节点mysql-master容器实例，进入数据库 docker exec -it 主节点容器实例 /bin/bash mysql -u root -proot master容器实例内创建数据同步用户并授予权限 CREATE USER &lsquo;slave&rsquo;@&rsquo;%&rsquo; IDENTIFIED BY &lsquo;123456&rsquo;; ALTER USER &lsquo;slave&rsquo;@&rsquo;%&rsquo; IDENTIFIED WITH mysql_native_password BY &lsquo;123456&rsquo;; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON . TO &lsquo;slave&rsquo;@&rsquo;%&rsquo;; 创建从节点MySQL容器实例对象，主机端口号为3308，容器内端口号为3306 docker run -p 3308:3306 \ --name mysql-slave \ -v /opt/mysql-slave/log:/var/log/mysql \ -v /opt/mysql-slave/data:/var/lib/mysql \ -v /opt/mysql-slave/conf:/etc/mysql/conf.d \ -e MYSQL_ROOT_PASSWORD=root \ -d mysql:8.0.19 进入/opt/mysql-slave/conf目录下新建my.cnf [mysqld] ## 设置server_id，同一局域网中需要唯一 server_id=102 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用 log-bin=mall-mysql-slave1-bin ## 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=mall-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## slave设置为只读（具有super权限的用户除外） read_only=1 修改完配置后重启slave实例 docker restart mysql-slave 在主节点数据库查看主从同步状态 show master status; 获取到File和Position
进入从节点，配置主从复制 change master to master_host='宿主机ip', master_user='slave', master_password='123456', master_port=3307, master_log_file='mall-mysql-bin.000001', master_log_pos=1030, master_connect_retry=30; 主从复制参数说明：
master_host：主数据库的IP地址；
在从节点数据库中查看主从同步状态 show slave status \G; 在从节点数据库开启主从同步 start slave; 在从节点数据库再次查看主从同步状态是否变为Yes show slave status \G; 主从复制测试 主节点新建数据库，创建表，插入数据，然后在从节点中进行查看。
从节点数据库查询查看：
18.1分布式储存—哈希取余算法 hash(key) % N个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。
优点：
简单粗暴，直接有效，只需要预估好数据规划好节点，例如3台、8台、10台，就能保证一段时间的数据支撑。使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。
缺点：
原来规划好的节点，进行扩容或者缩容就比较困难，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3会变成Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。
18.2分布式储存—一致性哈希算法 一致性哈希算法为了解决分布式缓存数据变动和映射问题。目的是当服务器个数发生变动时， 尽量减少影响客户端到服务器的映射关系
算法构建一致性哈希环
为了在节点数目发生改变时尽可能少的迁移数据，将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash后会顺时针找到临近的存储节点存放。而当有节点加入或退出时仅影响该节点在Hash环上顺时针相邻的后续节点。
优点
加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。
缺点
数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。
1. ![img](https://img-blog.csdnimg.cn/img_convert/f88c8deab38c0aded621f23ffe3a6d78.png) 18.3分布式储存—哈希槽算法 哈希槽(散列插槽)实质就是一个数组，数组[0,2^14 -1]形成hash slot空间。解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。
一共有多少个hash槽
一个集群只能有16384个槽，编号0-16383（0-2^14-1）。这些槽会分配给集群中的所有主节点，分配策略没有要求。可以指定哪些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要对key求哈希值，然后对16384取余，余数是几key就落入对应的槽里。slot = CRC16(key) % 16384。以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。
哈希槽计算
Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。
redis cluster采用数据分片的哈希槽来进行数据存储和数据的读取。redis cluster一共有2^14（16384）个槽，所有的master节点都会有一个槽区比如0～1000，槽数是可以迁移的。master节点的slave节点不分配槽，只拥有读权限。但是注意在代码中redis cluster执行读写操作的都是master节点，并不是读是从节点，写是主节点。
为什么是16384个槽？ 在握手成功后，两个节点之间会定期发送ping/pong消息，交换数据信息，在redis节点发送心跳包时需要把所有的槽信息放到这个心跳包里，以便让节点知道当前集群信息，在发送心跳包时使用char进行bitmap压缩后是2k（16384÷8÷1024=2kb），也就是说使用2k的空间创建了16k的槽数。 虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k（8 * 8 (8 bit) * 1024(1k) = 8K），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。
如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。 redis的集群主节点数量基本不可能超过1000个。集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。 槽位越小，节点少的情况下，压缩率高 18.4redis分片集群扩缩容配置案例架构 18.4.1分片集群搭建 关闭防火墙，启动docker后台服务 新建6个redis镜像的容器实例形成6个redis主从节点 docker run -d \ --name redis-node-1 \ --net host --privileged=true \ -v /data/redis/share/redis-node-1:/data redis:6.0.8 \ --cluster-enabled yes \ --appendonly yes \ --port 6381 命令选项详解：
进入容器redis-node-1并为6台机器构建集群关系 redis-cli --cluster create --cluster-replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003 命令选项详解：
三主三从，由于一个redis集群只有16384个插槽
所以主节点6381的插槽范围是[0-5460]
所以主节点6381的插槽范围是[5461-10922]
所以主节点6381的插槽范围是[10923-16384]
查看集群状态 采用命令查看帮助手册
redis-cli --cluster help 查看集群状态：
cluster nodes 6385是从节点，主节点是0e，即6383.
6386是从节点，主节点是aa，即6381.
6384是从节点，主节点是5b，即6382.
18.4.2主节点哈希槽范围说明 因为当存入k-v键值对时，redis会对key进行crc16算法得出一个值，然后对16384取余，得出的结果位于哪个主节点的哈希槽范围中就存入哪个主节点。
redis-cli -p 主节点端口 所以在连接redis集群的时候要添加参数“-c”表示以集群的形式连接redis，这样能实现在主节点之间实现切换。
redis-cli -c -p 主节点端口 查看当前集群状态
redis-cli --cluster check 主机ip:容器中redis端口 18.4.3主从容错 当主节点宕机之后，它的从节点会转换为主节点。
测试，停止主节点6381，查看其和它的从节点6386的状态
6381连接失败，其从节点6386变为主节点。
恢复之前的主节点6381，查看其和主节点6386的状态
6381变为从节点，6386还是主节点。
18.4.4主从扩容 在原来的3主3从得基础上扩容到4主4从
根据redis镜像新建两个容器实例，对应的端口号分别是6387、6388 docker run -d --name redis-node-1 --net host --privileged=true -v /data/redis/share/redis-node-1:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6381 将新增的6387节点作为master节点添加到redis集群中 查看帮助
redis-cli --cluster help 将6387节点作为主节点添加到集群当中
redis-cli --cluster add-node new_host:new_port existing_host:existing_port 查看集群状态 可以看到新加入的主节点 并没有分配哈希槽位
重新分配哈希槽位 再次查看集群状态 可以发现为新节点分配的哈希槽位并不是连续的，其哈希槽位是由原来的每一个主节点分配的。 添加新节点6388作为主节点6387的从节点 redis-cli --cluster add-node 192.168.26.135:6388 192.168.26.135:6387 --cluster-slave --cluster-master-id ee499e33ad50ffaf84d4ed95a39625c49dc1ae4b 18.4.5主从缩容 使6387和6388主从节点下线
查看6387和6388的节点ID 将6388从节点从主节点上删除 将主节点的哈希槽位清空并重新分配槽位给master1 删除空槽位的主节点 redis-cli --cluster del-node 192.168.26.135:6387 ee499e33ad50ffaf84d4ed95a39625c49dc1ae4b 19.1Dockerfile简介 Dockerfile reference | Docker Documentation
Dockerfile是用来构建Docker镜像的文本文件，是由一条条构建镜像所需的指令和参数构成的脚本
构建Docker镜像的步骤：
编写Dockerfile文件 docker build 命令构建镜像 docker run 镜像 运行容器实例 19.2Dockerfile内容基础知识 每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 #表示注释 每条指令都会创建一个新的镜像层并对镜像进行提交 19.3docker执行Dockerfile的大致流程 docker从基础镜像运行一个容器 执行一条指令并对容器作出修改 执行类似docker commit的操作提交一个新的镜像层 docker再基于刚提交的镜像运行一个新容器 执行dockerfile中的下一条指令直到所有指令都执行完成 19.4docker镜像、docker容器、Dockerfile 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段，
Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件镜像的运行态，即依照镜像运行的容器实例 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。
Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时会真正开始提供服务; Docker容器，容器是直接提供服务的。 19.5DockerFile常用保留字指令 FROM
基础镜像，当前新镜像是基于哪个镜像的，指定一个已经存在的镜像作为模板，第一条必须是from
MAINTAINER
镜像维护者的姓名和邮箱地址
RUN
容器构建时需要运行的命令
EXPOSE
当前容器对外暴露出的端口
WORKDIR
指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点
USER
指定该镜像以什么样的用户去执行，如果都不指定，默认是root
ENV
用来在构建镜像过程中设置环境变量
这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样；
也可以在其它指令中直接使用这些环境变量。
比如：WORKDIR $MY_PATH
ADD
将宿主机目录下的文件拷贝进镜像且会自动处理URL和解压tar压缩包
COPY
类似ADD，拷贝文件和目录到镜像中。
将从构建上下文目录中 &lt;源路径> 的文件/目录复制到新的一层的镜像内的 &lt;目标路径> 位置
VOLUME
容器数据卷，用于数据保存和持久化工作
CMD
指定容器启动后的要干的事情
ENTRYPOINT
也是用来指定一个容器启动时要运行的命令
类似于 CMD 指令，但是ENTRYPOINT不会被docker run后面的命令覆盖， 而且这些命令行参数会被当作 参数送给 ENTRYPOINT 指令指定的程序。
19.6使用Dockerfile实现自定义镜像 自定义镜像需求：具备vim+ifconfig+jdk8环境
创建文件夹myfile，并将jdk压缩包传到当前目录下 创建Dockerfile文件，编辑内容 FROM centos:7 MAINTAINER xha&lt;2533694604@qq.com> ENV MYPATH /usr/local WORKDIR $MYPATH #安装vim编辑器 RUN yum -y install vim #安装ifconfig命令查看网络IP RUN yum -y install net-tools #安装java8及lib库 RUN yum -y install glibc.i686 RUN mkdir /usr/local/java #ADD 是相对路径jar,把jdk-8u341-linux-x64.tar.gz添加到容器中,安装包必须要和Dockerfile文件在同一位置 ADD jdk-8u341-linux-x64.tar.gz /usr/local/java/ #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_341 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATH EXPOSE 80 CMD echo $MYPATH CMD echo "success--------------ok" CMD /bin/bash 注意：centos版本指定为7 centos:7
构建镜像 docker build -t centosjava8:1.0 . 构建完成：
查看创建的镜像
运行镜像 docker run -it 镜像ID /bin/bash 19.7虚悬镜像 仓库名和版本号都为的就是虚悬镜像
查看所有的虚悬镜像
docker image ls -f dangling=true 删除所有虚悬镜像
docker image prune 20.1将jar包部署到docker容器上 创建新目录，将jar包放在该目录下 编写Dockerfile文件 #基础镜像使用java FROM java:8 # 作者 MAINTAINER xha # VOLUME 指定临时文件目录为/tmp，在主机/var/lib/docker目录下创建了一个临时文件并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名 ADD docker_test-0.0.1-SNAPSHOT.jar dockertest.jar # 运行jar包 RUN bash -c 'touch /dockertest.jar' ENTRYPOINT ["java","-jar","/dockertest.jar"] #暴露8090端口作为微服务 EXPOSE 8090 构建镜像 在当前目录下构建镜像
docker build -t dockertest:1.0 . 镜像构建成功
运行镜像 docker run -d -p 8090:8090 镜像ID 测试项目接口 21.1docker网络是什么 Docker启动时会在主机上自动创建一个docker0网桥，即一个Linux网桥。容器借助网桥和主机或者其他容器进行通讯。
21.2docker启动与不启动时的网络情况 一：不启动docker时
ens33是宿主机ip 可以发现是在同一网段
lo是回环链路网络 virbr0 当选择虚拟化相关的服务后，启动网卡时就会有一个网桥连接的私网地址virbr0网卡（有一个固定的ip地址192.168.122.1），是做虚拟网桥使用的，其作用是连接虚拟机上的虚拟网卡，提供NAT访问外网的功能。
二：启动docker时
会产生一个名为docker0的虚拟网桥
21.3docker网络的相关命令 查看docker网络的相关命令
docker network --help 21.4docker网络的作用 容器间的互联和通信以及端口映射 容器IP变动时候可以通过服务名直接网络通信而不受到影响 21.5docker网络模式 21.5.1docker网络模式分类 21.5.2bridge网络模式 Docker 服务默认会创建一个 docker0 网桥（其上有一个 docker0 内部接口），该桥接网络的名称为docker0，它在内核层连通了其他的物理或虚拟网卡，这就将所有容器和本地主机都放到同一个物理网络。Docker 默认指定了 docker0 接口 的 IP 地址和子网掩码，让主机和容器之间。宿主机和容器之间、容器与容器之间可以通过网桥docer0(bridge)相互通信
查看bridge网桥
docker network inspect bridge 使用tomcat镜像创建两个容器，宿主机端口分别是8081和8082，容器端口都是8080
进入容器t1，查看网络模式
162：eth0@if163 有eth0
进入容器t2，查看网络模式
164：eth0@if165 有eth0
在宿主机查看网络模式
163：veth@if162 有veth
165：veth@if164 有veth
综上可以看出，容器内都有一个网络模式和网桥进行相连，实现和宿主机两两匹配验证。
21.5.3host网络模式 容器将不会获得一个独立的Network Namespace， 而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡而是使用宿主机的IP和端口。
在之前搭建redis分片集群的时候，各个redis主从节点使用的网络模式就是host
可以发现各个主从节点并没有对应的端口，即指定端口并没有意义，而是和宿主机公用ip和端口号。
使用host网络模式创建tomcat镜像
docker run -d -p 8083:8080 --network=host --name t3 tomcat:8.0.53 查看容器是否有端口
查看容器的网络模式，可以发现没有ip
查看宿主机和容器的ip信息，可以发现是一模一样的，即容器使用的是宿主机的ip和端口。
21.5.4none网络模式 在none模式下，并不为Docker容器进行任何网络配置。 也就是说，这个Docker容器没有网卡、IP、路由等信息，只有一个lo需要我们自己为Docker容器添加网卡、配置IP等。
21.5.5container网络模式 新建的容器和已经存在的一个容器共享一个网络ip配置而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。
测试：
创建一个tomcat容器，再创建第二个tomcat容器，创建第二个容器的时候指定使用第一个tomcat容器的网络配置。
docker run -d -p 8084:8080 --name t4 tomcat:8.0.19 docker run -d -p 8085:8080 --network container:t4 --name t5 tomcat:8.0.53 但是会提示端口号冲突，因为两台tomcat公用8080端口。
21.5.6自定义网络模式 一：使用刚刚创建的两个tomcat镜像容器实例t1和t2，在容器内部首先进行ping对方的ip
发现可以ping通
二：但是当按照服务名ping时发现没有ping通
三：自定义网络模式
docker多个容器之间的集群规划要使用服务名，因为ip是会变动，使用自定义网络模式能够使用服务名进行通信
创建网络，默认模式是bridge（网桥）模式 docker network create 网络名 创建两个容器实例，并使用自定义的网络模式 docker run -d -p 8081:8080 --network myselfnet --name t1 tomcat:8.0.19 测试使用服务名ping
21.6docker网络底层ip和容器的映射变化 docker容器内部的ip是有可能是会发生变化的
使用同一镜像创建两个容器实例，并查看网络信息
docker inspect 容器ID 第一个容器
第二个容器
删除第二个容器，再创建一个容器，查看ip
可以发现当第二个容器被停止之后，创建的第三个容器的ip与第二个相同。
22.1Docker-compose的定义 Docker-Compose就是容器编排，负责实现对Docker容器集群的快速编排。
Docker-Compose可以管理多个 Docker 容器组成一个应用。你需要定义一个 YAML 格式的配置文件docker-compose.yml，写好多个容器之间的调用关系。然后，只要一个命令，就能同时启动/关闭这些容器
22.2Docker-Compose的作用 ==docker建议我们每一个容器中只运行一个服务,因为docker容器本身占用资源极少,所以最好是将每个服务单独的分割开来。==但是这样我们又面临了一个问题？
如果我需要同时部署好多个服务,难道要每个服务单独写Dockerfile然后在构建镜像,构建容器,这样工作量会和大。所以docker官方给我们提供了docker-compose多服务部署的工具
例如要实现一个Web微服务项目，除了Web服务容器本身，往往还需要再加上后端的数据库mysql服务容器，redis服务器，注册中心eureka，甚至还包括负载均衡容器等等。
Compose允许用户通过一个单独的docker-compose.yml模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。
可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成构建。Docker-Compose 解决了容器与容器之间如何管理编排的问题。
22.3Docker-Compose的安装 Docker-Compose官网： Docker Desktop | Docker Documentation
安装命令：
DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker} mkdir -p $DOCKER_CONFIG/cli-plugins curl -SL https://github.com/docker/compose/releases/download/v2.11.2/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose 打印版本号，查看是否成功
docker compose version 22.4Docker-Compose的常用命令 22.5Docker-Compose核心概念 22.6Docker-Compose的使用步骤 编写Dockerfile定义各个微服务应用并构建出对应的镜像文件 使用 docker-compose.yml 定义一个完整业务单元，安排好整体应用中的各个容器服务。 最后，执行docker-compose up命令 来启动并运行整个应用程序，完成一键部署上线 22.7不使用Compose编排存在的问题 不使用Compose部署项目的方式是分别启动mysql服务、redis服务和微服务镜像文件，部署完成。 存在的问题 先后顺序要求固定，先mysql+redis才能微服务访问成功 多个run命令… 容器间的启停或宕机，有可能导致IP地址对应的容器实例变化，映射出错， 要么生产IP写死(可以但是不推荐)，要么通过服务调用 22.7使用Compose编排微服务 编写docker-compose.yml文件 在vim模式下 :set paste粘贴的文本数据不会乱 #compose版本 version: &ldquo;3&rdquo; #微服务项目 services: microService: #微服务镜像
image: zzyy_docker:1.6 container_name: ms01 ports: - &ldquo;6001:6001&rdquo; #数据卷 volumes: - /app/microService:/data networks: - atguigu_net depends_on: - redis - mysql
#redis服务 redis: image: redis:6.0.8 ports: - &ldquo;6379:6379&rdquo; volumes: - /app/redis/redis.conf:/etc/redis/redis.conf - /app/redis/data:/data networks: - atguigu_net command: redis-server /etc/redis/redis.conf
#mysql服务 mysql: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: &lsquo;123456&rsquo; MYSQL_ALLOW_EMPTY_PASSWORD: &rsquo;no&rsquo; MYSQL_DATABASE: &lsquo;db2021&rsquo; MYSQL_USER: &lsquo;zzyy&rsquo; MYSQL_PASSWORD: &lsquo;zzyy123&rsquo; ports: - &ldquo;3306:3306&rdquo; volumes: - /app/mysql/db:/var/lib/mysql - /app/mysql/conf/my.cnf:/etc/my.cnf - /app/mysql/init:/docker-entrypoint-initdb.d networks: - atguigu_net command: &ndash;default-authentication-plugin=mysql_native_password #解决外部无法访问
#创建自定义网络 networks: atguigu_net:
2. 修改微服务项目，将映射的ip修改为服务名
将SpringBoot项目的配置文件中mysql和redis的ip更改为项目名。之后打包成jar包，在docker中编写Dockerfile文件，构建镜像。
检查当前目录下compose.yml文件是否有语法错误 docker compose config -q 启动所有docker-compose服务并后台运行 docker compose up -d 23.1Protainer是什么 Portainer 是一款轻量级的应用，它提供了图形化界面，用于方便地管理Docker环境，包括单机环境和集群环境。
23.2Protainer安装 Docker and Kubernetes Management | Portainer
拉取portainer docker pull portainer/portainer 根据portainer镜像创建容器实例 restart=always表示即使重启docker服务，Protainer容器依旧存在。
docker run -d -p 9000:9000 --name portainer \ --restart=always \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /www/portainer/data:/data \ portainer/portainer 查看容器实例
访问宿主机ip加protainer端口号 输入密码
可以查看到镜像、容器、网络等等。
23.3在Protainer中安装Nginx 添加nginx容器 nginx镜像拉取成功，nginx容器创建成功 访问80端口查看nginx服务是否正常启动 24.1原生命令监控容器的详细信息 docker status 问题：
通过docker stats命令可以很方便的看到当前宿主机上所有容器的CPU,内存以及网络流量等数据，一般小公司够用了。
但是，docker stats统计结果只能是当前宿主机的全部容器，数据资料是实时的，没有地方存储、没有健康指标过线预警等功能
24.2CIG监控 CIG分别表示：CAdvisor监控收集+InfluxDB存储数据+Granfana展示图表
CAdvisor是一个容器资源监控工具包括容器的内存,CPU,网络IO,磁盘I0等监控。 InfluxDB是用Go语言编写的一个开源分布式时序、 事件和指标数据库,无需外部依赖。 Grafana是一个开源的数据监控分析可视化平台，支持多种数据源配置(支持的数据源包括 24.3使用Compose搭建CIG监控平台 新建cig目录，在目录下创建docker-compose.yml文件，实现容器编排。 version: '3.1' volumes: grafana_data: {} services: influxdb: image: tutum/influxdb:0.9 restart: always environment: - PRE_CREATE_DB=cadvisor ports: - "8083:8083" - "8086:8086" volumes: - ./data/influxdb:/data cadvisor: image: google/cadvisor links: - influxdb:influxsrv command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 restart: always ports: - "8080:8080" volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro grafana: user: "104" image: grafana/grafana user: "104" restart: always links: - influxdb:influxsrv ports: - "3000:3000" volumes: - grafana_data:/var/lib/grafana environment: - HTTP_USER=admin - HTTP_PASS=admin - INFLUXDB_HOST=influxsrv - INFLUXDB_PORT=8086 - INFLUXDB_NAME=cadvisor - INFLUXDB_USER=root - INFLUXDB_PASS=root 检查docker-compose.yml文件是否有错误 docker-compose config -q 后台启动docker-compose服务 docker-compose up -d</content></entry><entry><title>Docker Redis Connection refused解决方法</title><url>https://codingroam.github.io/post/docker-redis-connection-refused%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Problem-Solving</tag></tags><content type="html"> Docker Redis Connection refused解决方法
使用docker-compose部署了dnmp+redis环境，但是php在使用127.0.0.1连接redis时却报Connection refused 的错误，后来在stack overflow上找到了解决方法。
原贴地址： https://stackoverflow.com/questions/42360356/docker-redis-connection-refused/42361204
简单来说，容器之间相互隔绝，在进行了端口映射之后，宿主机可以通过127.0.0.1:6379访问redis，但php容器不行。
不过在php中可以直接使用hostname：redis 来连接redis容器</content></entry><entry><title>Linux命令大全：2W多字，一次实现Linux自由</title><url>https://codingroam.github.io/post/linux%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A82w%E5%A4%9A%E5%AD%97%E4%B8%80%E6%AC%A1%E5%AE%9E%E7%8E%B0linux%E8%87%AA%E7%94%B1/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Learning</tag></tags><content type="html"> Linux命令大全：2W多字，一次实现Linux自由
Linux 的学习对于一个程序员的重要性是不言而喻的。
学好它却是程序员必备修养之一。
同时，也是很多公司的面试题。 比如说，曾有一个网易的面试题是：
聊聊：你常用的几个Linux 命令？ 这里 站在网上其他小伙伴的梳理的基础上，做个Linux 命令大全的文章，帮助大家实现Linux自由。
使得大家可以充分展示一下大家雄厚的 “技术肌肉”，让面试官爱到 “不能自已、口水直流”。
从此，再也不怕 Linux命令的面试题，再也不用担心Linux命令如何使用了。
关于虚拟机环境 一直以来， 都使用的是 CentOS 7.7 虚拟机，并且通过百度网盘，共享虚拟机镜像给大家。
最近，由于需要弄 云原生K8S的本地环境，升级到了 CentOS 8.0 虚拟机，也会共享虚拟机镜像给大家。
虚拟机中，安装了常用的 Java JDK、Mysql、Redis、Nacos、Zookeeper、Nginx等基础组件
并且，也进行了高并发的文件句柄数、文件映射数的放开
大家不用做那些重复性的工作，拿过去一键导入，就可以享受那些基础组件的服务。
这个虚拟机的镜像，一级一键导入的操作方法， 可以找 获取，
至少，可以省去大家 2天的安装、基础组件部署、各种依赖踩坑的时间。
还自带K8S的学习环境，这个是大家都梦寐以求的。
接下来，咱们回到正题，帮助大家，实现Linux 命令自由。
操作系统 操作系统 Operating System 简称 OS ，是软件的一部分，它是硬件基础上的第一层软件，是硬件和其它软件沟通的桥梁。
操作系统会控制其他程序运行，管理系统资源，提供最基本的计算功能，如管理及配置内存、决定系统资源供需的优先次序等，同时还提供一些基本的服务程序。
什么是 Linux Linux 系统内核与 Linux 发行套件的区别 Linux 系统内核指的是由 Linus Torvalds 负责维护，提供硬件抽象层、硬盘及文件系统控制及多任务功能的系统核心程序。 Linux 发行套件系统是我们常说的 Linux 操作系统，也即是由 Linux 内核与各种常用软件的集合产品。 「总结：真正的 Linux 指的是系统内核，而我们常说的 Linux 指的是“发行版完整的包含一些基础软件的操作系统”。」
Linux 对比 Windows 稳定且有效率； 免费（或少许费用）； 漏洞少且快速修补； 多任务多用户； 更加安全的用户与文件权限策略； 适合小内核程序的嵌入系统； 相对不耗资源。 Linux 系统种类 「红帽企业版 Linux：」 RHEL 是全世界内使用最广泛的 Linux 系统。它具有极强的性能与稳定性，是众多生成环境中使用的（收费的）系统。 「Fedora：」 由红帽公司发布的桌面版系统套件，用户可以免费体验到最新的技术或工具，这些技术或工具在成熟后会被加入到 RHEL 系统中，因此 Fedora 也成为 RHEL 系统的试验版本。 「CentOS：」 通过把 RHEL 系统重新编译并发布给用户免费使用的 Linux 系统，具有广泛的使用人群。 「Deepin：」 中国发行，对优秀的开源成品进行集成和配置。 「Debian：」 稳定性、安全性强，提供了免费的基础支持，在国外拥有很高的认可度和使用率。 「Ubuntu：」 是一款派生自 Debian 的操作系统，对新款硬件具有极强的兼容能力。 Ubuntu 与 Fedora 都是极其出色的 Linux 桌面系统，而且 Ubuntu 也可用于服务器领域。 终端（ Xterm）连接虚拟机 使用的是 这个终端 xterm （X Window System）
建立SSH会话
然后输入服务器连接密码就可以顺利登陆远程服务器。
从现在开始我们就可以在本地电脑操作远程服务器。
Shell Shell 这个单词的原意是“外壳”，跟 kernel（内核）相对应，比喻内核外面的一层，即用户跟内核交互的对话界面。
Shell 是一个程序，提供一个与用户对话的环境。这个环境只有一个命令提示符，让用户从键盘输入命令，所以又称为命令行环境（ command line interface ，简写为 CLI ）。 Shell 接收到用户输入的命令，将命令送入操作系统执行，并将结果返回给用户。 Shell 是一个命令解释器，解释用户输入的命令。它支持变量、条件判断、循环操作等语法，所以用户可以用 Shell 命令写出各种小程序，又称为 Shell 脚本。这些脚本都通过 Shell 的解释执行，而不通过编译。 Shell 是一个工具箱，提供了各种小工具，供用户方便地使用操作系统的功能。 Shell 的种类 Shell 有很多种，只要能给用户提供命令行环境的程序，都可以看作是 Shell 。
历史上，主要的 Shell 有下面这些：
Bourne Shell（sh） Bourne Again shell（bash） C Shell（csh） TENEX C Shell（tcsh） Korn shell（ksh） Z Shell（zsh） Friendly Interactive Shell（fish） 其中 Bash 是目前最常用的 Shell 。 MacOS 中的默认 Shell 就是 Bash 。
通过执行 echo $SHELL 命令可以查看到当前正在使用的 Shell 。
还可以通过 cat /etc/shells 查看当前系统安装的所有 Shell 种类。
命令 命令行提示符 进入命令行环境以后，用户会看到 Shell 的提示符。
提示符往往是一串前缀，最后以 一个# （或者 $）结尾，用户可以在这个符号后面输入各种命令。
执行一个简单的命令 pwd ：
[root@centos1 ~]# pwd /root 命令解析：
root：表示用户名； iZm5e8dsxce9ufaic7hi3uZ：表示主机名； ~：表示目前所在目录为家目录，其中 root 用户的家目录是 /root 普通用户的家目录在 /home 下； #：指示你所具有的权限（ root 用户为 # ，普通用户为 $ ）。 执行 whoami 命令可以查看当前用户名； 执行 hostname 命令可以查看当前主机名； 关于如何创建、切换、删除用户，在后面的用户与权限会具体讲解，这里先使用 root 用户进行演示。
[备注] root 是超级用户，具备操作系统的一切权限。
注意： root 用户为 # ，普通用户为 $
[root@centos1 ~]# /root -bash: /root: Is a directory [root@centos1 ~]# su minikube [minikube@centos1 root]$ 命令格式 command parameters（命令 参数） 长短参数 「单个参数」：ls -a（a 是英文 all 的缩写，表示“全部”） 「多个参数」：ls -al（全部文件 + 列表形式展示） 「单个长参数」：ls &ndash;all 「多个长参数」：ls &ndash;reverse &ndash;all 「长短混合参数」：ls &ndash;all -l 参数值 「短参数」：command -p 10（例如：ssh root@121.42.11.34
-p 22） 「长参数」：command &ndash;paramters=10（例如：ssh root@121.42.11.34
&ndash;port=22） 快捷方式 在开始学习 Linux 命令之前，有这么一些快捷方式，是必须要提前掌握的，它将贯穿整个 Linux 使用生涯。
通过上下方向键 ↑ ↓ 来调取过往执行过的 Linux 命令； 命令或参数仅需输入前几位就可以用 Tab 键补全； Ctrl + R ：用于查找使用过的命令（history 命令用于列出之前使用过的所有命令，然后输入 ! 命令加上编号( !2 )就可以直接执行该历史命令）； Ctrl + L：清除屏幕并将当前行移到页面顶部； Ctrl + C：中止当前正在执行的命令； Ctrl + U：从光标位置剪切到行首； Ctrl + K：从光标位置剪切到行尾； Ctrl + W：剪切光标左侧的一个单词； Ctrl + Y：粘贴 Ctrl + U | K | Y 剪切的命令； Ctrl + A：光标跳到命令行的开头； Ctrl + E：光标跳到命令行的结尾； Ctrl + D：关闭 Shell 会话； 文件和目录 文件的组织 查看路径 pwd 显示当前目录的路径
which 查看命令的可执行文件所在路径， Linux 下，每一条命令其实都对应一个可执行程序，在终端中输入命令，按回车的时候，就是执行了对应的那个程序， which 命令本身对应的程序也存在于 Linux 中。
总的来说一个命令就是一个可执行程序。
浏览和切换目录 ls 列出文件和目录，它是 Linux 最常用的命令之一。
【常用参数】
-a 显示所有文件和目录包括隐藏的 -l 显示详细列表 -h 适合人类阅读的 -t 按文件最近一次修改时间排序 -i 显示文件的 inode （ inode 是文件内容的标识） cd cd 是英语 change directory 的缩写，表示切换目录。
cd / --> 跳转到根目录 cd ~ --> 跳转到家目录 cd .. --> 跳转到上级目录 cd ./home --> 跳转到当前目录的home目录下 cd /home/lion --> 跳转到根目录下的home目录下的lion目录 cd --> 不添加任何参数，也是回到家目录 [注意] 输入cd /ho + 单次 tab 键会自动补全路径 + 两次 tab 键会列出所有可能的目录列表。
du 列举目录大小信息。
【常用参数】
-h 适合人类阅读的； -a 同时列举出目录下文件的大小信息； -s 只显示总计大小，不显示具体信息。 浏览和创建文件 cat 一次性显示文件所有内容，更适合查看小的文件。
cat cloud-init.log 【常用参数】
-n 显示行号。 less 分页显示文件内容，更适合查看大的文件。
less cloud-init.log 【快捷操作】
空格键：前进一页（一个屏幕）； b 键：后退一页； 回车键：前进一行； y 键：后退一行； 上下键：回退或前进一行； d 键：前进半页； u 键：后退半页； q 键：停止读取文件，中止 less 命令； = 键：显示当前页面的内容是文件中的第几行到第几行以及一些其它关于本页内容的详细信息； h 键：显示帮助文档； / 键：进入搜索模式后，按 n 键跳到一个符合项目，按 N 键跳到上一个符合项目，同时也可以输入正则表达式匹配。 head 显示文件的开头几行（默认是10行）
head cloud-init.log 【参数】
-n 指定行数 head cloud-init.log -n 2 tail 显示文件的结尾几行（默认是10行）
tail cloud-init.log 【参数】
-n 指定行数 tail cloud-init.log -n 2 -f 会每过1秒检查下文件是否有更新内容，也可以用 -s 参数指定间隔时间 tail -f -s 4 xxx.log touch 创建一个文件
touch new_file mkdir 创建一个目录
mkdir new_folder 【常用参数】
-p 递归的创建目录结构 mkdir -p one/two/three 文件的复制和移动 cp 拷贝文件和目录
cp file file_copy --> file 是目标文件，file_copy 是拷贝出来的文件 cp file one --> 把 file 文件拷贝到 one 目录下，并且文件名依然为 file cp file one/file_copy --> 把 file 文件拷贝到 one 目录下，文件名为file_copy cp *.txt folder --> 把当前目录下所有 txt 文件拷贝到 folder 目录下 【常用参数】
-r 递归的拷贝，常用来拷贝一整个目录 mv 移动（重命名）文件或目录，与cp命令用法相似。
mv file one --> 将 file 文件移动到 one 目录下 mv new_folder one --> 将 new_folder 文件夹移动到one目录下 mv *.txt folder --> 把当前目录下所有 txt 文件移动到 folder 目录下 mv file new_file --> file 文件重命名为 new_file 文件的删除和链接 rm 删除文件和目录，由于 Linux 下没有回收站，一旦删除非常难恢复，因此需要谨慎操作
rm new_file --> 删除 new_file 文件 rm f1 f2 f3 --> 同时删除 f1 f2 f3 3个文件 【常用参数】
-i 向用户确认是否删除； -f 文件强制删除； -r 递归删除文件夹，著名的删除操作 rm -rf 。 ln 英文 Link 的缩写，表示创建链接。
学习创建链接之前，首先要理解链接是什么，我们先来看看 Linux 的文件是如何存储的：
Linux 文件的存储方式分为3个部分，文件名、文件内容以及权限，其中文件名的列表是存储在硬盘的其它地方和文件内容是分开存放的，每个文件名通过 inode 标识绑定到文件内容。
Linux 下有两种链接类型：硬链接和软链接。
硬链接 使链接的两个文件共享同样文件内容，就是同样的 inode ，一旦文件1和文件2之间有了硬链接，那么修改任何一个文件，修改的都是同一块内容，它的缺点是，只能创建指向文件的硬链接，不能创建指向目录的（其实也可以，但比较复杂）而软链接都可以，因此软链接使用更加广泛。
ln file1 file2 --> 创建 file2 为 file1 的硬链接 如果我们用 rm file1 来删除 file1 ，对 file2 没有什么影响，对于硬链接来说，删除任意一方的文件，共同指向的文件内容并不会从硬盘上删除。只有同时删除了 file1 与 file2 后，它们共同指向的文件内容才会消失。
软链接 软链接就类似 windows 下快捷方式。
ln -s file1 file2 执行 ls -l 命名查看当前目录下文件的具体信息
total 0 -rw-r--r-- 1 root root 0 Jan 14 06:29 file1 lrwxrwxrwx 1 root root 5 Jan 14 06:42 file2 -> file1 # 表示file2 指向 file1 其实 file2 只是 file1 的一个快捷方式，它指向的是 file1 ，所以显示的是 file1 的内容，但其实 file2 的 inode 与 file1 并不相同。如果我们删除了 file2 的话， file1 是不会受影响的，但如果删除 file1 的话， file2 就会变成死链接，因为指向的文件不见了。
用户与权限 用户 Linux 是一个多用户的操作系统。在 Linux 中，理论上来说，我们可以创建无数个用户，但是这些用户是被划分到不同的群组里面的，有一个用户，名叫 root ，是一个很特殊的用户，它是超级用户，拥有最高权限。
自己创建的用户是有限权限的用户，这样大大提高了 Linux 系统的安全性，有效防止误操作或是病毒攻击，但是我们执行的某些命令需要更高权限时可以使用 sudo 命令。
sudo 以 root 身份运行命令
sudo date --> 当然查看日期是不需要sudo的这里只是演示，sudo 完之后一般还需要输入用户密码的 useradd + passwd useradd 添加新用户 passwd 修改用户密码 这两个命令需要 root 用户权限
useradd lion --> 添加一个lion用户，添加完之后在 /home 路径下可以查看 passwd lion --> 修改lion用户的密码 userdel 删除用户，需要 root 用户权限
userdel lion --> 只会删除用户名，不会从/home中删除对应文件夹 userdel lion -r --> 会同时删除/home下的对应文件夹 su 切换用户，需要 root 用户权限
sudo su --> 切换为root用户（exit 命令或 CTRL + D 快捷键都可以使普通用户切换为 root 用户） su lion --> 切换为普通用户 su - --> 切换为root用户 群组的管理 Linux 中每个用户都属于一个特定的群组，如果你不设置用户的群组，默认会创建一个和它的用户名一样的群组，并且把用户划归到这个群组。
groupadd 创建群组，用法和 useradd 类似。
groupadd friends groupdel 删除一个已存在的群组
groupdel foo --> 删除foo群组 groups 查看用户所在群组
groups lion --> 查看 lion 用户所在的群组 usermod 用于修改用户的账户。
【常用参数】
-l 对用户重命名。需要注意的是 /home 中的用户家目录的名字不会改变，需要手动修改。 -g 修改用户所在的群组，例如 usermod -g friends lion 修改 lion 用户的群组为 friends 。 -G 一次性让用户添加多个群组，例如 usermod -G friends,foo,bar lion 。 -a -G 会让你离开原先的群组，如果你不想这样做的话，就得再添加 -a 参数，意味着 append 追加的意思。 chgrp 用于修改文件的群组。
chgrp bar file.txt --> file.txt文件的群组修改为bar chown 改变文件的所有者，需要 root 身份才能运行。
chown lion file.txt --> 把其它用户创建的file.txt转让给lion用户 chown lion:bar file.txt --> 把file.txt的用户改为lion，群组改为bar 【常用参数】
-R 递归设置子目录和子文件， chown -R lion:lion /home/frank 把 frank 文件夹的用户和群组都改为 lion 。 文件权限管理 chmod 修改访问权限。
chmod 740 file.txt 【常用参数】
-R 可以递归地修改文件访问权限，例如 chmod -R 777 /home/lion 修改权限的确简单，但是理解其深层次的意义才是更加重要的。下面我们来系统的学习 Linux 的文件权限。
[root@lion ~]# ls -l drwxr-xr-x 5 root root 4096 Apr 13 2020 climb lrwxrwxrwx 1 root root 7 Jan 14 06:41 hello2.c -> hello.c -rw-r--r-- 1 root root 149 Jan 13 06:14 hello.c 其中 drwxr-xr-x 表示文件或目录的权限。让我们一起来解读它具体代表什么？
d ：表示目录，就是说这是一个目录，普通文件是 - ，链接是 l 。 r ： read 表示文件可读。 w ： write 表示文件可写，一般有写的权限，就有删除的权限。 x ： execute 表示文件可执行。 ：表示没有相应权限。 权限的整体是按用户来划分的，如下图所示：
现在再来理解这句权限 drwxr-xr-x 的意思：
它是一个文件夹； 它的所有者具有：读、写、执行权限； 它的群组用户具有：读、执行的权限，没有写的权限； 它的其它用户具有：读、执行的权限，没有写的权限。 现在理解了权限，我们使用 chmod 来尝试修改权限。 chmod 它不需要是 root 用户才能运行的，只要你是此文件所有者，就可以用 chmod 来修改文件的访问权限。
数字分配权限 因此要改变权限，只要做一些简单的加法就行：
chmod 640 hello.c # 分析 6 = 4 + 2 + 0 表示所有者具有 rw 权限 4 = 4 + 0 + 0 表示群组用户具有 r 权限 0 = 0 + 0 + 0 表示其它用户没有权限 对应文字权限为：-rw-r----- 用字母来分配权限 u ： user 的缩写，用户的意思，表示所有者。 g ： group 的缩写，群组的意思，表示群组用户。 o ： other 的缩写，其它的意思，表示其它用户。 a ： all 的缩写，所有的意思，表示所有用户。 ：加号，表示添加权限。 ：减号，表示去除权限。 = ：等于号，表示分配权限。 chmod u+rx file --> 文件file的所有者增加读和运行的权限 chmod g+r file --> 文件file的群组用户增加读的权限 chmod o-r file --> 文件file的其它用户移除读的权限 chmod g+r o-r file --> 文件file的群组用户增加读的权限，其它用户移除读的权限 chmod go-r file --> 文件file的群组和其他用户移除读的权限 chmod +x file --> 文件file的所有用户增加运行的权限 chmod u=rwx,g=r,o=- file --> 文件file的所有者分配读写和执行的权限，群组其它用户分配读的权限，其他用户没有任何权限 查找文件 locate 搜索包含关键字的所有文件和目录。后接需要查找的文件名，也可以用正则表达式。
安装 locate yum -y install mlocate --> 安装包 updatedb --> 更新数据库 locate file.txt locate fil*.txt [注意] locate 命令会去文件数据库中查找命令，而不是全磁盘查找，因此刚创建的文件并不会更新到数据库中，所以无法被查找到，可以执行 updatedb 命令去更新数据库。
find 用于查找文件，它会去遍历你的实际硬盘进行查找，而且它允许我们对每个找到的文件进行后续操作，功能非常强大。
find &lt;何处> &lt;何物> &lt;做什么> 「何处」：指定在哪个目录查找，此目录的所有子目录也会被查找。 「何物」：查找什么，可以根据文件的名字来查找，也可以根据其大小来查找，还可以根据其最近访问时间来查找。 「做什么」：找到文件后，可以进行后续处理，如果不指定这个参数， find 命令只会显示找到的文件。 根据文件名查找 find -name "file.txt" --> 当前目录以及子目录下通过名称查找文件 find . -name "syslog" --> 当前目录以及子目录下通过名称查找文件 find / -name "syslog" --> 整个硬盘下查找syslog find /var/log -name "syslog" --> 在指定的目录/var/log下查找syslog文件 find /var/log -name "syslog*" --> 查找syslog1、syslog2 ... 等文件，通配符表示所有 find /var/log -name "*syslog*" --> 查找包含syslog的文件 [注意] find 命令只会查找完全符合 “何物” 字符串的文件，而 locate 会查找所有包含关键字的文件。
根据文件大小查找 find /var -size +10M --> /var 目录下查找文件大小超过 10M 的文件 find /var -size -50k --> /var 目录下查找文件大小小于 50k 的文件 find /var -size +1G --> /var 目录下查找文件大小查过 1G 的文件 find /var -size 1M --> /var 目录下查找文件大小等于 1M 的文件 根据文件最近访问时间查找 find -name "*.txt" -atime -7 --> 近 7天内访问过的.txt结尾的文件 仅查找目录或文件 find . -name "file" -type f --> 只查找当前目录下的file文件 find . -name "file" -type d --> 只查找当前目录下的file目录 操作查找结果 find -name "*.txt" -printf "%p - %u\n" --> 找出所有后缀为txt的文件，并按照 %p - %u\n 格式打印，其中%p=文件名，%u=文件所有者 find -name "*.jpg" -delete --> 删除当前目录以及子目录下所有.jpg为后缀的文件，不会有删除提示，因此要慎用 find -name "*.c" -exec chmod 600 { } \; --> 对每个.c结尾的文件，都进行 -exec 参数指定的操作，{ } 会被查找到的文件替代，\; 是必须的结尾 find -name "*.c" -ok chmod 600 { } \; --> 和上面的功能一直，会多一个确认提示 软件仓库 Linux 下软件是以包的形式存在，一个软件包其实就是软件的所有文件的压缩包，是二进制的形式，包含了安装软件的所有指令。 Red Hat 家族的软件包后缀名一般为 .rpm ， Debian 家族的软件包后缀是 .deb 。
Linux 的包都存在一个仓库，叫做软件仓库，它可以使用 yum 来管理软件包， yum 是 CentOS 中默认的包管理工具，适用于 Red Hat 一族。可以理解成 Node.js 的 npm 。
yum 常用命令 yum update | yum upgrade 更新软件包 yum search xxx 搜索相应的软件包 yum install xxx 安装软件包 yum remove xxx 删除软件包 切换 CentOS 软件源 有时候 CentOS 默认的 yum 源不一定是国内镜像，导致 yum 在线安装及更新速度不是很理想。这时候需要将 yum 源设置为国内镜像站点。国内主要开源的镜像站点是网易和阿里云。
1、首先备份系统自带 yum 源配置文件 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
2、下载阿里云的 yum 源配置文件到 /etc/yum.repos.d/CentOS7
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 3、生成缓存
yum makecache 阅读手册 Linux 命令种类繁杂，我们凭借记忆不可能全部记住，因此学会查用手册是非常重要的。
man 安装更新 man sudo yum install -y man-pages --> 安装 sudo mandb --> 更新 man 手册种类 可执行程序或 Shell 命令； 系统调用（ Linux 内核提供的函数）； 库调用（程序库中的函数）； 文件（例如 /etc/passwd ）； 特殊文件（通常在 /dev 下）； 游戏； 杂项（ man(7) ，groff(7) ）； 系统管理命令（通常只能被 root 用户使用）； 内核子程序。 man + 数字 + 命令 输入 man + 数字 + 命令/函数，可以查到相关的命令和函数，若不加数字， man 默认从数字较小的手册中寻找相关命令和函数
man 3 rand --> 表示在手册的第三部分查找 rand 函数 man ls --> 查找 ls 用法手册 man 手册核心区域解析：(以 man pwd 为例)
NAME # 命令名称和简单描述 pwd -- return working directory name SYNOPSIS # 使用此命令的所有方法 pwd [-L | -P] DESCRIPTION # 包括所有参数以及用法 The pwd utility writes the absolute pathname of the current working directory to the standard output. Some shells may provide a builtin pwd command which is similar or identical to this utility. Consult the builtin(1) manual page. The options are as follows: -L Display the logical current working directory. -P Display the physical current working directory (all symbolic links resolved). If no options are specified, the -L option is assumed. SEE ALSO # 扩展阅读相关命令 builtin(1), cd(1), csh(1), sh(1), getcwd(3) help man 命令像新华词典一样可以查询到命令或函数的详细信息，但其实我们还有更加快捷的方式去查询， command &ndash;help 或 command -h ，它没有 man 命令显示的那么详细，但是它更加易于阅读。
文本操作 grep 全局搜索一个正则表达式，并且打印到屏幕。简单来说就是，在文件中查找关键字，并显示关键字所在行。
基础语法 grep text file # text代表要搜索的文本，file代表供搜索的文件 # 实例 [root@lion ~]# grep path /etc/profile pathmunge () { pathmunge /usr/sbin pathmunge /usr/local/sbin pathmunge /usr/local/sbin after pathmunge /usr/sbin after unset -f pathmunge 常用参数 -i 忽略大小写， grep -i path /etc/profile -n 显示行号，grep -n path /etc/profile -v 只显示搜索文本不在的那些行，grep -v path /etc/profile -r 递归查找， grep -r hello /etc ，Linux 中还有一个 rgrep 命令，作用相当于 grep -r 高级用法 grep 可以配合正则表达式使用。
grep -E path /etc/profile --> 完全匹配path grep -E ^path /etc/profile --> 匹配path开头的字符串 grep -E [Pp]ath /etc/profile --> 匹配path或Path sort 对文件的行进行排序。
基础语法 sort name.txt # 对name.txt文件进行排序 实例用法 为了演示方便，我们首先创建一个文件 name.txt ，放入以下内容：
Christopher Shawn Ted Rock Noah Zachary Bella 执行 sort name.txt 命令，会对文本内容进行排序。
常用参数 -o 将排序后的文件写入新文件， sort -o name_sorted.txt name.txt ； -r 倒序排序， sort -r name.txt ； -R 随机排序， sort -R name.txt ； -n 对数字进行排序，默认是把数字识别成字符串的，因此 138 会排在 25 前面，如果添加了 -n 数字排序的话，则 25 会在 138 前面。 wc word count 的缩写，用于文件的统计。它可以统计单词数目、行数、字符数，字节数等。
基础语法 wc name.txt # 统计name.txt 实例用法 [root@lion ~]# wc name.txt 13 13 91 name.txt 第一个13，表示行数； 第二个13，表示单词数； 第三个91，表示字节数。 常用参数 -l 只统计行数， wc -l name.txt ； -w 只统计单词数， wc -w name.txt ； -c 只统计字节数， wc -c name.txt ； -m 只统计字符数， wc -m name.txt 。 uniq 删除文件中的重复内容。
基础语法 uniq name.txt # 去除name.txt重复的行数，并打印到屏幕上 uniq name.txt uniq_name.txt # 把去除重复后的文件保存为 uniq_name.txt 【注意】它只能去除连续重复的行数。
常用参数 -c 统计重复行数， uniq -c name.txt ； -d 只显示重复的行数， uniq -d name.txt 。 cut 剪切文件的一部分内容。
基础语法 cut -c 2-4 name.txt # 剪切每一行第二到第四个字符 常用参数 -d 用于指定用什么分隔符（比如逗号、分号、双引号等等） cut -d , name.txt ； -f 表示剪切下用分隔符分割的哪一块或哪几块区域， cut -d , -f 1 name.txt 。 重定向 管道 流 在 Linux 中一个命令的去向可以有3个地方：终端、文件、作为另外一个命令的入参。
命令一般都是通过键盘输入，然后输出到终端、文件等地方，它的标准用语是 stdin 、 stdout 以及 stderr 。
标准输入 stdin ，终端接收键盘输入的命令，会产生两种输出； 标准输出 stdout ，终端输出的信息（不包含错误信息）； 标准错误输出 stderr ，终端输出的错误信息。 重定向 把本来要显示在终端的命令结果，输送到别的地方（到文件中或者作为其他命令的输入）。
输出重定向 > 表示重定向到新的文件， cut -d , -f 1 notes.csv > name.csv ，它表示通过逗号剪切 notes.csv 文件（剪切完有3个部分）获取第一个部分，重定向到 name.csv 文件。
我们来看一个具体示例，学习它的使用，假设我们有一个文件 notes.csv ，文件内容如下：
Mark1,951/100,很不错1 Mark2,952/100,很不错2 Mark3,953/100,很不错3 Mark4,954/100,很不错4 Mark5,955/100,很不错5 Mark6,956/100,很不错6 执行命令： cut -d , -f 1 notes.csv > name.csv 最后输出如下内容：
Mark1 Mark2 Mark3 Mark4 Mark5 Mark6 【注意】使用 > 要注意，如果输出的文件不存在它会新建一个，如果输出的文件已经存在，则会覆盖。因此执行这个操作要非常小心，以免覆盖其它重要文件。
输出重定向 &raquo; 表示重定向到文件末尾，因此它不会像 > 命令这么危险，它是追加到文件的末尾（当然如果文件不存在，也会被创建）。
再次执行 cut -d , -f 1 notes.csv &raquo; name.csv ，则会把名字追加到 name.csv 里面。
Mark1 Mark2 Mark3 Mark4 Mark5 Mark6 Mark1 Mark2 Mark3 Mark4 Mark5 Mark6 我们平时读的 log 日志文件其实都是用这个命令输出的。
输出重定向 2> 标准错误输出
cat not_exist_file.csv > res.txt 2> errors.log 当我们 cat 一个文件时，会把文件内容打印到屏幕上，这个是标准输出； 当使用了 > res.txt 时，则不会打印到屏幕，会把标准输出写入文件 res.txt 文件中； 2> errors.log 当发生错误时会写入 errors.log 文件中。 输出重定向 2&raquo; 标准错误输出（追加到文件末尾）同 &raquo; 相似。
输出重定向 2>&amp;1 标准输出和标准错误输出都重定向都一个地方
cat not_exist_file.csv > res.txt 2>&amp;1 # 覆盖输出 cat not_exist_file.csv >> res.txt 2>&amp;1 # 追加输出 目前为止，我们接触的命令的输入都来自命令的参数，其实命令的输入还可以来自文件或者键盘的输入。
输入重定向 &lt; &lt; 符号用于指定命令的输入。
cat &lt; name.csv # 指定命令的输入为 name.csv 虽然它的运行结果与 cat name.csv 一样，但是它们的原理却完全不同。
cat name.csv 表示 cat 命令接收的输入是 notes.csv 文件名，那么要先打开这个文件，然后打印出文件内容。 cat &lt; name.csv 表示 cat 命令接收的输入直接是 notes.csv 这个文件的内容， cat 命令只负责将其内容打印，打开文件并将文件内容传递给 cat 命令的工作则交给终端完成。 输入重定向 &laquo; 将键盘的输入重定向为某个命令的输入。
sort -n &lt;&lt; END # 输入这个命令之后，按下回车，终端就进入键盘输入模式，其中END为结束命令（这个可以自定义） wc -m &lt;&lt; END # 统计输入的单词 管道 | 把两个命令连起来使用，一个命令的输出作为另外一个命令的输入，英文是 pipeline ，可以想象一个个水管连接起来，管道算是重定向流的一种。
举几个实际用法案例：
cut -d , -f 1 name.csv | sort > sorted_name.txt # 第一步获取到的 name 列表，通过管道符再进行排序，最后输出到sorted_name.txt du | sort -nr | head # du 表示列举目录大小信息 # sort 进行排序,-n 表示按数字排序，-r 表示倒序 # head 前10行文件 grep log -Ir /var/log | cut -d : -f 1 | sort | uniq # grep log -Ir /var/log 表示在log文件夹下搜索 /var/log 文本，-r 表示递归，-I 用于排除二进制文件 # cut -d : -f 1 表示通过冒号进行剪切，获取剪切的第一部分 # sort 进行排序 # uniq 进行去重 流 流并非一个命令，在计算机科学中，流 stream 的含义是比较难理解的，记住一点即可：「流就是读一点数据, 处理一点点数据。其中数据一般就是二进制格式。」 上面提及的重定向或管道，就是把数据当做流去运转的。
到此我们就接触了，流、重定向、管道等 Linux 高级概念及指令。其实你会发现关于流和管道在其它语言中也有广泛的应用。 Angular 中的模板语法中可以使用管道。 Node.js 中也有 stream 流的概念。
查看进程 在 Windows 中通过 Ctrl + Alt + Delete 快捷键查看软件进程。
w 帮助我们快速了解系统中目前有哪些用户登录着，以及他们在干什么。
[root@lion ~]# w 06:31:53 up 25 days, 9:53, 1 user, load average: 0.00, 0.01, 0.05 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 118.31.243.53 05:56 1.00s 0.02s 0.00s w 06:31:53：表示当前时间 up 25 days, 9:53：表示系统已经正常运行了“25天9小时53分钟” 1 user：表示一个用户 load average: 0.00, 0.01, 0.05：表示系统的负载，3个值分别表示“1分钟的平均负载”，“5分钟的平均负载”，“15分钟的平均负载” USER：表示登录的用于 TTY：登录的终端名称为pts/0 FROM：连接到服务器的ip地址 LOGIN@：登录时间 IDLE：用户有多久没有活跃了 JCPU：该终端所有相关的进程使用的 CPU 时间，每当进程结束就停止计时，开始新的进程则会重新计时 PCPU：表示 CPU 执行当前程序所消耗的时间，当前进程就是在 WHAT 列里显示的程序 WHAT：表示当下用户正运行的程序是什么，这里我运行的是 w ps 用于显示当前系统中的进程， ps 命令显示的进程列表不会随时间而更新，是静态的，是运行 ps 命令那个时刻的状态或者说是一个进程快照。
基础语法 [root@lion ~]# ps PID TTY TIME CMD 1793 pts/0 00:00:00 bash 4756 pts/0 00:00:00 ps PID：进程号，每个进程都有唯一的进程号 TTY：进程运行所在的终端 TIME：进程运行时间 CMD：产生这个进程的程序名，如果在进程列表中看到有好几行都是同样的程序名，那么就是同样的程序产生了不止一个进程 常用参数 -ef 列出所有进程; -efH 以乔木状列举出所有进程; -u 列出此用户运行的进程; -aux 通过 CPU 和内存使用来过滤进程 ps -aux | less ; -aux &ndash;sort -pcpu 按 CPU 使用降序排列， -aux &ndash;sort -pmem 表示按内存使用降序排列; -axjf 以树形结构显示进程， ps -axjf 它和 pstree 效果类似。 top 获取进程的动态列表。
top - 07:20:07 up 25 days, 10:41, 1 user, load average: 0.30, 0.10, 0.07 Tasks: 67 total, 1 running, 66 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.7 us, 0.3 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 1882072 total, 552148 free, 101048 used, 1228876 buff/cache KiB Swap: 0 total, 0 free, 0 used. 1594080 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 956 root 10 -10 133964 15848 10240 S 0.7 0.8 263:13.01 AliYunDun 1 root 20 0 51644 3664 2400 S 0.0 0.2 3:23.63 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.05 kthreadd 4 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H top - 07:20:07 up 25 days, 10:41, 1 user, load average: 0.30, 0.10, 0.07 相当 w 命令的第一行的信息。 展示的这些进程是按照使用处理器 %CPU 的使用率来排序的。 kill 结束一个进程， kill + PID 。
kill 956 # 结束进程号为956的进程 kill 956 957 # 结束多个进程 kill -9 7291 # 强制结束进程 管理进程 进程状态 主要是切换进程的状态。我们先了解下 Linux 下进程的五种状态：
状态码 R ：表示正在运行的状态； 状态码 S ：表示中断（休眠中，受阻，当某个条件形成后或接受到信号时，则脱离该状态）； 状态码 D ：表示不可中断（进程不响应系统异步信号，即使用kill命令也不能使其中断）； 状态码 Z ：表示僵死（进程已终止，但进程描述符依然存在，直到父进程调用 wait4() 系统函数后将进程释放）； 状态码 T ：表示停止（进程收到 SIGSTOP 、 SIGSTP 、 SIGTIN 、 SIGTOU 等停止信号后停止运行）。 前台进程 &amp; 后台进程 默认情况下，用户创建的进程都是前台进程，前台进程从键盘读取数据，并把处理结果输出到显示器。例如运行 top 命令，这就是一个一直运行的前台进程。
后台进程的优点是不必等待程序运行结束，就可以输入其它命令。在需要执行的命令后面添加 &amp; 符号，就表示启动一个后台进程。
&amp; 启动后台进程，它的缺点是后台进程与终端相关联，一旦关闭终端，进程就自动结束了。
cp name.csv name-copy.csv &amp; nohup 使进程不受挂断（关闭终端等动作）的影响。
nohup cp name.csv name-copy.csv nohup 命令也可以和 &amp; 结合使用。
nohup cp name.csv name-copy.csv &amp; bg 使一个“后台暂停运行”的进程，状态改为“后台运行”。
bg %1 # 不加任何参数的情况下，bg命令会默认作用于最近的一个后台进程，如果添加参数则会作用于指定标号的进程 实际案例1：
1. 执行 grep -r "log" / > grep_log 2>&amp;1 命令启动一个前台进程，并且忘记添加 &amp; 符号 2. ctrl + z 使进程状态转为后台暂停 3. 执行 bg 将命令转为后台运行 实际案例2：
前端开发时我们经常会执行 yarn start 启动项目 此时我们执行 ctrl + z 先使其暂停 然后执行 bg 使其转为后台运行 这样当前终端就空闲出来可以干其它事情了，如果想要唤醒它就使用 fg 命令即可（后面会讲） jobs 显示当前终端后台进程状态。
[root@lion ~]# jobs [1]+ Stopped top [2]- Running grep --color=auto -r "log" / > grep_log 2>&amp;1 &amp; fg fg 使进程转为前台运行，用法和 bg 命令类似。
我们用一张图来表示前后台进程切换：
我们可以使程序在后台运行，成为后台进程，这样在当前终端中我们就可以做其他事情了，而不必等待此进程运行结束。
守护进程 一个运行起来的程序被称为进程。在 Linux 中有些进程是特殊的，它不与任何进程关联，不论用户的身份如何，都在后台运行，这些进程的父进程是 PID 为1的进程， PID 为1的进程只在系统关闭时才会被销毁。它们会在后台一直运行等待分配工作。我们将这类进程称之为守护进程 daemon 。
守护进程的名字通常会在最后有一个 d ，表示 daemon 守护的意思，例如 systemd 、httpd 。
systemd systemd 是一个 Linux 系统基础组件的集合，提供了一个系统和服务管理器，运行为 PID 1 并负责启动其它程序。
[root@lion ~]# ps -aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.2 51648 3852 ? Ss Feb01 1:50 /usr/lib/systemd/systemd --switched-root --system --deserialize 22 通过命令也可以看到 PID 为1的进程就是 systemd 的系统进程。
systemd 常用命令（它是一组命令的集合）：
systemctl start nginx # 启动服务 systemctl stop nginx # 停止服务 systemctl restart nginx # 重启服务 systemctl status nginx # 查看服务状态 systemctl reload nginx # 重载配置文件(不停止服务的情况) systemctl enable nginx # 开机自动启动服务 systemctl disable nginx # 开机不自动启动服务 systemctl is-enabled nginx # 查看服务是否开机自动启动 systemctl list-unit-files --type=service # 查看各个级别下服务的启动和禁用情况 文件压缩解压 打包：是将多个文件变成一个总的文件，它的学名叫存档、归档。 压缩：是将一个大文件（通常指归档）压缩变成一个小文件。 我们常常使用 tar 将多个文件归档为一个总的文件，称为 archive 。 然后用 gzip 或 bzip2 命令将 archive 压缩为更小的文件。
tar 创建一个 tar 归档。
基础用法 tar -cvf sort.tar sort/ # 将sort文件夹归档为sort.tar tar -cvf archive.tar file1 file2 file3 # 将 file1 file2 file3 归档为archive.tar 常用参数 -cvf 表示 create（创建）+ verbose（细节）+ file（文件），创建归档文件并显示操作细节； -tf 显示归档里的内容，并不解开归档； -rvf 追加文件到归档， tar -rvf archive.tar file.txt ； -xvf 解开归档， tar -xvf archive.tar 。 gzip / gunzip “压缩/解压”归档，默认用 gzip 命令，压缩后的文件后缀名为 .tar.gz 。
gzip archive.tar # 压缩 gunzip archive.tar.gz # 解压 tar 归档+压缩 可以用 tar 命令同时完成归档和压缩的操作，就是给 tar 命令多加一个选项参数，使之完成归档操作后，还是调用 gzip 或 bzip2 命令来完成压缩操作。
tar -zcvf archive.tar.gz archive/ # 将archive文件夹归档并压缩 tar -zxvf archive.tar.gz # 将archive.tar.gz归档压缩文件解压 zcat、zless、zmore 之前讲过使用 cat less more 可以查看文件内容，但是压缩文件的内容是不能使用这些命令进行查看的，而要使用 zcat、zless、zmore 进行查看。
zcat archive.tar.gz zip/unzip “压缩/解压” zip 文件（ zip 压缩文件一般来自 windows 操作系统）。
命令安装 # Red Hat 一族中的安装方式 yum install zip yum install unzip 基础用法 unzip archive.zip # 解压 .zip 文件 unzip -l archive.zip # 不解开 .zip 文件，只看其中内容 zip -r sort.zip sort/ # 将sort文件夹压缩为 sort.zip，其中-r表示递归 编译安装软件 之前我们学会了使用 yum 命令进行软件安装，如果碰到 yum 仓库中没有的软件，我们就需要会更高级的软件安装“源码编译安装”。
编译安装 简单来说，编译就是将程序的源代码转换成可执行文件的过程。大多数 Linux 的程序都是开放源码的，可以编译成适合我们的电脑和操纵系统属性的可执行文件。
基本步骤如下：
下载源代码 解压压缩包 配置 编译 安装 实际案例 1、下载 我们来编译安装 htop 软件，首先在它的官网下载源码：
https://bintray.com/htop/source/htop#files
下载好的源码在本机电脑上使用如下命令同步到服务器上：
scp 文件名 用户名@服务器ip:目标路径 scp ~/Desktop/htop-3.0.0.tar.gz root@121.42.11.34:. 也可以使用 wegt 进行下载：
wegt+下载地址 wegt https://bintray.com/htop/source/download_file?file_path=htop-3.0.0.tar.gz 2、解压文件 tar -zxvf htop-3.0.0.tar.gz # 解压 cd htop-3.0.0 # 进入目录 3、配置 执行 ./configure ，它会分析你的电脑去确认编译所需的工具是否都已经安装了。
4、编译 执行 make 命令
5、安装 执行 make install 命令，安装完成后执行 ls /usr/local/bin/ 查看是否有 htop 命令。如果有就可以执行 htop 命令查看系统进程了。
网络 ifconfig 查看 ip 网络相关信息，如果命令不存在的话， 执行命令 yum install net-tools 安装。
[root@lion ~]# ifconfig eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.31.24.78 netmask 255.255.240.0 broadcast 172.31.31.255 ether 00:16:3e:04:9c:cd txqueuelen 1000 (Ethernet) RX packets 1592318 bytes 183722250 (175.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1539361 bytes 154044090 (146.9 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73&lt;UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 参数解析：
eth0 对应有线连接（对应你的有线网卡），就是用网线来连接的上网。 eth 是 Ethernet 的缩写，表示“以太网”。有些电脑可能同时有好几条网线连着，例如服务器，那么除了 eht0 ，你还会看到 eth1 、 eth2 等。 lo 表示本地回环（ Local Loopback 的缩写，对应一个虚拟网卡）可以看到它的 ip 地址是 127.0.0.1 。每台电脑都应该有这个接口，因为它对应着“连向自己的链接”。这也是被称之为“本地回环”的原因。所有经由这个接口发送的东西都会回到你自己的电脑。看起来好像并没有什么用，但有时为了某些缘故，我们需要连接自己。例如用来测试一个网络程序，但又不想让局域网或外网的用户查看，只能在此台主机上运行和查看所有的网络接口。例如在我们启动一个前端工程时，在浏览器输入 127.0.0.1:3000 启动项目就能查看到自己的 web 网站，并且它只有你能看到。 wlan0 表示无线局域网（上面案例并未展示）。 host ip 地址和主机名的互相转换。
软件安装 yum install bind-utils 基础用法 [root@lion ~]# host github.com baidu.com has address 13.229.188.59 [root@lion ~]# host 13.229.188.59 59.188.229.13.in-addr.arpa domain name pointer ec2-13-229-188-59.ap-southeast-1.compute.amazonaws.com. ssh 连接远程服务器 通过非对称加密以及对称加密的方式（同 HTTPS 安全连接原理相似）连接到远端服务器。
ssh 用户@ip:port 1、ssh root@172.20.10.1:22 # 端口号可以省略不写，默认是22端口 2、输入连接密码后就可以操作远端服务器了 配置 ssh config 文件可以配置 ssh ，方便批量管理多个 ssh 连接。
配置文件分为以下几种：
全局 ssh 服务端的配置： /etc/ssh/sshd_config ； 全局 ssh 客户端的配置： /etc/ssh/ssh_config（很少修改）； 当前用户 ssh 客户端的配置： ~/.ssh/config 。 【服务端 config 文件的常用配置参数】
[注意] 修改完服务端配置文件需要重启服务 systemctl restart sshd
【客户端 config 文件的常用配置参数】
配置当前用户的 config ：
# 创建config vim ~/.ssh/config # 填写一下内容 Host lion # 别名 HostName 172.x.x.x # ip 地址 Port 22 # 端口 User root # 用户 这样配置完成后，下次登录时，可以这样登录 ssh lion 会自动识别为 root 用户。
[注意] 这段配置不是在服务器上，而是你自己的机器上，它仅仅是设置了一个别名。
免密登录 ssh 登录分两种，一种是基于口令（账号密码），另外一种是基于密钥的方式。
基于口令，就是每次登录输入账号和密码，显然这样做是比较麻烦的，今天主要学习如何基于密钥实现免密登录。
基于密钥验证原理 客户机生成密钥对（公钥和私钥），把公钥上传到服务器，每次登录会与服务器的公钥进行比较，这种验证登录的方法更加安全，也被称为“公钥验证登录”。
具体实现步骤 1、在客户机中生成密钥对（公钥和私钥） ssh-keygen（默认使用 RSA 非对称加密算法）
运行完 ssh-keygen 会在 ~/.ssh/ 目录下，生成两个文件：
id_rsa.pub ：公钥 id_rsa ：私钥 2、把客户机的公钥传送到服务
执行 ssh-copy-id root@172.x.x.x
（ssh-copy-id 它会把客户机的公钥追加到服务器 ~/.ssh/authorized_keys 的文件中）。
执行完成后，运行 ssh root@172.x.x.x
就可以实现免密登录服务器了。
配合上面设置好的别名，直接执行 ssh lion 就可以登录，是不是非常方便。
wget 可以使我们直接从终端控制台下载文件，只需要给出文件的HTTP或FTP地址。
wget [参数][URL地址] wget http://www.minjieren.com/wordpress-3.1-zh_CN.zip wget 非常稳定，如果是由于网络原因下载失败， wget 会不断尝试，直到整个文件下载完毕。
常用参数 -c 继续中断的下载。 备份 scp 它是 Secure Copy 的缩写，表示安全拷贝。 scp 可以使我们通过网络，把文件从一台电脑拷贝到另一台电脑。
scp 是基于 ssh 的原理来运作的， ssh 会在两台通过网络连接的电脑之间创建一条安全通信的管道， scp 就利用这条管道安全地拷贝文件。
scp source_file destination_file # source_file 表示源文件，destination_file 表示目标文件 其中 source_file 和 destination_file 都可以这样表示： user@ip:file_name ， user 是登录名， ip 是域名或 ip 地址。 file_name 是文件路径。
scp file.txt root@192.168.1.5:/root # 表示把我的电脑中当前文件夹下的 file.txt 文件拷贝到远程电脑 scp root@192.168.1.5:/root/file.txt file.txt # 表示把远程电脑上的 file.txt 文件拷贝到本机 rsync rsync 命令主要用于远程同步文件。它可以同步两个目录，不管它们是否处于同一台电脑。它应该是最常用于“增量备份”的命令了。它就是智能版的 scp 命令。
软件安装 yum install rsync 基础用法 rsync -arv Images/ backups/ # 将Images 目录下的所有文件备份到 backups 目录下 rsync -arv Images/ root@192.x.x.x:backups/ # 同步到服务器的backups目录下 常用参数 -a 保留文件的所有信息，包括权限，修改日期等； -r 递归调用，表示子目录的所有文件也都包括； -v 冗余模式，输出详细操作信息。 默认地， rsync 在同步时并不会删除目标目录的文件，例如你在源目录中删除一个文件，但是用 rsync 同步时，它并不会删除同步目录中的相同文件。如果向删除也可以这么做： rsync -arv &ndash;delete Images/ backups/ 。
系统 halt 关闭系统，需要 root 身份。
halt reboot 重启系统，需要 root 身份。
reboot poweroff 直接运行即可关机，不需要 root 身份。
Vim 是什么？ Vim 是从 vi 发展出来的一个文本编辑器。其代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。和 Emacs 并列成为类 Unix 系统用户最喜欢的编辑器。
Vim 常用模式 交互模式 插入模式 命令模式 可视模式 交互模式 也成为正常模式，这是 Vim 的默认模式，每次运行 Vim 程序的时候，就会进入这个模式。
例如执行 vim name.txt 则会进入交互模式。
交互模式特征：
在这个模式下，你不能输入文本； 它可以让我们在文本间移动，删除一行文本，复制黏贴文本，跳转到指定行，撤销操作，等等。 插入模式 这个模式是我们熟悉的文本编辑器的模式，就是可以输入任何你想输入的内容。进入这个模式有几种方法，最常用的方法是按字母键 i （ i、I、a、A、o、O 都可以进入插入模式，只是所处的位置不同），退出这种模式，只需要按下 Esc 键。
i, I 进入输入模式 Insert mode ： i 为“从目前光标所在处输入”， I 为“在目前所在行的第一个非空格符处开始输入”； a, A 进入输入模式 Insert mode ： a 为“从目前光标所在的下一个字符处开始输入”， A 为“从光标所在行的最后一个字符处开始输入”； o, O 进入输入模式 Insert mode ： o 为“在目前光标所在的下一行处输入新的一行”； O 为在目前光标所在处的上一行输入新的一行。 命令模式 命令模式也称为底线命令模式，这个模式下可以运行一些命令例如“退出”，“保存”，等动作。
也可以用这个模式来激活一些 Vim 配置，例如语法高亮，显示行号，等。甚至还可以发送一些命令给终端命令行，例如 ls、cp 。
为了进入命令模式，首先要进入交互模式，再按下冒号键。
用一张图表示三种模式如何切换：
基本操作 打开 Vim 在终端命令行中输入 vim 回车后 Vim 就会被运行起来，也可以用 Vim 来打开一个文件，只需要在 vim 后面再加文件名。如 vim file.name ，如果文件不存在，那么会被创建。
插入 进入文件之后，此时处于交互模式，可以通过输入 i 进入插入模式。
移动 在 Vim 的交互模式下，我们可以在文本中移动光标。
h 向左移动一个字符 j 向下移动一个字符 k 向上移动一个字符 i 向右移动一个字符 当然也可以使用四个方向键进行移动，效果是一样的。
跳至行首和行末 「行首：」 在交互模式下，为了将光标定位到一行的开始位置，只需要按下数字键 0 即可，键盘上的 Home 键也有相同效果。 「行末：」 在交互模式下，为了将光标定位到一行的末尾，只需要按下美元符号键 $ 即可，键盘上的 End 键也有相同效果。 按单词移动 在交互模式下，按字母键 w 可以一个单词一个单词的移动。
退出文件 在交互模式下，按下冒号键 : 进入命令模式，再按下 q 键，就可以退出了。
如果在退出之前又修改了文件，就直接想用 :q 退出 Vim ，那么 Vim 会显示一个红字标明错误信息。此时我们有两个选择：
保存并退出 :wq 或 :x ； 不保存且退出 :q! 。 标准操作 删除字符 在交互模式下，将光标定位到一个你想要删除的字符上，按下字母键 x 你会发现这个字符被删除了。
也可以一次性删除多个字符，只需要在按 x 键之前输入数字即可。
删除（剪切）单词，行 「删除一行」：连按两次 d 来删除光标所在的那一行。 「删除多行」：例如先输入数字 2 ，再按下 dd ，就会删除从光标所在行开始的两行。 「删除一个单词」：将光标置于一个单词的首字母处，然后按下 dw 。 「删除多个单词」：例如先按数字键 2 再按 dw 就可以删除两个单词了。 「从光标所在位置删除至行首」： d0 。 「从光标所在位置删除至行末」： d$ 。 复制单词，行 「复制行」：按两次 y 会把光标所在行复制到内存中，和 dd 类似， dd 用于“剪切”光标所在行。 「复制单词」： yw 会复制一个单词。 「复制到行末」： y$ 是复制从光标所在处到行末的所有字符。 「复制到行首」： y0 是复制光标所在处到行首的所有字符。 粘贴 如果之前用 dd 或者 yy 剪切复制过来的，可以使用 p 来粘贴。同样也可以使用 数字+p 来表示复制多次。
替换一个字符 在交互模式下，将光标置于想要替换的字符上。按下 r 键，接着输入你要替换的字符即可。
撤销操作 如果要撤销最近的修改，只需要按下 u 键，如果想要撤销最近四次修改，可以按下4，再按下 u 。
重做 取消撤销，也就是重做之前的修改使用 ctrl + r 。
跳转到指定行 Vim 编辑的文件中，每一行都有一个行号，行号从1开始，逐一递增。
行号默认是不显示，如果需要它显示的话，可以进入命令模式，然后输入 set nu ，如果要隐藏行号的话，使用 set nonu 。
跳转到指定行： 数字+gg ，例如 7gg ，表示跳转到第7行。 要跳转到最后一行，按下 G 。 要跳转到第一行，按下 gg 。 高级操作 查找 处于交互模式下，按下 / 键，那么就进入查找模式，输入你要查找的字符串，然后按下回车。光标就会跳转到文件中下一个查找到的匹配处。如果字符串不存在，那么会显示 &ldquo;pattern not found&rdquo; 。
n 跳转到下一个匹配项； N 跳转到上一个匹配项。 ?
查找并替换 替换光标所在行第一个匹配的字符串：
# 语法 :s/旧字符串/新字符串 # 实例 :s/one/two 替换光标所在行所有旧字符串为新字符串：
# 语法 :s/旧字符串/新字符串/g 替换第几行到第几行中所有字符串：
# 语法 :n,m s/旧字符串/新字符串/g # 实例 :2,4 s/one/two/g 最常用的就是全文替换了：
# 语法 :%s/旧字符串/新字符串/g 合并文件 可以用冒号 +r ( :r ) 实现在光标处插入一个文件的内容。
:r filename # 可以用Tab键来自动补全另外一个文件的路径 分屏 Vim 有一个特别便捷的功能那就是分屏，可以同时打开好几个文件，分屏之后，屏幕每一块被称为一个 viewport ，表示“视口”。
横向分屏 :sp 文件名 垂直分屏 :vsp 文件名 分屏模式下的快捷键 Ctrl + w 再加 Ctrl + w ，表示从一个 viewport 移动光标到另外一个 viewport ； Ctrl + w 再加 “方向键”，就可以移动到这个方向所处的下一个视口了； Ctrl + w 再加 + 号，表示扩大当前视口； Ctrl + w 再加 - 号，表示缩小当前视口； Ctrl + w 再加 = 号，表示平均当前视口； Ctrl + w 再加 r 键，会反向调换视口位置； Ctrl + w 再加 q 键，会关闭当前视口； Ctrl + w 再加 o 键，会关闭除当前视口以外的所有视口； 运行外部命令 :! 在 Vim 中可以运行一些终端命令，只要先输入 :! ，然后接命令名称。
例如：
:!ls # 在Vim中打开的文件所在的目录运行ls命令 可视模式 前面只讲了 Vim 的三种模式，其实还有一种模式叫做可视模式。
进入它的三种方式（都是从交互模式开始）：
v 字符可视模式，进入后配合方向键选中字符后，然后再按 d 键可以删除选中。 V 行可视模式，进入后光标所在行默认被选中，然后再按 d 键可以删除所在行。 Ctrl + v 块可视模式，它是可视模式最有用的功能了，配合 d 和 I 键可以实现删除选中的内容和插入内容。 同时选中多行，并在选中行头部插入内容的具体操作步骤：
ctrl + v 进入块可视模式 使用方向键进行选中（上下左右）假设选中5行 输入 I 键进行多行同时插入操作 插入完成后连续按两下 esc 键，实现多行同时插入相同字符 进入可视模式之后的操作键：
d 键，表示删除选中； I 键，表示在选中之前插入； u 键，表示选中变为小写； U 键，表示选中变为大写； Vim 配置 选项参数 在 Vim 被启动后，可以运行一些指令来激活一些选项参数，但是这些选项参数的配置在退出 Vim 时会被忘记，例如前面讲解的激活行号。如果希望所在的配置是永久性的，那么需要在家目录（ cd ~ ）创建一个 Vim 的配置文件 .vimrc 。
.vimrc set number " 显示行号 syntax on " 激活语法高亮 set showcmd " 实时看到输入的命令 set ignorecase " 搜索时不区分大小写 set mouse=a " 激活鼠标，用鼠标选中时相当于进入可视模式 Vim 配置非常丰富，我们可以通过个性化配置把 Vim 打造成属于自己的 IDE 等等。在 github 上也可以搜索到一些强大的 Vim 配置文件。
总结 相信通过本文的学习，你应该会对 Linux 有一个更加全面的认识。
最后，实现Linux 命令自由。
参考文献 juejin.cn/post/6938385978004340744
推荐阅读： 《 网易二面：CPU狂飙900%，该怎么处理？
》
《 阿里二面：千万级、亿级数据，如何性能优化？ 教科书级 答案来了
》
《 峰值21WQps、亿级DAU，小游戏《羊了个羊》是怎么架构的？
》
《 场景题：假设10W人突访，你的系统如何做到不 雪崩？
》
《 2个大厂 100亿级 超大流量 红包 架构方案
》
《 Nginx面试题（史上最全 + 持续更新）
》
《 K8S面试题（史上最全 + 持续更新）
》
《 操作系统面试题（史上最全、持续更新）
》
《 Docker面试题（史上最全 + 持续更新）
》
《 Springcloud gateway 底层原理、核心实战 (史上最全)
》
《 Flux、Mono、Reactor 实战（史上最全）
》
《 sentinel （史上最全）
》
《 Nacos (史上最全)
》
《 TCP协议详解 (史上最全)
》
《 分库分表 Sharding-JDBC 底层原理、核心实战（史上最全）
》
《 clickhouse 超底层原理 + 高可用实操 （史上最全）
》
《 nacos高可用（图解+秒懂+史上最全）
》
《 队列之王： Disruptor 原理、架构、源码 一文穿透
》
《 环形队列、 条带环形队列 Striped-RingBuffer （史上最全）
》
《 一文搞定：SpringBoot、SLF4j、Log4j、Logback、Netty之间混乱关系（史上最全）
》
《 单例模式（史上最全）
》
《 红黑树（ 图解 + 秒懂 + 史上最全）
》
《 分布式事务 （秒懂）
》
《 缓存之王：Caffeine 源码、架构、原理（史上最全，10W字 超级长文）
》
《 缓存之王：Caffeine 的使用（史上最全）
》
《 Java Agent 探针、字节码增强 ByteBuddy（史上最全）
》
《 Docker原理（图解+秒懂+史上最全）
》
《 Redis分布式锁（图解 - 秒懂 - 史上最全）
》
《 Zookeeper 分布式锁 - 图解 - 秒懂
》
《 Zookeeper Curator 事件监听 - 10分钟看懂
》
《 Netty 粘包 拆包 | 史上最全解读
》
《 Netty 100万级高并发服务器配置
》
《 Springcloud 高并发 配置 （一文全懂）
》## 推荐阅读：
《 网易二面：CPU狂飙900%，该怎么处理？
》
《 阿里二面：千万级、亿级数据，如何性能优化？ 教科书级 答案来了
》
《 峰值21WQps、亿级DAU，小游戏《羊了个羊》是怎么架构的？
》
《 场景题：假设10W人突访，你的系统如何做到不 雪崩？
》
《 2个大厂 100亿级 超大流量 红包 架构方案
》
《 Nginx面试题（史上最全 + 持续更新）
》
《 K8S面试题（史上最全 + 持续更新）
》
《 操作系统面试题（史上最全、持续更新）
》
《 Docker面试题（史上最全 + 持续更新）
》
《 Springcloud gateway 底层原理、核心实战 (史上最全)
》
《 Flux、Mono、Reactor 实战（史上最全）
》
《 sentinel （史上最全）
》
《 Nacos (史上最全)
》
《 TCP协议详解 (史上最全)
》
《 分库分表 Sharding-JDBC 底层原理、核心实战（史上最全）
》
《 clickhouse 超底层原理 + 高可用实操 （史上最全）
》
《 nacos高可用（图解+秒懂+史上最全）
》
《 队列之王： Disruptor 原理、架构、源码 一文穿透
》
《 环形队列、 条带环形队列 Striped-RingBuffer （史上最全）
》
《 一文搞定：SpringBoot、SLF4j、Log4j、Logback、Netty之间混乱关系（史上最全）
》
《 单例模式（史上最全）
》
《 红黑树（ 图解 + 秒懂 + 史上最全）
》
《 分布式事务 （秒懂）
》
《 缓存之王：Caffeine 源码、架构、原理（史上最全，10W字 超级长文）
》
《 缓存之王：Caffeine 的使用（史上最全）
》
《 Java Agent 探针、字节码增强 ByteBuddy（史上最全）
》
《 Docker原理（图解+秒懂+史上最全）
》
《 Redis分布式锁（图解 - 秒懂 - 史上最全）
》
《 Zookeeper 分布式锁 - 图解 - 秒懂
》
《 Zookeeper Curator 事件监听 - 10分钟看懂
》
《 Netty 粘包 拆包 | 史上最全解读
》
《 Netty 100万级高并发服务器配置
》
《 Springcloud 高并发 配置 （一文全懂）
》</content></entry><entry><title>navicat连接docker环境mysql8.0报1251</title><url>https://codingroam.github.io/post/navicat%E8%BF%9E%E6%8E%A5docker%E7%8E%AF%E5%A2%83mysql8.0%E6%8A%A51251/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>Problem-Solving</tag><tag>Navicat</tag></tags><content type="html"> navicat连接docker环境mysql8.0报1251
docker exec -it mysql bash mysql -uroot -p #输入密码 #进入到mysql use mysql; select host, user, authentication_string, plugin from user; ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456'; (设置远程登录密码123456,密码别设置的太简单，否则知道你服务器ip就随便登录你的mysql了，不然就给服务器3306端口加上来源限制，真的血的教训) // 设置远程连接权限 grant all on *.* to 'root'@'%'; //刷新权限 flush privileges;</content></entry><entry><title>MySQL RR级别下仍然会发生幻读和不可重复读</title><url>https://codingroam.github.io/post/mysql-rr%E7%BA%A7%E5%88%AB%E4%B8%8B%E4%BB%8D%E7%84%B6%E4%BC%9A%E5%8F%91%E7%94%9F%E5%B9%BB%E8%AF%BB%E5%92%8C%E4%B8%8D%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>MVCC</tag><tag>Learning</tag></tags><content type="html"> MySQL RR级别下仍然会发生幻读和不可重复读
前言 InnoDB的RR级别中，MVCC解决了脏读，快照读的不可重复和幻读，但是当快照读和当前读同时使用时，仍然可能会发生不可重复读和幻读。下面就来讲这两个问题什么时候会发生以及如何解决。
RR级别下当前读的幻读问题 回顾一下什么是幻读，幻读就是在一个事务中两次查询某个范围的数据，但查询结果的条数不一样。
发生幻读的场景 user表中的数据如下：
时序图如下所示，在T5时刻发生了幻读：
为什么会发生幻读 在T4时刻，由于update语句采用的是当前读，会对事务2新增的记录进行加锁、修改age字段值、修改DB_TRX_ID隐藏字段值。 在T5时刻使用快照读时，根据可见性算法，这条新增记录的DB_TRX_ID是当前事务，所以是可见的，所以输出了三条记录。 T5时刻输出的记录条数和T1时刻的不一样，这就表示发生了幻读。
如何解决 在MySQL中提供了next-key lock来解决此类幻读问题。我们要在事务中尽可能早的执行select … for update语句，MySQL会对这个查询的范围加next-key lock来防止其他事务插入新的记录。 next-key lock包含两部分：行锁（record lock），间隙锁（gap lock）。行锁的对象是索引记录项，间隙锁的对象是索引项之间的间隙。
加锁后的时序图如下：
注：101 U 102 U (102, +∞)表示 对id=101的索引项加行锁 + 对id=102的索引项加行锁 + 对id在(102,+∞)的间隙加间隙锁
RR级别下的不可重复读问题 回顾一下什么是不可重复读，不可重复读就是在一个事务中两次查询同一条数据，但由于其他事务的修改导致两次看到的数据结果不一样。
发生不可重复度的场景 user表中的数据如下：
时序图如下所示，在T5时刻发生了不可重复读：
为什么会发生不可重复读 在T4时刻，由于update语句采用的是当前读，所以会对这条记录进行加锁、修改name字段值、修改DB_TRX_ID隐藏字段值。 在T5时刻使用快照读时，根据可见性算法，这条记录最新版本的DB_TRX_ID是当前事务，所以是可见的。 T5时刻读到了事务2的修改，这就表示发生了不可重复读。
如何解决 可以使用行锁来解决此类问题。我们要在事务中尽可能早的执行select … for update语句，MySQL会对这个记录加行锁来防止其他事务修改此记录。
加锁后的时序图如下：
注：如果将此场景中的id=1改为id>=1则加的锁是next-key lock。更</content></entry><entry><title>MySQL七种日志介绍</title><url>https://codingroam.github.io/post/mysql%E4%B8%83%E7%A7%8D%E6%97%A5%E5%BF%97%E4%BB%8B%E7%BB%8D/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>底层原理</tag><tag>Learning</tag></tags><content type="html"> MySQL七种日志介绍
进入正题前先简单看看MySQL的逻辑架构，相信我用的着。
MySQL逻辑架构
MySQL的逻辑架构大致可以分为三层：
第一层：处理客户端连接、授权认证，安全校验等。 第二层：服务器server层，负责对SQL解释、分析、优化、执行操作引擎等。 第三层：存储引擎，负责MySQL中数据的存储和提取。 我们要知道MySQL的服务器层是不管理事务的，事务是由存储引擎实现的，而MySQL中支持事务的存储引擎又属InnoDB使用的最为广泛，所以后续文中提到的存储引擎都以InnoDB为主。
MySQL数据更新流程
记住！ 记住！ 记住！ 上边这张图，她是MySQL更新数据的基础流程，其中包括redo log、bin log、undo log三种日志间的大致关系，好了闲话少说直奔主题。
1. redo log（重做日志） redo log属于MySQL存储引擎InnoDB的事务日志。
MySQL的数据是存放在磁盘中的，每次读写数据都需做磁盘IO操作，如果并发场景下性能就会很差。为此MySQL提供了一个优化手段，引入缓存Buffer Pool。这个缓存中包含了磁盘中部分数据页（page）的映射，以此来缓解数据库的磁盘压力。
当从数据库读数据时，首先从缓存中读取，如果缓存中没有，则从磁盘读取后放入缓存；当向数据库写入数据时，先向缓存写入，此时缓存中的数据页数据变更，这个数据页称为脏页，Buffer Pool中修改完数据后会按照设定的更新策略，定期刷到磁盘中，这个过程称为刷脏页。
1.1 MySQL宕机 如果刷脏页还未完成，可MySQL由于某些原因宕机重启，此时Buffer Pool中修改的数据还没有及时的刷到磁盘中，就会导致数据丢失，无法保证事务的持久性。
为了解决这个问题引入了redo log，redo Log如其名侧重于重做！它记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页，且只能恢复到最后一次提交的位置。
redo log用到了WAL（Write-Ahead Logging）技术，这个技术的核心就在于修改记录前，一定要先写日志，并保证日志先落盘，才能算事务提交完成。
有了redo log再修改数据时，InnoDB引擎会把更新记录先写在redo log中，在修改Buffer Pool中的数据，当提交事务时，调用fsync把redo log刷入磁盘。至于缓存中更新的数据文件何时刷入磁盘，则由后台线程异步处理。
注意：此时redo log的事务状态是prepare，还未真正提交成功，要等bin log日志写入磁盘完成才会变更为commit，事务才算真正提交完成。
这样一来即使刷脏页之前MySQL意外宕机也没关系，只要在重启时解析redo log中的更改记录进行重放，重新刷盘即可。
1.2 大小固定 redo log采用固定大小，循环写入的格式，当redo log写满之后，重新从头开始如此循环写，形成一个环状。
那为什么要如此设计呢？
因为redo log记录的是数据页上的修改，如果Buffer Pool中数据页已经刷磁盘后，那这些记录就失效了，新日志会将这些失效的记录进行覆盖擦除。
上图中的write pos表示redo log当前记录的日志序列号LSN(log sequence number)，写入还未刷盘，循环往后递增；check point表示redo log中的修改记录已刷入磁盘后的LSN，循环往后递增，这个LSN之前的数据已经全落盘。
write pos到check point之间的部分是redo log空余的部分（绿色），用来记录新的日志；check point到write pos之间是redo log已经记录的数据页修改数据，此时数据页还未刷回磁盘的部分。当write pos追上check point时，会先推动check point向前移动，空出位置（刷盘）再记录新的日志。
注意：redo log日志满了，在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求的，此刻MySQL的性能会下降。所以在并发量大的情况下，合理调整redo log的文件大小非常重要。
1.3 crash-safe 因为redo log的存在使得Innodb引擎具有了crash-safe的能力，即MySQL宕机重启，系统会自动去检查redo log，将修改还未写入磁盘的数据从redo log恢复到MySQL中。
MySQL启动时，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。会先检查数据页中的LSN，如果这个 LSN 小于 redo log 中的LSN，即write pos位置，说明在redo log上记录着数据页上尚未完成的操作，接着就会从最近的一个check point出发，开始同步数据。
简单理解，比如：redo log的LSN是500，数据页的LSN是300，表明重启前有部分数据未完全刷入到磁盘中，那么系统则将redo log中LSN序号300到500的记录进行重放刷盘。
2. undo log（回滚日志） undo log也是属于MySQL存储引擎InnoDB的事务日志。
undo log属于逻辑日志，如其名主要起到回滚的作用，它是保证事务原子性的关键。记录的是数据修改前的状态，在数据修改的流程中，同时会记录一条与当前操作相反的逻辑日志到undo log中。
我们举个栗子：假如更新ID=1记录的name字段，name原始数据为小富，现改name为程序员内点事
事务执行update X set name = 程序员内点事 where id =1语句时，先会在undo log中记录一条相反逻辑的update X set name = 小富 where id =1记录，这样当某些原因导致服务异常事务失败，就可以借助undo log将数据回滚到事务执行前的状态，保证事务的完整性。
那可能有人会问：同一个事物内的一条记录被多次修改，那是不是每次都要把数据修改前的状态都写入undo log呢？
答案是不会的！
undo log只负责记录事务开始前要修改数据的原始版本，当我们再次对这行数据进行修改，所产生的修改记录会写入到redo log，undo log负责完成回滚，redo log负责完成前滚。
2.1 回滚 未提交的事务，即事务未执行commit。但该事务内修改的脏页中，可能有一部分脏块已经刷盘。如果此时数据库实例宕机重启，就需要用回滚来将先前那部分已经刷盘的脏块从磁盘上撤销。
2.2 前滚 未完全提交的事务，即事务已经执行commit，但该事务内修改的脏页中只有一部分数据被刷盘，另外一部分还在buffer pool缓存上，如果此时数据库实例宕机重启，就需要用前滚来完成未完全提交的事务。将先前那部分由于宕机在内存上的未来得及刷盘数据，从redo log中恢复出来并刷入磁盘。
数据库实例恢复时，先做前滚，后做回滚。
如果你仔细看过了上边的 MySQL数据更新流程图 就会发现，undo log、redo log、bin log三种日志都是在刷脏页之前就已经刷到磁盘了的，相互协作最大限度保证了用户提交的数据不丢失。
3. bin log（归档日志） bin log是一种数据库Server层（和什么引擎无关），以二进制形式存储在磁盘中的逻辑日志。bin log记录了数据库所有DDL和DML操作（不包含 SELECT 和 SHOW等命令，因为这类操作对数据本身并没有修改）。
默认情况下，二进制日志功能是关闭的。可以通过以下命令查看二进制日志是否开启：
mysql> SHOW VARIABLES LIKE 'log_bin'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | OFF | +---------------+-------+ bin log也被叫做归档日志，因为它不会像redo log那样循环写擦除之前的记录，而是会一直记录日志。一个bin log日志文件默认最大容量1G（也可以通过max_binlog_size参数修改），单个日志超过最大值，则会新创建一个文件继续写。
mysql> show binary logs; +-----------------+-----------+ | Log_name | File_size | +-----------------+-----------+ | mysq-bin.000001 | 8687 | | mysq-bin.000002 | 1445 | | mysq-bin.000003 | 3966 | | mysq-bin.000004 | 177 | | mysq-bin.000005 | 6405 | | mysq-bin.000006 | 177 | | mysq-bin.000007 | 154 | | mysq-bin.000008 | 154 | bin log日志的内容格式其实就是执行SQL命令的反向逻辑，这点和undo log有点类似。一般来说开启bin log都会给日志文件设置过期时间（expire_logs_days参数，默认永久保存），要不然日志的体量会非常庞大。
mysql> show variables like 'expire_logs_days'; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | expire_logs_days | 0 | +------------------+-------+ 1 row in set mysql> SET GLOBAL expire_logs_days=30; Query OK, 0 rows affected bin log主要应用于MySQL主从模式（master-slave）中，主从节点间的数据同步；以及基于时间点的数据还原。
3.1 主从同步 通过下图MySQL的主从复制过程，来了解下bin log在主从模式下的应用。
用户在主库master执行DDL和DML操作，修改记录顺序写入bin log; 从库slave的I/O线程连接上Master，并请求读取指定位置position的日志内容; Master收到从库slave请求后，将指定位置position之后的日志内容，和主库bin log文件的名称以及在日志中的位置推送给从库; slave的I/O线程接收到数据后，将接收到的日志内容依次写入到relay log文件最末端，并将读取到的主库bin log文件名和位置position记录到master-info文件中，以便在下一次读取用; slave的SQL线程检测到relay log中内容更新后，读取日志并解析成可执行的SQL语句，这样就实现了主从库的数据一致; 3.2 基于时间点还原 我们看到bin log也可以做数据的恢复，而redo log也可以，那它们有什么区别？
层次不同：redo log 是InnoDB存储引擎实现的，bin log 是MySQL的服务器层实现的，但MySQL数据库中的任何存储引擎对于数据库的更改都会产生bin log。 作用不同：redo log 用于碰撞恢复（crash recovery），保证MySQL宕机也不会影响持久性；bin log 用于时间点恢复（point-in-time recovery），保证服务器可以基于时间点恢复数据和主从复制。 内容不同：redo log 是物理日志，内容基于磁盘的页Page；bin log的内容是二进制，可以根据binlog_format参数自行设置。 写入方式不同：redo log 采用循环写的方式记录；binlog 通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上。 刷盘时机不同：bin log在事务提交时写入；redo log 在事务开始时即开始写入。 bin log 与 redo log 功能并不冲突而是起到相辅相成的作用，需要二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。
4. relay log（中继日志） relay log日志文件具有与bin log日志文件相同的格式，从上边MySQL主从复制的流程可以看出，relay log起到一个中转的作用，slave先从主库master读取二进制日志数据，写入从库本地，后续再异步由SQL线程读取解析relay log为对应的SQL命令执行。
5. slow query log 慢查询日志（slow query log）: 用来记录在 MySQL 中执行时间超过指定时间的查询语句，在 SQL 优化过程中会经常使用到。通过慢查询日志，我们可以查找出哪些查询语句的执行效率低，耗时严重。
出于性能方面的考虑，一般只有在排查慢SQL、调试参数时才会开启，默认情况下，慢查询日志功能是关闭的。可以通过以下命令查看是否开启慢查询日志：
mysql> SHOW VARIABLES LIKE 'slow_query%'; +---------------------+--------------------------------------------------------+ | Variable_name | Value | +---------------------+--------------------------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /usr/local/mysql/data/iZ2zebfzaequ90bdlz820sZ-slow.log | +---------------------+--------------------------------------------------------+ 通过如下命令开启慢查询日志后，我发现iZ2zebfzaequ90bdlz820sZ-slow.log 日志文件里并没有内容啊，可能因为我执行的 SQL 都比较简单没有超过指定时间。
mysql> SET GLOBAL slow_query_log=ON; Query OK, 0 rows affected 上边提到超过 指定时间 的查询语句才算是慢查询，那么这个时间阈值又是多少嘞？我们通过long_query_time参数来查看一下，发现默认是 10 秒。
mysql> SHOW VARIABLES LIKE 'long_query_time'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 这里我们将long_query_time 参数改小为 0.001秒再次执行查询SQL，看看慢查询日志里是否有变化。
mysql> SET GLOBAL long_query_time=0.001; Query OK, 0 rows affected 果然再执行 SQL 的时，执行时间大于 0.001秒，发现慢查询日志开始记录了。
6. general query log 一般查询日志（general query log）：用来记录用户的所有操作，包括客户端何时连接了服务器、客户端发送的所有SQL以及其他事件，比如 MySQL 服务启动和关闭等等。MySQL服务器会按照它接收到语句的先后顺序写入日志文件。
由于一般查询日志记录的内容过于详细，开启后 Log 文件的体量会非常庞大，所以出于对性能的考虑，默认情况下，该日志功能是关闭的，通常会在排查故障需获得详细日志的时候才会临时开启。
我们可以通过以下命令查看一般查询日志是否开启，命令如下：
mysql> show variables like 'general_log'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | general_log | OFF | +---------------+-------+ 下边开启一般查询日志并查看日志存放的位置。
mysql> SET GLOBAL general_log=on; Query OK, 0 rows affected mysql> show variables like 'general_log_file'; +------------------+---------------------------------------------------+ | Variable_name | Value | +------------------+---------------------------------------------------+ | general_log_file | /usr/local/mysql/data/iZ2zebfzaequ90bdlz820sZ.log | +------------------+---------------------------------------------------+ 执行一条查询 SQL 看看日志内容的变化。
mysql> select * from t_config; +---------------------+------------+---------------------+---------------------+ | id | remark | create_time | last_modify_time | +---------------------+------------+---------------------+---------------------+ | 1325741604307734530 | 我是广播表 | 2020-11-09 18:06:44 | 2020-11-09 18:06:44 | +---------------------+------------+---------------------+---------------------+ 我们看到日志内容详细的记录了所有执行的命令、SQL、SQL的解析过程、数据库设置等等。
7. error log 错误日志（error log）: 应该是 MySQL 中最好理解的一种日志，主要记录 MySQL 服务器每次启动和停止的时间以及诊断和出错信息。
默认情况下，该日志功能是开启的，通过如下命令查找错误日志文件的存放路径。
mysql> SHOW VARIABLES LIKE 'log_error'; +---------------+----------------------------------------------------------------+ | Variable_name | Value | +---------------+----------------------------------------------------------------+ | log_error | /usr/local/mysql/data/LAPTOP-UHQ6V8KP.err | +---------------+----------------------------------------------------------------+ 注意：错误日志中记录的可并非全是错误信息，像 MySQL 如何启动 InnoDB 的表空间文件、如何初始化自己的存储引擎，初始化 buffer pool 等等，这些也记录在错误日志文件中。</content></entry><entry><title>Java SPI</title><url>https://codingroam.github.io/post/java-spi/</url><categories><category>Java</category></categories><tags><tag>Java</tag><tag>Java Base</tag><tag>Learning</tag></tags><content type="html"> SPI之ServiceLoader应用与源码分析
在SPI的应用中，Java支持对外开放了一些服务接口（或者抽象类，也可以使用普通类，但是不建议），但是不提供具体的实现，这些实现可以由第三方提供，支持用户以扩展的方式集成到系统中。那么这就需要一种服务发现机制。也就是根据一定的规则到指定的位置找到指定服务的配置文件，然后找到实现类。ServiceLoader便提供了这么一个手段，能够在系统中"指定"位置（META-INF/services）的"指定"文件（文件名是服务全限定名称）中寻找指定服务的三方实现。
比如在servlet容器中，需要支持servlet规范中对于ServletContainerInitializer的使用定义，会在容器启动的时候寻找ServletContainerInitializer的实现类，调用其onStartup方法。那么可以这样寻找其实现类：
ServiceLoader&lt;ServletContainerInitializer> loadedInitializers = ServiceLoader.load(ServletContainerInitializer.class); for (ServletContainerInitializer sci:loadedInitializers){ //do something...... } ServiceLoader的使用方式一般就是先通过load方法得到ServiceLoader实例，然后迭代获取每个实现。我们知道对于增强的for()循环，在编译之后会转变成对应迭代器遍历的字节码实现，所以ServiceLoader需要实现迭代器Iterable接口，事实上也是这样：
public final class ServiceLoader&lt;S> implements Iterable&lt;S>{ ...... } 接下来我们看看load方法是如何实现的：
public static &lt;S> ServiceLoader&lt;S> load(Class&lt;S> service) { //使用TCCL类加载器 ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); } SPI使用的是TCCL类加载器，关于TCCL，在 深入OpenJDK源码全面理解Java类加载器
进行过介绍，这里不再多聊。之所以使用TCCL是因为ServiceLoader定义在核心包中，会被BootStrapClassLoader加载，但是服务实现者却定义在应用jar包中，BootStrapClassLoader无法加载，那根据Java类加载的双亲委派机制，拿这些实现类还没有办法，所以需要通过TCCL的手段实现父类加载器调用子类加载器加载类的需求。 获取了TCCL之后，我们进入ServiceLoader.load(service, cl)方法调用逻辑：
public static &lt;S> ServiceLoader&lt;S> load(Class&lt;S> service, ClassLoader loader){ return new ServiceLoader&lt;>(service, loader); } 直接创建的ServiceLoader实例，那么转到ServiceLoader的构造函数：
private ServiceLoader(Class&lt;S> svc, ClassLoader cl) { //判空 service = Objects.requireNonNull(svc, "Service interface cannot be null"); //如果cl为空，那么从getSystemClassLoader获取，这个方法默认返回AppClassLoader loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; //安全检查 acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload(); } 经过简单的处理之后，会调用reload方法完成服务实现者的发现与加载动作(关于ClassLoader.getSystemClassLoader，在 深入OpenJDK源码全面理解Java类加载器
中有详细分析)，reload方法实现如下：
public void reload() { //清理providers，以便于重新加载服务提供者 providers.clear(); //创建一个LazyIterator lookupIterator = new LazyIterator(service, loader); } 主要根据service（服务）和loader（类加载器）创建了一个LazyIterator，这个LazyIterator看名字就知道是一个懒加载的迭代器，是ServiceLoader的一个私有内部类。前文提到了ServiceLoader实现了Iterable接口，那么我们看看它的iterator();方法是如何实现的：
public Iterator&lt;S> iterator() { return new Iterator&lt;S>() { //providers在reload的时候会清空 Iterator&lt;Map.Entry&lt;String,S>> knownProviders = providers.entrySet().iterator(); public boolean hasNext() { if (knownProviders.hasNext()) return true; return lookupIterator.hasNext(); } public S next() { if (knownProviders.hasNext()) return knownProviders.next().getValue(); return lookupIterator.next(); } public void remove() { throw new UnsupportedOperationException(); } }; } 可以看到，reload之后调用hasNext和next方法实际调用的是lookupIterator，也就是reload方法创建的LazyIterator。所以我们转到LazyIterator对应方法的实现（其中的next方法会调用nextService方法，hasNext方法会调用hasNextService方法，所以这里关注nextService和hasNextService方法就可以了）：
private class LazyIterator implements Iterator&lt;S>{ Class&lt;S> service; ClassLoader loader; Enumeration&lt;URL> configs = null; Iterator&lt;String> pending = null; String nextName = null; private boolean hasNextService() { if (nextName != null) { return true; } if (configs == null) { try { //PREFIX是"META-INF/services/" String fullName = PREFIX + service.getName(); //调用的ClassLoader.getResources方法寻找资源 if (loader == null) configs = ClassLoader.getSystemResources(fullName); else configs = loader.getResources(fullName); } catch (IOException x) { fail(service, "Error locating configuration files", x); } } while ((pending == null) || !pending.hasNext()) { if (!configs.hasMoreElements()) { return false; } //解析找到的资源（URL） pending = parse(service, configs.nextElement()); } //保存到nextName，在nextService方法中加载与实例化 nextName = pending.next(); return true; } private S nextService() { if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?> c = null; try { //加载找到的服务实现 c = Class.forName(cn, false, loader); } catch (ClassNotFoundException x) { fail(service, "Provider " + cn + " not found"); } if (!service.isAssignableFrom(c)) { fail(service, "Provider " + cn + " not a subtype"); } try { //实例化 S p = service.cast(c.newInstance()); //存入providers中 providers.put(cn, p); return p; } catch (Throwable x) { fail(service, "Provider " + cn + " could not be instantiated", x); } throw new Error(); // This cannot happen } } 代码看起来不少，不过核心逻辑就几点：首先在hasNextService方法中根据PREFIX 和serviceName组装一个fullName，对于ServletContainerInitializer的例子，fullName就是：META-INF/services/javax.servlet.ServletContainerInitializer。然后通过ClassLoader提供的getResources方法寻找实现类，最后在nextService方法中完成实现类的加载与实例化操作，还会存入providers中。 注意ClassLoader的getResources方法返回的是一个URL的枚举类（Enumeration，现在基本都使用迭代器了），会在parse方法中解析该URL：
private Iterator&lt;String> parse(Class&lt;?> service, URL u) throws ServiceConfigurationError { InputStream in = null; BufferedReader r = null; ArrayList&lt;String> names = new ArrayList&lt;>(); try { in = u.openStream(); r = new BufferedReader(new InputStreamReader(in, "utf-8")); int lc = 1; while ((lc = parseLine(service, u, r, lc, names)) >= 0); } catch (IOException x) { fail(service, "Error reading configuration file", x); } finally { try { if (r != null) r.close(); if (in != null) in.close(); } catch (IOException y) { fail(service, "Error closing configuration file", y); } } return names.iterator(); } 这个方法逻辑很简单，就不进行说明了，需要注意的是，一个META-INF/services/{serviceName}文件中可以有多个实现类，parse方法会将解析到的全限定类名都存入names集合中，然后返回其迭代器，交给后面的逻辑迭代加载和实例化。 这里还需要提一点的就是ClassLoader的getResources方法，该方法里会调用findResources方法，这个方法在ClassLoader中返回的是一个空集合：
protected Enumeration&lt;URL> findResources(String name) throws IOException { return java.util.Collections.emptyEnumeration(); } 所以需要子类去实现，ExtClassLoader和AppClassLoader都继承了URLClassLoader，在URLClassLoader中有该方法的具体实现，最终是通过URLClassPath完成的资源查找（支持网络字节码和本地文件字节码），最终返回URL枚举类。
ServiceLoader主要依赖一个懒加载迭代器LazyIterator，LazyIterator生成后会在对ServiceLoader进行迭代的时候才进行资源的搜索和解析工作，工作流程总体说来如下：
根据传入的服务名生成该服务按照规范应该所在的文件名，比如：META-INF/services/javax.servlet.ServletContainerInitializer 使用指定的TCCL类加载器(如果为空则是AppClassLoader)寻找对应的资源列表，以枚举(Enumeration)的方式返回 然后遍历找到的资源文件，解析内容得到所有配置的服务实现类 对这些实现类完成加载和实例化的工作，返回实现类实例对象，同时存入ServiceLoader的providers属性中（一个LinkedHashMap，key为name，value为对应的实例对象）</content></entry><entry><title>Java程序使用jPowershell与powershell交互</title><url>https://codingroam.github.io/post/java%E7%A8%8B%E5%BA%8F%E4%BD%BF%E7%94%A8jpowershell%E4%B8%8Epowershell%E4%BA%A4%E4%BA%92/</url><categories><category>Java</category></categories><tags><tag>Java</tag><tag>Learning</tag><tag>Powershell</tag></tags><content type="html"> Java程序使用jPowershell与powershell交互
一、java自带的Runtime 在某些场景下，需要在Java程序中使用Powershell进行终端交互，这种情况下当然可以直接使用自带的Runtime来完成：Runtime.getRuntime().exec(“powershell.exe Get-Item”); 但是这种只适合需要单条指令的情况，而存在多条指令时，无法保证前后指令的关联性。这里介绍一个第三方的类库：jPowershell
二、jPowershell 1、 Maven依赖： &lt;dependency> &lt;groupId>com.profesorfalken&lt;/groupId> &lt;artifactId>jPowerShell&lt;/artifactId> &lt;version>3.1.1&lt;/version> &lt;/dependency> 2、 程序代码 获取Powershell的实例：PowerShell session = PowerShell.openSession(); PowerShell对象提供了如下的基础方法： configuration(Map&lt;String,String> arg0)：指定配置对象 void 修改默认配置时使用 openSession()：PowerShell 启动一个PowerShell会话，在一个会话内可以与powershell交互多次命令 openSession(String arg0)：指定PowerShell终端的路径 PowerShell 启动一个PowerShell会话 executeCommand(String arg0)：Powershell指令，返回值PowerShellResponse是执行结果和信息 executeSingleCommand()：执行单一PowerShell指令（无会话模式） executeCommandAndChain(String arg0,PowerShellResponseHandler.. arg1)：PowerShell指令，Response处理器 PowerShell 执行一条PowerShell指令并对其Response进行处理 handleResponse(PowerShellResponseHandler arg0,PowerShellResponse arg1)：Response处理器，Response void 使用指定Response处理器处理响应 isLastCommandInError()：boolean 最后一条指令是否出错 executeScript(String arg0)：脚本路径 PowerShellResponse 执行指定的脚本 executeScript(String arg0, String arg1)：脚本路径，执行参数 PowerShellResponse 传入参数执行指定脚本 executeScript(BufferedReader arg0)：脚本缓冲对象 PowerShellResponse 从缓冲对象中执行脚本 executeScript(BufferedReader arg0, String arg1)：脚本缓冲对象，执行参数PowerShellResponse 传入参数执行缓冲对象中的脚本 close() ： void 关闭会话 closeAndWait(Future&lt;String> arg0)： 异步对象列表 boolean 等待异步对象列表中全部Task完成后关闭会话 checkState() ：void 检查当前会话状态 获取到的session对象是一个新的Powershell会话实例，可以通过下述方式来执行指令：
PowerShell session = PowerShell.openSession(); String readContext = "Get-Content D:\\1.sql"; session.executeCommand("$user = \""+username+"\""); session.executeCommand("$password = ConvertTo-SecureString \"" +password + "\" -AsPlainText -Force"); session.executeCommand("$credential = New-Object System.Management.Automation.PSCredential($user,$password)"); response = session.executeCommand("Invoke-Command -ComputerName " + server + " -Credential $credential -ScriptBlock {" + readContext + "}"); System.out.println(response.getCommandOutput().length()); 上述代码中，首先定义了两个shell变量，u s e r 和 user和 user和password，而后通过调用System.Management.Automation.PSCredential构造了一个$credential对象，再将其作为-Credential的值传入下一条指令中，完成远程Invoke-Command进行文件内容长度的读取。
每一条session.executeCommand()都会返回一个PowerShellResponse对象，可以通过调用这个对象的getCommandOutput()方法来获取命令输出。注意看日志最后的数字：
当然，在某些情况下，调用的指令执行时间会很长，在默认情况下，Powershell实例只会等待10秒钟，如果10秒钟之后还没有返回值，就直接跳到下一条继续执行了。这种情况下，可以通过配置Powershell的实例来修改等待时间。来看源码：
public class PowerShell implements AutoCloseable { private static final Logger logger = Logger.getLogger(PowerShell.class.getName()); private Process p; private long pid = -1L; private PrintWriter commandWriter; private boolean closed = false; private ExecutorService threadpool; private static final String DEFAULT_WIN_EXECUTABLE = "powershell.exe"; private static final String DEFAULT_LINUX_EXECUTABLE = "powershell"; private int waitPause = 5; private long maxWait = 10000L; private File tempFolder = null; private boolean scriptMode = false; public static final String END_SCRIPT_STRING = "--END-JPOWERSHELL-SCRIPT--"; private PowerShell() { } public PowerShell configuration(Map&lt;String, String> config) { try { this.waitPause = Integer.valueOf(config != null &amp;&amp; config.get("waitPause") != null ? (String)config.get("waitPause") : PowerShellConfig.getConfig().getProperty("waitPause")); this.maxWait = Long.valueOf(config != null &amp;&amp; config.get("maxWait") != null ? (String)config.get("maxWait") : PowerShellConfig.getConfig().getProperty("maxWait")); this.tempFolder = config != null &amp;&amp; config.get("tempFolder") != null ? this.getTempFolder((String)config.get("tempFolder")) : this.getTempFolder(PowerShellConfig.getConfig().getProperty("tempFolder")); } catch (NumberFormatException var3) { logger.log(Level.SEVERE, "Could not read configuration. Using default values.", var3); } return this; } } Powershell提供了一个configuration()方法，这个方法接受一个Map类型的配置参数，并将内容解析后代替默认参数。我们主要关注的是maxWait的值。通过成员定义可以看出，默认的maxWait的值就是10000毫秒，也就是10秒。因此，可以通过如下代码来使其支持更长时间指令的调用：
PowerShell session = PowerShell.openSession(); Map&lt;String,String> configMap = new HashMap&lt;>(); configMap.put("maxWait","600000"); session.configuration(configMap); 这样就把指令的最长等待时间设置为了600秒。</content></entry><entry><title>Maven工程打jar包的N种方式</title><url>https://codingroam.github.io/post/maven%E5%B7%A5%E7%A8%8B%E6%89%93jar%E5%8C%85%E7%9A%84n%E7%A7%8D%E6%96%B9%E5%BC%8F/</url><categories><category>Maven</category></categories><tags><tag>Maven</tag><tag>Jar</tag><tag>Hands-on</tag></tags><content type="html"> Maven工程打jar包的N种方式
Maven工程打jar包 一、IDEA自带打包插件 二、maven插件打包 2.1 制作瘦包（直接打包，不打包依赖包） 2.2 制作瘦包和依赖包（相互分离） 2.3 制作胖包（项目依赖包和项目打为一个包） 2.4 制作胖包（transform部分自定义） 三、SpringBoot项目打包 四、Scala项目打包 五、groovy项目打包
内容：此种方式可以自己选择制作胖包或者瘦包，但推荐此种方式制作瘦包。 输出：输出目录在out目录下 流程步骤：
第一步： 依次选择 file->projecct structure->artifacts->点击+ (选择jar)->选择 from module with dependencies 第二步：弹出窗口中指定Main Class，是否选择依赖jar包，是否包含测试。（尽量不选依赖包，防止依赖包选择不全） ![img](https://img-blog.csdnimg.cn/20201127122706462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center)![img](https://img-blog.csdnimg.cn/20201127122723284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center)![img](https://img-blog.csdnimg.cn/20201127122732840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center)![img](https://img-blog.csdnimg.cn/20201127122750258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center) 3. 第三步：点击Build–>Build Artifacts–>选择bulid 输出：输出目录在target目录下
2.1 制作瘦包（直接打包，不打包依赖包） 内容：仅打包出项目中的代码到JAR包中。 方式：在pom.xml中添加如下plugin; 随后执行maven install
&lt;!-- java编译插件 --> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-compiler-plugin&lt;/artifactId> &lt;version>指定版本&lt;/version> &lt;configuration> &lt;source>1.8&lt;/source> &lt;target>1.8&lt;/target> &lt;encoding>UTF-8&lt;/encoding> &lt;/configuration> &lt;/plugin> 2.2 制作瘦包和依赖包（相互分离） 内容：将依赖JAR包输出到lib目录方式（打包方式对于JAVA项目是通用的） 将项目中的JAR包的依赖包输出到指定的目录下,修改outputDirectory配置，如下面的${project.build.directory}/lib。 方式：
pom.xml的build>plugins中添加如下配置。 点击maven project（右边栏）->选择Lifecycle->点击package打包 注意：如果想将打包好的JAR包通过命令直接运行，如java -jar xx.jar。需要制定manifest配置的classpathPrefix与上面配置的相对应。如上面把依赖JAR包输出到了lib，则这里的classpathPrefix也应指定为lib/；同时，并指定出程序的入口类，在配置mainClass节点中配好入口类的全类名。 &lt;plugins> &lt;!-- java编译插件 --> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-compiler-plugin&lt;/artifactId> &lt;configuration> &lt;source>1.8&lt;/source> &lt;target>1.8&lt;/target> &lt;encoding>UTF-8&lt;/encoding> &lt;/configuration> &lt;/plugin> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-jar-plugin&lt;/artifactId> &lt;configuration> &lt;archive> &lt;manifest> &lt;addClasspath>true&lt;/addClasspath> &lt;classpathPrefix>lib/&lt;/classpathPrefix> &lt;mainClass>com.yourpakagename.mainClassName&lt;/mainClass> &lt;/manifest> &lt;/archive> &lt;/configuration> &lt;/plugin> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-dependency-plugin&lt;/artifactId> &lt;executions> &lt;execution> &lt;id>copy&lt;/id> &lt;phase>install&lt;/phase> &lt;goals> &lt;goal>copy-dependencies&lt;/goal> &lt;/goals> &lt;configuration> &lt;outputDirectory>${ project.build.directory}/lib&lt;/outputDirectory> &lt;/configuration> &lt;/execution> &lt;/executions> &lt;/plugin> &lt;/plugins> 注意：默认的classpath会在jar包内。为了方便,可以在Main方法配置后加上manifestEntries配置，指定classpath。
&lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-jar-plugin&lt;/artifactId> &lt;configuration> &lt;classesDirectory>target/classes/&lt;/classesDirectory> &lt;archive> &lt;manifest> &lt;!-- 主函数的入口 --> &lt;mainClass>com.yourpakagename.mainClassName&lt;/mainClass> &lt;!-- 打包时 MANIFEST.MF文件不记录的时间戳版本 --> &lt;useUniqueVersions>false&lt;/useUniqueVersions> &lt;addClasspath>true&lt;/addClasspath> &lt;classpathPrefix>lib/&lt;/classpathPrefix> &lt;/manifest> &lt;manifestEntries> &lt;Class-Path>.&lt;/Class-Path> &lt;/manifestEntries> &lt;/archive> &lt;/configuration> &lt;/plugin> 2.3 制作胖包（项目依赖包和项目打为一个包） 内容：将项目中的依赖包和项目代码都打为一个JAR包 方式：
pom.xml的build>plugins中添加如下配置； 点击maven project（右边栏）->选择Plugins->选择assembly->点击assembly:assembly 注意：1. 针对传统的JAVA项目打包； 2. 打包指令为插件的assembly命令，尽量不用package指令。 &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-assembly-plugin&lt;/artifactId> &lt;version>2.5.5&lt;/version> &lt;configuration> &lt;archive> &lt;manifest> &lt;mainClass>com.xxg.Main&lt;/mainClass> &lt;/manifest> &lt;/archive> &lt;descriptorRefs> &lt;descriptorRef>jar-with-dependencies&lt;/descriptorRef> &lt;/descriptorRefs> &lt;/configuration> &lt;/plugin> 2.4 制作胖包（transform部分自定义） &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-shade-plugin&lt;/artifactId> &lt;version>2.4.3&lt;/version> &lt;executions> &lt;execution> &lt;phase>package&lt;/phase> &lt;goals> &lt;goal>shade&lt;/goal> &lt;/goals> &lt;configuration> &lt;filters> &lt;filter> &lt;artifact>*:*&lt;/artifact> &lt;excludes> &lt;exclude>META-INF/*.SF&lt;/exclude> &lt;exclude>META-INF/*.DSA&lt;/exclude> &lt;exclude>META-INF/*.RSA&lt;/exclude> &lt;/excludes> &lt;/filter> &lt;/filters> &lt;transformers> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"> &lt;resource>META-INF/spring.handlers&lt;/resource> &lt;/transformer> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"> &lt;resource>META-INF/spring.schemas&lt;/resource> &lt;/transformer> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"> &lt;resource>META-INF/spring.tooling&lt;/resource> &lt;/transformer> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"> &lt;mainClass>com.xxx.xxxInvoke&lt;/mainClass> &lt;/transformer> &lt;/transformers> &lt;minimizeJar>true&lt;/minimizeJar> &lt;shadedArtifactAttached>true&lt;/shadedArtifactAttached> &lt;/configuration> &lt;/execution> &lt;/executions> &lt;/plugin> 内容：将当前项目里所有依赖包和当前项目的源码都打成一个JAR包，同时还会将没有依赖包的JAR包也打出来，以.original保存 方式：
在pom.xml的build>plugins中加入如下配置 点击maven project（右边栏）->选择Lifecycle->点击package或install打包 &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> 方式：
在pom.xml的build>plugins中加入如下配置 点击maven project（右边栏）->选择Lifecycle->点击package或install打包 &lt;plugin> &lt;groupId>org.scala-tools&lt;/groupId> &lt;artifactId>maven-scala-plugin&lt;/artifactId> &lt;executions> &lt;execution> &lt;goals> &lt;goal>compile&lt;/goal> &lt;goal>testCompile&lt;/goal> &lt;/goals> &lt;/execution> &lt;/executions> &lt;configuration> &lt;scalaVersion>${ scala.version}&lt;/scalaVersion> &lt;args> &lt;arg>-target:jvm-1.5&lt;/arg> &lt;/args> &lt;/configuration> &lt;/plugin> 方式：
在pom.xml的build>plugins中加入如下配置 点击maven project（右边栏）->选择Lifecycle->点击package或install打包 &lt;plugin> &lt;groupId>org.codehaus.gmavenplus&lt;/groupId> &lt;artifactId>gmavenplus-plugin&lt;/artifactId> &lt;version>1.2&lt;/version> &lt;executions> &lt;execution> &lt;goals> &lt;goal>addSources&lt;/goal> &lt;goal>addStubSources&lt;/goal> &lt;goal>compile&lt;/goal> &lt;goal>execute&lt;/goal> &lt;/goals> &lt;/execution> &lt;/executions> &lt;/plugin></content></entry><entry><title>MySQL总结--MVCC（read view和undo log）</title><url>https://codingroam.github.io/post/mysql%E6%80%BB%E7%BB%93--mvccread-view%E5%92%8Cundo-log/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>MVCC</tag><tag>Learning</tag></tags><content type="html"> MySQL总结&ndash;MVCC（read view和undo log）
MVCC(Multi-Version Concurrency Control)，即多版本并发控制，数据库通过它能够做到遇到并发读写的时候，在不加锁的前提下实现安全的并发读操作，是一种乐观锁的实现方式，能大大提高数据库的并发性能。
当前读：读取的是记录的最新版本，需要保证其它事务不能修改读取记录，所以会对记录进行加锁。比如 select for update、select lock in share mode、update等，都属于当前读。 快照读：基于MVCC实现的读，不对读操作加任何锁，读取的时候根据版本链和read view进行可见性判断，所以读取的数据不一定是数据库中的最新值。注意在串行化隔离级别下，读操作也会加锁，所以属于当前读。 undo log日志版本链 undo log是一种逻辑日志，当一个事务对记录做了变更操作就会产生undo log，也就是说undo log记录了记录变更的逻辑过程。当一个事务要更新一行记录时，会把当前记录当做历史快照保存下来，多个历史快照会用两个隐藏字段trx_id和roll_pointer串起来（关于隐藏字段，这里不用考虑隐式主键id:DB_ROW_ID），形成一个历史版本链。可以用于MVCC和事务回滚。 比如多个事务对id为1的数据做了更新，会形成如下图这种历史版本链： 注：更多关于undo log的内容，在MySQL–buffer pool、redo log、undo log、binlog中进行了较为详细的介绍，这里不再赘述。
read view（读视图）与可见性判断 在MySQL中，一个事务开启(注意这里说的不是传统意义上的开启，在InnoDB中，begin/start一个事务，并不会立即分配事务id，而是真正执行了操作才会分配事务id)的时候会被分配一个递增的ID。在事务执行快照读的时候，会将此时数据库中所有的活跃事务（未提交事务）id列表和已创建的最大事务id(+1)生成一个视图快照，如果是在可重复读隔离级别下，这个快照在此事务活跃期间不会变化，如果是读已提交隔离级别下，每次快照读都会重新生成。我们从read view中获取两个属性：
min_trx_id：read view生成时，活跃事务id列表中的最小id max_trx_id：read view生成时，数据库即将分配的事务id，也就是当前已创建最大事务id+1 一个事务在对一行数据做读取操作的时候，会从undo log历史版本链中从最新版本开始往前比对，通过一系列的规则，根据快照版本中的trx_id字段和read view来确定该版本对于当前事务是否可见，如果当前比对版本不可见，那么就通过roll_pointer找到上一个版本进行比对，直到找到可见版本或找不到任何一个可见版本。这些规则定义如下：
如果 trx_id &lt; min_trx_id，则说明该版本对于当前事务(read view)来说，是已提交事务生成的，那么对于当前事务可见。 如果trx_id >= max_trx_id：则说明该版本对于当前事务(read view)来说，是"将来"的事务生成的，那么对于当前事务不可见。 如果min_trx_id &lt;= trx_id &lt; max_trx_id： - 如果trx_id在read view的**活跃事务id列表**中，则说明该版本对于当前事务(read view)来说，是已开始但未提交的事务生成的，那么对于当前事务**不可见**。 - 如果trx_id不在read view的**活跃事务id列表**中，则说明该版本对于当前事务(read view)来说，是已提交的事务生成的，那么对于当前事务**可见**。 注：当前事务id(current_trx_id)也会在活跃事务id列表中，如果undo log是由当前事务生成的，也就是trx_id == current_trx_id，那么此版本对于当前事务来说当然可见
另外，在前面undo log的文章中提到过，InnoDB对于删除操作，其实并不是直接删除数据，而是“相当于”一个update操作，也会生成一个对应删除事务的update undo log，只是将delete mark设置为1，之后会由purge线程清理。当根据上述规则比对时发现delete mark为1，就代表该记录已被删除，没有数据。
事务id和可见性 前面提到了，在InnoDB中，begin/start一个事务并不会立即分配事务id，而是真正执行了操作才会分配事务id。这会有什么现象呢？现假设事务A和事务B根据下图时间线执行： 虽然事务A先begin，但是它在执行do select的时候仍然能看到事务B提交的数据，因为事务在begin的时候并没有真正开始一个事务，事务A的read view是在do select的时候生成的，此时事务B对数据修改的版本快照按照规则来说就属于：trx_id &lt; min_trx_id，属于已提交事务生成，也就对于事务A来说可见。</content></entry><entry><title>Spring源码-Bean生命周期总结</title><url>https://codingroam.github.io/post/spring%E6%BA%90%E7%A0%81bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%80%BB%E7%BB%93/</url><categories><category>Spring</category></categories><tags><tag>Spring</tag><tag>源码</tag><tag>Learning</tag></tags><content type="html"> Spring系列-Bean的生命周期
BeanFactory是Spring中的顶层接口，只负责管理bean，而ApplicationContext也实现了BeanFactory，提供了管理bean的功能，不过除了管理bean之外，它还提供了一些其它功能，比如依赖注入、事件发布等。只是ApplicationContext的bean管理功能也是通过BeanFactory提供的，在实现上就是ApplicationContext持有一个BeanFactory的引用，比如GenericApplicationContext中持有一个DefaultListableBeanFactory类型的beanFactory引用。
在Spring容器中，一个Bean会首先被组装成一个BeanDefinition，然后将其注册到容器中，之后才会被实例化和初始化。Spring将这个过程细化成了很多步骤，在这些步骤中穿插了很多的后置处理器（PostProcessor）对各个环节进行拦截和扩展，以此实现了Spring极高的扩展性。 对于BeanDefinitionRegistry来说，它提供了注册BeanDefinition的接口，实现该接口表示了其具备注册BeanDefinition的能力。而BeanDefinitionReader的存在则为根据资源批量注册BeanDefinition提供了可能。从代码层面上看，就是BeanDefinitionReader能够通过loadBeanDefinitions方法根据一系列的Resource，创建BeanDefinition，然后通过BeanDefinitionRegistry进行注册。从面向对象的角度来看，就是BeanDefinitionReader依赖一些资源（Resource）创建BeanDefinition，并且依赖BeanDefinitionRegistry实现BeanDefinition的批量注册。 不过在Spring源码对于BeanDefinitionReader的注释中有明确的说明，BeanDefinitionReader虽然作为一个顶层接口提供了从资源批量创建注册BeanDefinition的接口。但是在具体的实现中，一个BeanDefinitionReader并不一定非要实现BeanDefinitionReader接口，而是在命名上遵循这个标准命名约定就行了，所以在实现上也可能并不是严格按照BeanDefinitionReader中定义的接口来进行BeanDefinition的注册的，Spring没有严格限制BeanDefinition注册的来源和方式，不论是对自己还是对开发者。 比如以我们最熟悉的AnnotatedBeanDefinitionReader来说，它是一个BeanDefinitionReader，但是实际上它并没有实现BeanDefinitionReader接口。在它的逻辑中，向BeanFactory注册了一些初始的PostProcessor，依赖这些PostProcessor来代替以前XML相关的操作。 比如在非Web环境下，我们如果要基于注解来使用Spring，则会创建一个AnnotationConfigApplicationContext，并且传入一个配置类。在AnnotationConfigApplicationContext的逻辑中就会创建一个前面提到的AnnotatedBeanDefinitionReader，而在AnnotatedBeanDefinitionReader初始化的过程中，会向容器注册了一个很重要的BeanDefinitionRegistryPostProcessor：ConfigurationClassPostProcessor。这个PostProcessor负责解析我们传入的配置类，比如解析@ComponentScan注解，然后根据ClassPathBeanDefinitionScanner从@ComponentScan注解配置的扫描包路径中扫描出合法的资源，最后创建BeanDefinition，注册到容器中，这个相当于是一个"另类"的BeanDefinition创建和注册逻辑，而这个逻辑会在后面的源码分析中详细总结。
概述 一个被Spring管理的Bean，在加载之后首先会被包装成一个BeanDefinition，BeanDefinition代表了Spring中一个Bean的各种基础属性（元数据），比如beanName、作用域、是否懒加载、所属类class、自动装配类型等等，同时还包含了初始化、依赖注入等需要的数据，Spring根据BeanDefinition来实例化和初始化bean。 类被包装成BeanDefinition之后，会存放在BeanFactory的一个ConcurrentHashMap结构的beanDefinitionMap中，key为bean的名称，value为BeanDefinition。Spring容器在刷新的时候，会首先把所有的BeanDefinition都创建缓存完毕，然后调用一系列的后置处理器对其进行加工，此处和BeanDefinition相关的后置处理器主要有两种，分别是：
BeanFactoryPostProcessor BeanDefinitionRegistryPostProcessor：继承了BeanFactoryPostProcessor 首先，通过BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法可以向BeanFactory中继续注册BeanDefinition，之后通过BeanFactoryPostProcessor可以将BeanFactory中的BeanDefinition中取出来做进一步的加工，当这两种后置处理器处理完成之后，会冻结所有已经注册的BeanDefinition的元数据，这表示这些已经注册的BeanDefinition的元数据信息不能再被修改，接下来就要开始实例化。 现在有了BeanDefinition，就代表有了创建Bean的模板，就可以开始创建非懒加载的单例Bean了，在创建Bean的过程中也有几个关键相关的后置处理器调用，分别是：
BeanPostProcessor MergedBeanDefinitionPostProcessor：继承了BeanPostProcessor InstantiationAwareBeanPostProcessor：继承了BeanPostProcessor 接下来进入创建bean的流程：循环BeanFactory中的BeanDefinitionMap，实例化所有单例bean，在这个过程中会先把beanName放入到一个Set中标识当前bean正在创建（此处涉及到循环依赖和三级缓存的问题，这个后面再进行详细说明），然后循环调用InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation方法（这里还涉及到AOP的部分实现，后面再详细说明）拦截bean的实例化，如果有某一个InstantiationAwareBeanPostProcessor返回了一个实例对象，那么就说明bean的实例化被拦截了，那么接着循环调用BeanPostProcessor的postProcessAfterInitialization方法，这时候代表实例对象创建成功，并且完成了初始化。 否则说明bean的实例化没有被拦截，那么根据BeanDefinition获取Bean的构造方式进行对象的实例化，这个实例化的过程很复杂，涉及到构造器推断等，这里面也涉及到后置处理器的调用，比如构造器函数上的@Autowired支持和方法上的@Lookup注解支持都是在这里实现的，涉及到的是AutowiredAnnotationBeanPostProcessor的determineCandidateConstructors方法调用。这个方法实际上归属于SmartInstantiationAwareBeanPostProcessor，包括三级缓存中的getEarlyBeanReference方法也属于这个后置处理器，它是后置处理器InstantiationAwareBeanPostProcessor的一个扩展。实例化之后会循环调用所有MergedBeanDefinitionPostProcessor的postProcessMergedBeanDefinition方法，在这个后置处理器中可以根据实例化之后的对象对BeanDefinition进行进一步的扩展， 这个行为发生在对象实例化之后，初始化之前，所以可以为后续的初始化操作准备一些必要的数据，同时也支持修改BeanDefinition中的部分数据，比如@Autowired注解支持的AutowiredAnnotationBeanPostProcessor后置处理器在这里对@Autowired进行预解析，找到并缓存需要注入的点。
注：此时生产出来的bean还只是一个"空的"，还没有进行任何初始化操作，称之为早期对象，会将其放入三级缓存中，这个概念在解决循环依赖的实现中有着重要作用，注意三级缓存存放的不是bean，而是一个函数接口~
接下来对bean进行属性填充（populateBean），不过在填充之前，还会循环调用所有InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation，允许在Bean实例化之后对Bean的属性填充进行拦截，该方法返回值为布尔类型，如果有任何一个InstantiationAwareBeanPostProcessor的该方法处理bean的时候返回false，那么会中断后续的bean属性填充操作。通过这个后置处理器可以实现对bean中的字段做自定义填充，不过这个在日常工作中基本不会使用，在spring源码中实现该方法的类都默认返回的true。如果没有被postProcessAfterInstantiation方法拦截，那么就会进行bean的属性填充操作。属性填充的过程是先将需要的数据封装成一个PropertyValues，然后调用所有InstantiationAwareBeanPostProcessor的postProcessProperties方法和postProcessPropertyValues方法对PropertyValues在应用到bean之前进行进一步的加工和合法性判断等，然后将PropertyValues应用到bean中。 在bean的属性填充之后，就来到了真正开始正式初始化bean的环节，而在初始化bean之前，会先调用容器默认的3个Aware方法，分别是BeanNameAware、BeanClassLoaderAware和BeanFactoryAware，如果bean实现了这三个接口，那么会带着各自的参数调用它们各自的方法。接着调用所有BeanPostProcessor的postProcessBeforeInitialization方法，该方法允许对已经经过属性填充的bean进行进一步包装或者属性更新和扩展，方法返回值就是后续需要使用的bean，如果有任何一个处理器返回null，那么后续的处理器都不会被调用，比如@PostConstruct注解标识的初始化方法就是依赖该后置处理器实现的。接下来调用bean的初始化方法，bean的初始化方法除了刚刚提到的使用@PostConstruct注解标识外，还可以通过实现InitializingBean接口和xml中指定init-method方法，所以在通过后置处理器处理了@PostConstruct注解之后，就需要调用通过另外两种方式指定的初始化方法，顺序是先执行InitializingBean接口的afterPropertiesSet方法，再调用指定的init-method方法，当然如果init-method方法就是afterPropertiesSet，则只会调用一次。 到这里一个bean就算实例化+初始化完成了，那么接下来调用所有BeanPostProcessor的postProcessAfterInitialization方法，这个方法又允许对已经初始化后的bean做进一步的包装处理，比如创建动态代理（注意这里是普通情况下的动态代理，如果循环依赖遇上了动态代理，那么动态代理在三级缓存中就创建了）。到这步就表示bean已经初始化完成，如果该bean实现了DisposableBean、AutoCloseable接口或者指定了destroyMethod等支持容器回调bean的销毁方法，那么就将该bean包装成一个DisposableBeanAdapter，放到一个名为disposableBeans的LinkedHashMap中，在容器关闭的时候会调用bean的销毁方法。 最后，如果bean实现了SmartInitializingSingleton接口，那么还会调用SmartInitializingSingleton的afterSingletonsInstantiated方法。
注： 对于FactoryBean来说，容器刷新的以后在单例缓存池中保存的实例对象为原对象，而当调用getBean获取该对象的时候，如果发现该对象是一个FactoryBean，那么会尝试去从factoryBeanObjectCache缓存池中获取真实的bean，如果没有获取到，那么调用getBean方法创建真实对象，然后将该对象放入到factoryBeanObjectCache缓存池，之后直接从缓存中获取。如果要获取原对象，那么在调用getBean的时候在传入的beanName前面加上"&amp;&ldquo;字符，spring会根据该字符判断是获取真实对象还是原对象。在spring中实现BeanFactory注册和管理功能的类是FactoryBeanRegistrySupport，而我们所使用的BeanFactory继承了这个抽象类，所以具备这能力。 在Spring的类体系中，将很多行为都抽象为了一个一个的接口或者抽象类，一个类如果想要具备某个行为能力，那么就继承对应的抽象类或者实现对应的接口。
最后就是bean的销毁流程，这个发生在容器关闭的时候，在销毁的时候也涉及到后置处理器的调用：DestructionAwareBeanPostProcessor（继承自BeanPostProcessor）。那么销毁流程是怎么样的呢？我们以实现DisposableBean接口为例，在前面已经提到过，在bean初始化完成之后，如果bean实现了DisposableBean接口，那么会将bean包装成一个DisposableBeanAdapter缓存到disposableBeans中，容器在关闭的时候，会将disposableBeans中的bean都取出来，调用其destroy方法，而在DisposableBeanAdapter的destroy方法中，就会调用所有DestructionAwareBeanPostProcessor的postProcessBeforeDestruction方法，@PreDestroy注解就是依赖该后置处理器实现的，最后调用bean的destroy方法完成bean的销毁。
bean生命周期总结 扫描class创建BeanDefinition BeanDefinitionRegistryPostProcessor#postProcessBeanDefinitionRegistry BeanFactoryPostProcessor#postProcessBeanFactory InstantiationAwareBeanPostProcessor#postProcessBeforeInstantiation（实例化之前，可以拦截bean的实例化） 实例化bean MergedBeanDefinitionPostProcessor#postProcessMergedBeanDefinition（根据实例化bean进一步扩展BeanDefinition，比如解析缓存需要依赖注入的属性等） InstantiationAwareBeanPostProcessor#postProcessAfterInstantiation（实例化之后，可以拦截属性填充）：该方法在populateBean方法中调用 bean属性填充：populateBean方法，先通过InstantiationAwareBeanPostProcessor的postProcessPropertyValues方法将需要注入的属性值封装到PropertyValues的pvs中，然后通过applyPropertyValues方法设置到实例对象中 三个Aware接口的调用，分别是BeanNameAware、BeanClassLoaderAware和BeanFactoryAware（还有很多其他Aware是通过后置处理器ApplicationContextAwareProcessor的postProcessBeforeInitialization实现的） BeanPostProcessor#postProcessBeforeInitialization（初始化之前，@PostConstruct注解在这里实现） 执行InitializingBean接口的afterPropertiesSet方法 执行xml指定的init-method BeanPostProcessor#postProcessAfterInitialization（初始化之后，可以在这里创建bean的代理） 如果bean支持销毁回调，包装DisposableBeanAdapter并缓存 SmartInitializingSingleton#afterSingletonsInstantiated方法（该方法在单例bean完全初始化完成后调用，用于实现最后阶段的自定义初始化操作，可以看做是InitializingBean的替代方案） bean销毁 取出前面缓存的DisposableBeanAdapter调用其destroy方法，在这个适配器的destroy方法中会触发DestructionAwareBeanPostProcessor的postProcessBeforeDestruction方法执行 调用DisposableBean的destroy方法 调用destroyMethod 注：Spring依赖注入主要依靠了后置处理器AutowiredAnnotationBeanPostProcessor，它相当于一个复合后置处理器，实现了多个后置处理器的行为，穿插运行于bean生命周期的几个阶段。比如，determineCandidateConstructors方法（实例化之前）：解析@Lookup注解，检查构造方法中的 @Autowired或@Value注解；postProcessMergedBeanDefinition（实例化之后，populateBean之前）：执行findAutowiringMetadata逻辑，收集@Autowired或@Value注解方法字段等存入缓存；postProcessPropertyValues（实例化之后，populateBean）：处理依赖注入对象，为属性赋值做准备等
Spring容器核心方法之invokeBeanFactoryPostProcessors方法 这个方法主要做的工作就是先调用BeanDefinitionRegistryPostProcessor，再调用BeanFactoryPostProcessor，只是调用的时候会根据是否实现PriorityOrdered接口、是否实现Ordered接口等确定调用顺序。 另外还需要注意的是，在AbstractApplicationContext的invokeBeanFactoryPostProcessors方法中，会先从ApplicationContext中获取已有的BeanFactoryPostProcessor（当然也包括BeanDefinitionRegistryPostProcessor）列表，然后在后续的逻辑中，会先调用这些后置处理器，然后再调用从BeanFactory中获取的后置处理器。这些直接从ApplicationContext中获取的后置处理器是通过addBeanFactoryPostProcessor方法直接添加到容器中的，在Spring的环境下这是空的，但是在SpringBoot的环境中，在容器刷新之前会添加，这个在分析SpringBoot源码的时候再细说。所以说这两个后置处理器的来源有两个，一个是context本身，一个是BeanFactory，他们的调用顺序都是这样的：
[from context]调用从context获取的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法，这里没有进行排序，而是按照传入list的顺序调用 [from beanFactory]调用实现了PriorityOrdered接口的**BeanDefinitionRegistryPostProcessor**的postProcessBeanDefinitionRegistry方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用实现了Ordered接口的**BeanDefinitionRegistryPostProcessor**的postProcessBeanDefinitionRegistry方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用没有实现上述两个接口的**BeanDefinitionRegistryPostProcessor**的postProcessBeanDefinitionRegistry方法，如果有多个处理器，那么顺序得不到保证 [from beanFactory]调用从context获取的**BeanDefinitionRegistryPostProcessor**的**postProcessBeanFactory**方法（因为BeanDefinitionRegistryPostProcessor也继承了BeanFactoryPostProcessor） [**from context**]调用**从context获取**的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，按照传入list的顺序调用 [from beanFactory]调用实现了PriorityOrdered接口的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用实现了Ordered接口的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用没有实现上述两个接口的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，如果有多个处理器，那么顺序得不到保证 注：在该方法的逻辑中，实现了BeanDefinitionRegistry接口的BeanFactory和没有实现该接口的BeanFactory调用逻辑不一样，如果BeanFactory没有实现BeanDefinitionRegistry，那么就不会有BeanDefinitionRegistryPostProcessor的调用，不过我们使用的都是DefaultListableBeanFactory，所以不考虑没有实现BeanDefinitionRegistry的情况</content></entry><entry><title>Spring源码-循环依赖和三级缓存</title><url>https://codingroam.github.io/post/spring%E6%BA%90%E7%A0%81-%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E5%92%8C%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98/</url><categories><category>Spring</category></categories><tags><tag>Spring</tag><tag>源码</tag><tag>Learning</tag></tags><content type="html"> Spring源码（循环依赖和三级缓存）
目录 循环依赖 多级缓存 一级缓存 二级缓存 当循环依赖遇上AOP 三级缓存 Spring三级缓存源码实现 总结
BeanFactory作为bean工厂管理我们的单例bean，那么肯定需要有个缓存来存储这些单例bean，在Spring中就是一个Map结构的缓存，key为beanName，value为bean。在获取一个bean的时候，先从缓存中获取，如果没有获取到，那么触发对象的实例化和初始化操作，完成之后再将对象放入缓存中，这样就能实现一个简单的bean工厂。 由于Spring提供了依赖注入的功能，支持将一个对象自动注入到另一个对象的属性中，这就可能出现循环依赖（区别于dependsOn）的问题：A对象在创建的时候依赖于B对象，而B对象在创建的时候依赖于A对象。 可以看到，由于A依赖B，所以创建A的时候触发了创建B，而B又依赖A，又会触发获取A，但是此时A正在创建中，还不在缓存中，就引发了问题。
一级缓存 前面引出了循环依赖的问题，那么该如何解决呢？其实很简单，单单依赖一级缓存就能解决。对于一级缓存，我们不再等对象初始化完成之后再存入缓存，而是等对象实例化完成后就存入一级缓存，由于此时缓存中的bean还没有进行初始化操作，可以称之为早期对象，也就是将A对象提前暴露了。这样B在创建过程中获取A的时候就能从缓存中获取到A对象，最后在A对象初始化工作完成后再更新一级缓存即可，这样就解决了循环依赖的问题。 但是这样又引出了另一个问题：早期对象和完整对象都存在于一级缓存中，如果此时来了其它线程并发获取bean，就可能从一级缓存中获取到不完整的bean，这明显不行，那么我们不得已只能在从一级缓存获取对象处加一个互斥锁，以避免这个问题。 而加互斥锁也带来了另一个问题，容器刷新完成后的普通获取bean的请求都需要竞争锁，如果这样处理，在高并发场景下使用spring的性能一定会极低。
二级缓存 既然只依赖一级缓存解决循环依赖需要靠加锁来保证对象的安全发布，而加锁又会带来性能问题，那该如何优化呢？答案就是引入另一个缓存。这个缓存也是一个Map结构，key为beanName，value为bean，而这个缓存可以称为二级缓存。我们将早期对象存到二级缓存中，一级缓存还是用于存储完整对象（对象初始化工作完成后），这样在接下来B创建的过程中获取A的时候，先从一级缓存获取，如果一级缓存没有获取到，则从二级缓存获取，虽然从二级缓存获取的对象是早期对象，但是站在对象内存关系的角度来看，二级缓存中的对象和后面一级缓存中的对象（指针）都指向同一对象，区别是对象处于不同的阶段，所以不会有什么问题。 既然获取bean的逻辑是先从一级缓存获取，没有的话再从二级缓存获取，那么也可能出现其它线程获取到不完整对象的问题，所以还是需要加互斥锁。不过这里的加锁逻辑可以下沉到二级缓存，因为早期对象存储在二级缓存中，从一级缓存获取对象不用加锁，这样的话当容器初始化完成之后，普通的getBean请求可以直接从一级缓存获取对象，而不用去竞争锁。
当循环依赖遇上AOP 似乎二级缓存已经解决了循环依赖的问题，看起来也非常简单，但是不要忘记Spring提供的另一种特性：AOP。Spring支持以CGLIB和JDK动态代理的方式为对象创建代理类以提供AOP支持，在前面总结bean生命周期的文章中已经提到过，代理对象的创建(通常)是在bean初始化完成之后进行（通过BeanPostProcessor后置处理器）的，而且按照正常思维来看，一个代理对象的创建也应该在原对象完整的基础上进行，但是当循环依赖遇上了AOP就不那么简单了。 还是在前面A和B相互依赖的场景中，试想一下如果A需要被代理呢？由于二级缓存中的早期对象是原对象，而代理对象是在A初始化完成之后再创建的，这就导致了B对象中引用的A对象不是代理对象，于是出现了问题。 要解决这问题也很简单，把代理对象提前创建不就行了？也就是如果没有循环依赖，那么代理对象还是在初始化完成后创建，如果有循环依赖，那么就提前创建代理对象。那么怎么判断发生了循环依赖呢？在B创建的过程中获取A的时候，发现二级缓存中有A，就说明发生了循环依赖，此时就为A创建代理对象，将其覆盖到二级缓存中，并且将代理对象复制给B的对应属性，解决了问题。当然，最终A初始化完成之后，在一级缓存中存放的肯定是代理对象。 如果在A和B相互依赖的基础上，还有一个对象C也依赖了A：A依赖B，B依赖A，A依赖C，C依赖A。那么在为A创建代理对象的时候，就要注意不能重复创建。
可以在对象实例化完成之后，将其beanName存到一个Set结构中，标识对应的bean正在创建中，而当其他对象创建的过程依赖某个对象的时候，判断其是否在这个set中，如果在就说明发生了循环依赖。
三级缓存 虽然仅仅依靠二级缓存能够解决循环依赖和AOP的问题，但是从解决方案上看来，维护代理对象的逻辑和getBean的逻辑过于耦合，Spring没有采取这种方案，而是引入了另一个三级缓存。三级缓存的key还是为beanName，但是value是一个函数（ObjectFactory#getBean方法），在该函数中执行获取早期对象的逻辑：getEarlyBeanReference方法。 在getEarlyBeanReference方法中，Spring会调用所有SmartInstantiationAwareBeanPostProcessor的getEarlyBeanReference方法，通过该方法可以修改早期对象的属性或者替换早期对象。这个是Spring留给开发者的另一个扩展点，虽然我们很少使用，不过在循环依赖遇到AOP的时候，代理对象就是通过这个后置处理器创建。
那么三级缓存在spring中是如何体现的呢？我们来到DefaultSingletonBeanRegistry中：
//一级缓存，也就是单例池，存储最终对象 private final Map&lt;String, Object> singletonObjects = new ConcurrentHashMap&lt;>(256); //二级缓存，存储早期对象 private final Map&lt;String, Object> earlySingletonObjects = new HashMap&lt;>(16); //三级缓存，存储的是一个函数接口， private final Map&lt;String, ObjectFactory&lt;?>> singletonFactories = new HashMap&lt;>(16); 其实所谓三级缓存，在源码中就是3个Map，一级缓存用于存储最终单例对象，二级缓存用于存储早期对象，三级缓存用于存储函数接口。 在容器刷新时调用的十几个方法中，finishBeanFactoryInitialization方法主要用于实例化单例bean，在其逻辑中会冻结BeanDefinition的元数据，接着调用beanFactory的preInstantiateSingletons方法，循环所有被管理的beanName，依次创建Bean，我们这里主要关注创建bean的逻辑，也就是AbstractBeanFactory的doGetBean方法（该方法很长，这里只贴了部分代码）：
protected &lt;T> T doGetBean(final String name, @Nullable final Class&lt;T> requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException { //解析FactoryBean的name(&amp;)和别名 final String beanName = transformedBeanName(name); Object bean; //尝试从缓存中获取对象 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) { //包含了处理FactoryBean的逻辑，可以通过&amp;+beanName获取原对象，通过beanName获取真实对象 //FactoryBean的真实bean有单独的缓存factoryBeanObjectCache（Map结构）存放 //如果是普通的bean，那么直接返回对应的对象 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { //只能解决单例对象的循环依赖 if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } //如果存在父工厂，并且当前工厂中不存在对应的BeanDefinition，那么尝试到父工厂中寻找 //比如spring mvc整合spring的场景 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) { //bean的原始名称 String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) { return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); } else if (args != null) { return (T) parentBeanFactory.getBean(nameToLookup, args); } else { return parentBeanFactory.getBean(nameToLookup, requiredType); } } try { //处理dependsOn的依赖(不是循环依赖) String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { if (isDependent(beanName, dep)) { //循环depends-on 抛出异常 throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Circular depends-on relationship between '" + beanName + "' and '" + dep + "'"); } registerDependentBean(dep, beanName); try { getBean(dep); } catch (NoSuchBeanDefinitionException ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, "'" + beanName + "' depends on missing bean '" + dep + "'", ex); } } } //创建单例bean if (mbd.isSingleton()) { //把beanName 和一个ObjectFactory类型的singletonFactory传入getSingleton方法 //ObjectFactory是一个函数，最终创建bean的逻辑就是通过回调这个ObjectFactory的getObject方法完成的 sharedInstance = getSingleton(beanName, () -> { try { //真正创建bean的逻辑 return createBean(beanName, mbd, args); } catch (BeansException ex) { destroySingleton(beanName); throw ex; } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } return (T) bean; } 对于单例对象，调用doGetBean方法的时候会调用getSingleton方法，该方法的入参是beanName和一个ObjectFactory的函数，在getSingleton方法中通过回调ObjectFactory的getBean方法完成bean的创建，那我们来看看getSingleton方法(部分代码)：
public Object getSingleton(String beanName, ObjectFactory&lt;?> singletonFactory) { Assert.notNull(beanName, "Bean name must not be null"); //互斥锁 synchronized (this.singletonObjects) { //首先尝试从尝试从单例缓存池中获取对象，如果获取到了对象则直接返回，否则进入创建的逻辑 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { //在创建之前将要创建的beanName存入singletonsCurrentlyInCreation中，这个是一个Map结构，用于标识单例正在创建中 beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) { this.suppressedExceptions = new LinkedHashSet&lt;>(); } try { //回调singletonFactory的getObject方法，该函数在外层doGetBean方法中传入 //实际调用了createBean方法 singletonObject = singletonFactory.getObject(); newSingleton = true; } finally { if (recordSuppressedExceptions) { this.suppressedExceptions = null; } //将beanName从正在创建单例bean的集合singletonsCurrentlyInCreation中移除 afterSingletonCreation(beanName); } if (newSingleton) { //将创建好的单例bean放入一级缓存，并且从二级缓存、三级缓存中移除 //添加到registeredSingletons中 addSingleton(beanName, singletonObject); } } return singletonObject; } } getSingleton方法的主线逻辑很简单，就是通过传入的函数接口创建单例bean，然后存入一级缓存中，同时清理bean在二级缓存、三级缓存中对应的数据。前面已经分析过了，传入的函数接口调用的是createBean方法，那么我们又来到createBean方法，createBean方法主要调用doCreateBean方法，在doCreateBean调用之前会先调用nstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation拦截bean的实例化，如果这里的后置处理器返回了bean，则不会到后面的doCreateBean方法中，不过我们这里不用关心这个逻辑，直接跳到doCreateBean方法（部分代码）：
protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { //创建bean，也是就是实例化，会把实例化后的bean包装在BeanWrapper中 instanceWrapper = createBeanInstance(beanName, mbd, args); } //eanWrapper中获取到对象 final Object bean = instanceWrapper.getWrappedInstance(); Class&lt;?> beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) { mbd.resolvedTargetType = beanType; } synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { //MergedBeanDefinitionPostProcessor后置处理器的处理 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Post-processing of merged bean definition failed", ex); } mbd.postProcessed = true; } } //判断该对象是否支持提前暴露，核心条件就是要求单例对象 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { //把我们的早期对象包装成一个singletonFactory对象 该对象提供了一个getObject方法,该方法内部调用getEarlyBeanReference方法 //将早期对象存入三级缓存，三级缓存存的是一个接口实现（ObjectFactory接口），其getBean方法会调用getEarlyBeanReference方法 addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); } // Initialize the bean instance. Object exposedObject = bean; try { //属性赋值操作，这里就可能出现循环依赖问题 populateBean(beanName, mbd, instanceWrapper); //实例化操作：调用三个Aware、后置处理器beforeInitialization（包含@PostConstruct的实现）、afterPropertiesSet、init-method、后置处理器afterInitialization等 //个方法里的后置处理器可能会改变exposeObject exposedObject = initializeBean(beanName, exposedObject, mbd); } catch (Throwable ex) { if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Initialization of bean failed", ex); } } if (earlySingletonExposure) { //这里的入参为false,表示只从一级缓存和二级缓存中获取 //非循环依赖的情况下，不会调用到三级缓存，三级缓存中的接口会在单例对象入一级缓存的时候移除 Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { //如果获取到了早期暴露对象earlySingletonReference不为空 if (exposedObject == bean) { //如果exposeObject在经过initializeBean方法后没有改变，那么说明 //没有被后置处理器修改，那么用earlySingletonReference替换exposeObject exposedObject = earlySingletonReference; } } } return exposedObject; } 这个方法的逻辑就是先实例化对象，如果对象支持暴露早期对象，那么会将早期对象作为入参传入getEarlyBeanReference方法，然后包装成一个ObjectFactory存入三级缓存，接着再调用populateBean方法填充对象的属性。而在填充对象属性的过程中，就可能发现循环依赖，比如当前正在创建A，然后在populateBean方法中发现了依赖B，那么就会调用getBean(B)，B也会走上面A走的这个流程，当B也走到populateBean方法填充属性的时候，又发现依赖了A，那么又会调用getBean(A)。那么在populateBean创建A的时候，会从三级缓存中获取bean，如果A需要被代理，那么会创建代理对象，这样B中的A属性就是代理对象了，接着把三级缓存获取的对象存入二级缓存中。 在B处理完成之后就回到了A的逻辑，假设populateBean(A)的逻辑完成了，那么接着进入initializeBean方法进行A的初始化操作，注意这里执行初始化操作的对象是A原对象，代理对象存储在二级缓存中。由于initializeBean方法会调用后置处理器的before-afterInitialization方法，这个后置处理器可能会改变对象，所以在后面的逻辑中，如果从二级缓存中获取到了A的代理对象，会判断原对象经过后置处理器后有没有变化，如果还是原对象，那么用二级缓存中的代理对象覆盖原对象，所以doCreateBean方法返回的就是代理对象，最终存入一级缓存中。流程就是：创建A实例对象->设置A的B属性->创建B实例对象->设置B的A属性->创建A的代理对象存入二级缓存->初始化A对象->从二级缓存取出A的代理对象覆盖A对象->A的代理对象存入一级缓存 我们需要关注的就是在B的populateBean逻辑里调用getBean(A)是什么样的逻辑。当然也是getBean->doGetBean->getSingleton这个逻辑，我们着重关注一下getSingleton方法的实现：
protected Object getSingleton(String beanName, boolean allowEarlyReference) { //首先从一级缓存中获取 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) { //如果从一级缓存没有获取到，并且获取的对象正在创建中，这里已经能够确定发生了循环依赖 synchronized (this.singletonObjects) { //为了防止其它线程获取到不完整工单对象，这里使用同步锁控制 //从二级缓存中获取早期对象 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) { //如果二级缓存中也没有获取到，且allowEarlyReference为true（这里传入的是true） //那么尝试从三级缓存中获取 ObjectFactory&lt;?> singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { //从三级缓存中获取到了对象，注意三级缓存存储的是ObjectFactory //调用getObject方法，前面提到了早期暴露的对象存入三级缓存的ObjectFactory的getBean方法调用 //的是getEarlyBeanReference方法，所以这里会调用getEarlyBeanReference方法 singletonObject = singletonFactory.getObject(); //把早期对象存入二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); //三级缓存中的接口没用了，直接移除 this.singletonFactories.remove(beanName); } } } } return singletonObject; } 在getSingleton方法的逻辑中，先从一级缓存获取，如果一级缓存没有找到，那么如果获取的bean正在创建中，则从二级缓存获取，如果二级缓存没有找到，那么从三级缓存获取，三级缓存中存的是ObjectFactory实现，最终会调用其getBean方法获取bean，然后存入二级缓存中，同时清除三级缓存。同时提供了一个allowEarlyReference参数控制是否能从三级缓存中获取。 对于循环依赖的情况，getBean(A)->存入正在创建缓存->存入三级缓存->populateBean(A)->getBean(B)->populateBean(B)->getBean(A)->getSingleton(A)，当在populateBean(B)的过程中调用getSingleton(A)的时候，明显一级缓存和二级缓存都为空，但是三级缓存不为空，所以会通过三级缓存获取bean，三级缓存的创建逻辑如下：
boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); } 所以我们直接来到getEarlyBeanReference方法获取早期对象：
protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) { Object exposedObject = bean; //如果容器中有InstantiationAwareBeanPostProcessors后置处理器 if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof SmartInstantiationAwareBeanPostProcessor) { //找到SmartInstantiationAwareBeanPostProcessor后置处理器 SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; //调用SmartInstantiationAwareBeanPostProcessor的getEarlyBeanReference方法 exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); } } } return exposedObject; } 这个getEarlyBeanReference方法的逻辑很简单，但是非常重要，该方法主要就是对SmartInstantiationAwareBeanPostProcessor后置处理器的调用，而循环依赖时的AOP就是通过这个SmartInstantiationAwareBeanPostProcessor的getEarlyBeanReference方法实现的，相关的具体类是AnnotationAwareAspectJAutoProxyCreator，这个留待AOP原理分析的文章中详细说明。
多级缓存并不只是为了解决循环依赖和AOP的问题，还考虑到了逻辑的分离、结构的扩展性和保证数据安全前提下的效率问题等。由于存在二级缓存提前暴露不完整对象的情况，所以为了防止getSingleton方法返回不完整的bean，在该方法中使用了synchronized加锁，而为了不影响容器刷新完成后正常获取bean，从一级缓存获取对象没有加锁，事实上也不用加锁，因为一级缓存中要么没有对象，要么就是完整的对象。 通常情况下，容器刷新会触发单例对象的实例化和初始化，大致流程如下：循环依赖发生在对象都属性赋值，也就是populateBean阶段，在对象实例化完成后将其包装成一个函数接口ObjectFactory存入三级缓存，通过getEarlyBeanReference方法触发SmartInstantiationAwareBeanPostProcessor后置处理器的执行，如果循环依赖遇上了AOP，那么就在这里进行处理。 这里每个单例对象实例化之后存入三级缓存，即使没有发生循环依赖，这个是为之后可能发生的循环依赖做准备，如果最终没有发生循环依赖，那么对象实例化之后，正常初始化，然后存入一级缓存即可，而在存入一级缓存的时候会清空二级和三级缓存。</content></entry><entry><title>Sql问题记录一（distinct、group by）</title><url>https://codingroam.github.io/post/sql%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%E4%B8%80distinctgroup-by/</url><categories><category>MySQL</category></categories><tags><tag>SQL</tag><tag>MySQL</tag></tags><content type="html"> mysql使用distinct问题记录
distinct可以单字段去重，也可以多字段去重，
假如有表 A,其有id，name,sex,addr,tel,createTime等如下字段，其中id是主键，故唯一
那么我们就可以对其他字段进行去重操作了。
1、按单字段去重，
select distinct name from A
这样，可以仅仅可以查询name这一列，且name没有相同数据。
2、多字段去重
select distinct name, sex, addr from A
mysql会对dintinct所有字段当作整体去重，这样，只有当这三个字段全部相同时，才能成功去重，有一个
字段不同就不会过滤掉。
如果我们去重后想查看所有字段，想写出类似这样一个sql：
select distinct(name,sex,addr) name,sex,addr,createTime from A 这种写法是不被允许的，但是用group by来实现
select * from A where id in (select max(id) from A group by name,sex,addr)
子表是按group by 分组，也就相当于去重，去重之后，只找到一个最大的id,然后select * in 这些id中就可以
完成去重后查看所有的字段了。</content></entry><entry><title>关于IDEA生成jar包后FileEncoding依然是GBK</title><url>https://codingroam.github.io/post/%E5%85%B3%E4%BA%8Eidea%E7%94%9F%E6%88%90jar%E5%8C%85%E5%90%8Efileencoding%E4%BE%9D%E7%84%B6%E6%98%AFgbk/</url><categories><category>Java</category></categories><tags><tag>Java</tag><tag>Problem-Solving</tag></tags><content type="html"> 关于IDEA生成jar包后FileEncoding依然是GBK
在启动类增加 Properties类 @SpringBootApplication public class TokenApplication { public static void main(String[] args) { Properties properties = System.getProperties(); properties.forEach((key,value)->{ System.out.println("key = " + key); System.out.println("value = " + value); }); SpringApplication.run(TokenApplication.class,args); } } 打印在控制台我们可以看到 file.encoding 的值 为UTF-8 ，说明我们的idea的file.encoding 是为UTF-8 如若不是可修改： 然后我我们使用maven 将其项目打成jar包，使用java-jar 运行jar包 我们看到 其值变为了GBK，在普通的运行当中我认为是不会出错，因今天我们的项目使用jar包运行，在调用python算法时，报utf-8 codec can&rsquo;t decode byte 此错误，是因为他转为GBK（在idea运行，调用算法没有报错） 3.1 （第一种方法，较麻烦）换用运行命令 我们之前使用的是java -jar 我们在他后边加上 -Dfile.encoding=utf-8 新命令： java -jar -Dfile.encoding=utf-8 jar包 运行 我们看到打印的结果为 其已经修改， 这时候他就已经是UTF-8了 这样我们就解决了其file.encoding 为GBK 的问题，但是我们不能总是java -jar 在加上他 我们可以写一个cmd脚本来运行此命令，但同样我们可以使用第二种更为简单的方法 3.2（第二种方法，简单）配置环境变量 同样我们可以再电脑的环境变量中添加 key：JAVA_TOOL_OPTIONS value： -Dfile.encoding=utf-8 添加之后我们保存 ，这时候我们再泳普通的 java -jar 来运行这个jar包 我们看到他已经变成了 utf-8,到这里也就解决了 无论如何在idea中设置jar包的file.encoding 为GBK的问题。</content></entry><entry><title>Centos安装 Docker</title><url>https://codingroam.github.io/post/centos%E5%AE%89%E8%A3%85docker/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Centos</tag></tags><content type="html"> Centos安装 Docker
Centos安装 Docker 从 2017 年 3 月开始 docker 在原来的基础上分为两个分支版本: Docker CE 和 Docker EE。
Docker CE 即社区免费版，Docker EE 即企业版，强调安全，但需付费使用。
本文介绍 Docker CE 的安装使用。
移除旧的版本：
$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 安装一些必要的系统工具：
sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息：
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新 yum 缓存：
# centos 7 sudo yum makecache fast # CentOS 8没有fast这个命令 sudo yum makecache 安装 Docker-ce：
sudo yum -y install docker-ce 查看已安装docker版本
docker version 启动 Docker 后台服务
sudo systemctl start docker 开机启动
sudo systemctl enable docker 镜像加速 鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决。
可以使用阿里云的docker镜像地址：https://7qyk8phi.mirror.aliyuncs.com
新版的 Docker 使用 /etc/docker/daemon.json（Linux，没有请新建）。
请在该配置文件中加入：
（没有该文件的话，请先建一个）
{ "registry-mirrors": ["https://7qyk8phi.mirror.aliyuncs.com"] } 重启docker
sudo systemctl daemon-reload sudo systemctl restart docker 检查加速器是否生效 配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行 docker info，如果从结果中看到了如下内容，说明配置成功。
Registry Mirrors: https://7qyk8phi.mirror.aliyuncs.com/ 下载docker-compose #运行此命令以下载 Docker Compose 的当前稳定版本 sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose #对二进制文件应用可执行权限 sudo chmod +x /usr/local/bin/docker-compose #测试安装 docker-compose --version #若有docker-compose version 1.29.2, build 5becea4c，则安装成功</content></entry><entry><title>Docker与Docker-compose</title><url>https://codingroam.github.io/post/docker%E4%B8%8Edocker-compose%E8%AF%A6%E8%A7%A3/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Learning</tag></tags><content type="html"> Docker与Docker-compose详解
1、Docker是什么？
在计算机中，虚拟化(英语: Virtualization) 是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。
在实际的生产环境中。虚拟化技术主要用来解决高性能的物理硬件产能过利和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件资源的充分利用。
虚拟化技术种类很多，例如:软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化、桌面虚拟化、服务虚拟化、虚拟机等等。.
Docker和传统虚拟机的区别
2、Docker的安装
2.1、Windows下的安装
直接在官网下载windows包双击运行即可，对于win10来说需要开启Hype-v，直接百度打开即可。
2.2、Linux下的安装
curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh --mirror Aliyun # 安装报错 Problem: problem with installed package buildah # 执行语句 yum erase podman buildah # 再进行安装 systemctl status docker systemctl restart docker docker info systemctl enable docker # 建立docker组 sudo groupadd docker sudo usermod -aG docker $USER # 重启服务 systemctl restart docker 2.3、核心概念
仓库 1. 远程仓库：开发者镜像及官方镜像 1. 本地仓库：只保存当前自己使用过的镜像及自定义镜像 1. 作用：用来存放docker镜像位置 5. 镜像 5. 作用：一个镜像就代表一个软件 7. 容器 7. 作用：一个幢像运行一次就会生成一个实例就是生成一个容器 2.4、Aliyun服务加速
docker提供了一个远程仓库，主要是用来存放镜像的，而我们所需要的镜像都需要去远程仓库进行拉取，dockerHub 地址： https://registry.hub.docker.com/_/mysql?tab=tags
，这里以mysql镜像为例，然后直接在虚拟机当中执行命令
# 获取最新版本的mysql docker pull mysql # 获取指定版本的mysql 8.0.18版 docker pull mysql:8.0.18 在这里的dockerhub是为全球服务的，速度难免会有点慢，这里可以配置阿里的镜像来进行获取docker远程仓库的镜像。阿里云服务加速配置 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors
，按照官网进行配置即可：
3、Docker的操作
3.1、Hello World
在安装docker之后，直接使用命令：docker run hello-world 表示直接运行hello-world这个镜像。而他的执行基本步骤如下：
3.2、Docker 的基本命令
3.2.1、docker引擎及帮助操作：
# 查看docker信息 docker info # 查看docker版本 docker version # 帮助命令 docker --help 3.2.2、镜像相关操作：
# 查看镜像 docker images docker images -a #展示所有镜像 docker images -q #只展示镜像的ID docker images mysql #只展示mysql镜像 # 下载镜像 docker pull 镜像名称:版本号 # 如 docker pull mysql:8.0.27 docker pull 镜像名称:@DIGEST #如：docker pull mysql:DIGEST:sha256:975b3b1a6df6bf66221d1702b76c4141a4cd09f93f22f70c32edc99a6c256fe8 # 搜索镜像 docker search 镜像 # docker search mysql # 搜索stars数在3000以上的image docker search mysql --filter=stars=3000 # 删除镜像 docker image rm 镜像名:版本或者id标识 # docker image rm mysql:8.0.27 docker image rm -f 镜像名:版本或者id标识 # 强制删除 # 简化删除 docker rmi 镜像名:版本 # 组合运用 # 清空本地仓库所有镜像 docker rmi -f $(docker images -q) 3.2.3、容器相关操作：
# 导入本地镜像 docker load -i 镜像文件 # 运行一个容器 docker run 镜像名称:版本号 # 运行容器与宿主机进行映射 docker run -p 8080:8080 镜像名称:版本号 # 启动容器映射端口，后台启动 docker run -p 8080:8080 -d 镜像名称:版本号 # 启动容器映射端口，后台启动，指定名称 docker run -p 8080:8080 --name 容器名称 -d 镜像名称:版本号 # 查看正在运行的容器 docker ps # 查看运行容器的历史记录 docker ps -a # 查看最近运行的两个容器 docker ps -a -n=2 # 查看正在运行的容器id docker ps -q # 查看所有容器的id docker ps -aq # 容器的启动和停止 docker start 容器名称或者容器id docker restart 容器名称或者容器id docker stop 容器名称或者容器id docker kill 容器名称或者容器id # 容器的删除 docker rm 容器的id或者名称 docker rm -f 容器的id或者名称 docker rm -f $(docker ps -aq) # 查看日志 docker logs 容器id或名称 # 实时展示日志 docker logs -f 容器id或名称 # 加入时间戳展示实时展示日志 docker logs -tf 容器id或名称 # 查看最后n行日志 docker logs --tail 5 容器id或名称 # 查看容器的内部进程 docker top 容器id或名称 # 与容器内部进行交互 docker exec -it 容器id或名称 bash # 从容器复制文件到操作系统 docker cp 容器id:路径 操作系统下的路径 # 从操作系统复制文件到容器当中 docker cp 操作系统下的路径 容器id:路径 在这里的文件复制主要还是运用到本地项目打包后的部署，比如说这里一个项目开发完成之后，打成一个jar包或者war包，丢给tomcat进行启动部署，而后直接将这个包给到tomcat镜像下的webapps目录下，重新启动tomcat或者重启容器，最后进行访问项目。
# 查看容器中的元数据 docker inspect 容器id # 数据卷（Volume）：实现宿主机系统和容器之间的文件共享 # 数据卷的使用： docker run -d -p 8080:8080 --name 容器名称 -v 操作系统下路径:容器下路径 镜像名称:版本 # aa代表一个数据卷的名字，名称可以自定义，docker在不存在时自动创建这个数据卷，并且同时自动映射宿主机当中的某个目录 # 同时在启动容器的时候会将aa对应容器目录中全部内容复制到aa映射目录当中 docker run -d -p 8080:8080 --name 容器名称 -v aa:容器下路径 镜像名称:版本 find / -name aa # 将容器打成一个新的镜像 docker commit -m "描述信息" -a "作者信息" 容器id或名称 打包的镜像名称:标签版本 # 将镜像备份出来 docker save 镜像名称:版本 -o 文件名 # 重新加载镜像 docker load -i 镜像文件 3.3、Docker 镜像分层原理
镜像是一种轻量级的，可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时所需的库、环境变量和配置文件。
Docker当中的镜像为什么这莫大？
Docker的设计：一个软件镜像不仅仅是原来软件包，镜像当中还包含软件包所需的操作系统依赖软件自身依赖以及自身软件包组成。
分层原理
很显然，在这里docker容器的设计简单来说，对于不同的环境都给抽离出来进行分层，就比如说很多的软件服务（比如说：Naocs、ES、Hadoop等等）都需要jdk的环境，那再进行拉取镜像的时候，这些镜像都会先检验jdk的环境，再进行后续的安装，那这里装个Naocs、ES、Hadoop要下载三次JDK，这显然浪费了很多的内存，所以在这里Docker采用了分层的原理，这里每一层的环境依赖都给分开了，再一次安装了jdk环境之后，后续安装的服务也要jdk依赖就不会再去拉取了，回直接使用本地有的jdk环境。
3.4、Docker 网络
在docker当中容器和容器之间也是可以进行通信的。就好比Linux中我们使用 ip addr 可以看到当前虚拟机的ip地址，在这里可以查看一下容器中的IP，docker exec -it 容器名 ip addr 会发现有一个对应的映射另一个映射。说明docker容器网络是通过veth-pair技术实现的。
并且在这里还可以通过docker inspect 容器名称或id 命令查看容器的元数据，这里也有该容器随机分配的ip地址。
而当我们启动多个容器之后，可以查看多个容器的ip地址，可以看到容器的ip地址都在同一个网段上，这就有点似曾相识的感觉了，在linux当中我们配置多台机器进行互相通信，那这里的容器通信那也是一样的不，直接进入到一台容器之内，使用ping命令，ping另外一个容器的ip。
再就是在启动容器之后，默认为分配的ip地址都同一个网桥上，而这里容器当中需要对网桥进行分割开又要如何操作呢？我们需要创建一个网桥，而后在启动容器的时候指定对应的网桥即可。
# 查看网桥 docker network ls # 创建网桥 docker network create 网桥名称 # 容器指定网桥挂载 docker run -p -d 8080:8080 --network 网桥名称 --name 容器名称 镜像:版本 # 在启动容器，生成的ip地址会和容器名称进行映射，这里除了使用ip进行访问，还可以使用容器名称进行访问 # 删除 docker network rm 网桥名称 # 网桥细节 docker inspect 网桥名称 3.5、Docker 数据卷
3.5.1、作用
是用来实现容器和宿主机之间的数据共享
3.5.2：特点
数据卷可以在容器之间进行共享和重用 对数据卷的修改会立即影响到对应的容器 对数据卷的修改不会影响镜像 数据卷默认一直存在，即使容器被删除 3.5.3、数据卷操作
# 自定义数据卷目录 docker run -v 绝对路径:容器内路径 # 自动创建数据卷 docker run -v 卷名:容器内路径 # 查看数据卷 docker volume ls # 查看数据卷的细节 docker volume inspect 卷名 # 创建数据卷 docker volume create 卷名 # 删除数据卷(没有使用的数据卷) docker volume prune # 删除指定的数据卷 docker volume rm 卷名 3.6、Docker 核心架构
4、Docker安装服务
4.1、mysql 的安装
首先我们需要确定服务的版本，拉取镜像到本地： dockerHub 拉取镜像描述文件
，在镜像的描述文件当中，会对服务的启动、查看服务日志、服务配置等等都有进行描述。
# 先获取mysql服务 docker pull mysql:8.0.18 服务的启动：这里需要指定运行的环境
# 基本启动 docker run -e MYSQL_ROOT_PASSWORD=root -d mysql:8.0.18 # 启动服务 后台运行 指定root用户账号密码(设置root账户的密码为root) 指定容器名称 docker run -d -p 3307:3306 --name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d mysql:8.0.18 # 启动服务 后台运行 指定root用户账号密码 指定容器名称 使用数据卷将数据持久化 # mysql 容器默认存储位置：/var/lib/mysql docker run -d -p 3307:3306 --name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d -v mysqldata:/var/lib/mysql mysql:8.0.18 # 启动服务 后台运行 指定root用户账号密码 指定容器名称 使用数据卷将数据持久化 已修改之后的配置文件启动 docker run -d -p 3307:3306 --name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d -v mysqldata:/var/lib/mysql -v mysqlconfig:/etc/mysql mysql:8.0.18 4.2、Tomcat 的安装
# 先获取镜像 docker pull tomcat:9.0-jdk8 # 服务启动 docker run -d -p 8080:8080 --name tomcat tomcat:9.0-jdk8 # 项目的部署目录 /usr/local/tomcat/webapps docker run -d -p 8080:8080 -v apps:/usr/local/tomcat/webapps --name tomcat tomcat:9.0-jdk8 # 配置文件目录 /usr/local/tomcat/conf docker run -d -p 8080:8080 -v apps:/usr/local/tomcat/webapps -v confs:/usr/local/tomcat/conf --name tomcat tomcat:9.0-jdk8 4.3、Redis的安装
# 拉取镜像 docker pull redis:6.2.6 # 启动服务 docker run -d -p 6379:6379 --name redis6 redis:6.2.6 # redis 持久化 docker run -d --name redis6 redis:6.2.6 redis-server --appendonly yes 4.4、ElasticSearch 的安装
5、Dockerfile
5.1、Dockerfile 概述
5.1.1、Dockerfile是什么？
Dockerfile是用来帮助自己构建一个自定义镜像
5.1.2、为什么会存在Dockerfile？
日常用户可以将自己应用进行打包成镜像，这样就可以让我们自己的应用在容器当中运行
5.1.3、Dockerfile构建镜像原理
5.2、Dockerfile 语法
FROM：构建一个自定义的镜像
# 新建一个dockerfile文件 vim Dockerfile # 写入内容 FROM centos:8 # 进行build docker build -t mycentos8:01 . # RUN : 对镜像进行扩展 docker run -it centos:7 # 不支持vim，对于vim的扩展，在原本的dockerfile文件当中加入 RUN yum install -y vim # 或者使用这种语法 RUN ["yum","install","-y","vim"] # EXPORT : 镜像暴露端口 EXPOSE 8888 #指定工作目录 WORKDIR /data # 复制文件 COPY aa.txt /data/aa # 添加内容 ADD bb.txt /data/bb ADD 下载地址 /data/tomcat 5.3、idea对Dockerfile支持
打开idea的settings，在以来当中找到docker的依赖，进行安装该依赖，安装之后重启idea就可以对Dockerfile文件进行编辑了。
第二个就是，Dockerfile文件都在linux上，在idea当中怎么进行编辑呢，可以选择Tools下的deployment的browse remote host 进行连接远程虚拟机，这里直接连接上去之后在右侧就会有虚拟机上的文件目录信息，并且可以直接在idea当中进行打开了。
6、Docker compose
6.1、Docker compose 概述
6.1.1、compose的作用
用来负责对Docker容器集群的快速编排
6.1.2、compose的定位
是用来定义和运行多个docker容器的应用 同时可以对多个容器进行编排
6.1.3、compose的核心概念
服务：一个应用的容器，服务可以存在多个 项目：由一组关联的应用容器组成的一个完整业务单元，在docker-compose.yml文件当中进行定义 6.1.4、compose的安装
github下载地址： https://github.com/docker/compose/releases
首先在github上面下载对应的版本包，下载之后将包上传到linux服务器上。将文件进行重命名并且复制到/usr/local/bin目录下，并且给该目录赋予权限。最后直接使用docker-compose -v命令查看版本进行校验是否安装成功
mv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose -v 6.1.5、docker和docker-compose直接的版本对应
docker官网地址： https://docs.docker.com/compose/compose-file/
使用命令 docker -v 查看docker的版本，可以在官网当中看到compose对docker版本的支持
6.2、Docker compose —— HelloWorld
在前面有说到compose当中的组成部分，分别是服务和项目，这里首先创建一个目录用来表示这第一个helloworld项目，在项目当中添加docker-compose.yml文件用来编写compose。
# 创建目录 mkdir hello cd hello # 新建docker-compose.yml文件 vim docker-compose.yml # 写入内容（这里在vim当中编辑yml文件挺难受的，可以在idea当中编辑远程主机的文件） version: "3.0" # 指定compose的版本 services: # 指定服务 tomcat: # 单个服务 image: tomcat:9.0.27-jdk8 # 服务镜像 ports: - 8081:8080 # 暴露对应的端口 # 保持文件内容后进行启动compose docker-compose up # 服务启动之后，可以直接进行访问8081端口 http://远程主机ip/8081 6.3、Docker compose —— 命令模板
version: "3.0" # compose版本 services: user: build: context: user # dockerfile的镜像 dockerfile: Dockerfile # 读取dockerfile文件进行打包获取镜像 container_name: user ports: - "8888:8888" networks: - hello depends_on: - tomcat tomcat: # 单个服务标识 container_name: tomcat # 启动后的容器名称 相当于 --name 指定的名称 image: tomcat:9.0.27-jdk8 # 镜像 ports: - 8081:8080 # 端口映射 volumes: - tomcatwebapps:/usr/local/tomcat/webapps # 指定对应的数据卷 networks: - hello # 指定网桥 depends_on: # 服务启动依赖 - tomcat1 # 服务标识 - mysql healthcheck: # 健康检查 test: ['CMD','curl','-f','http://localhost'] interval: 1m30s timeout: 10s retries: 3 sysctls: # 修改内核参数 - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits: # 修改最大进程数 nproc: 65335 nofile: soft: 20000 hard: 40000 tomcat1: container_name: tomcat2 image: tomcat:9.0.27-jdk8 ports: - 8082:8080 volumes: - tomcatwebapps1:/usr/local/tomcat/webapps networks: - hello mysql: container_name: mysql8 image: mysql:8.0.18 ports: - 3307:3306 # environment: # 指定启动的环境 # - MYSQL_ROOT_PASSWORD=root env_file: # 使用文件进行代替 - ./mysql.env # mysql.evn文件内容就是 MYSQL_ROOT_PASSWORD=root volumes: - mysqldata:/var/lib/mysql - mysqlconfig:/etc/mysql networks: - hello # 数据卷都要在这统计管理 volumes: tomcatwebapps: tomcatwebapps1: # external: # 使用自定义数据卷名称 默认命名为 项目名_数据卷名 自定义后为 数据卷名 # true mysqldata: mysqlconfig: # 统一管理网桥 networks: hello: 6.4、Docker compose 指令
6.4.1、模板指令与指令
模板指令：用来书写在docker-compose.yml文件当中的指令，是用来为服务进行服务的 指令：用来对整个docker-compose.yml对应的这个项目进行操作 6.4.2、常用指令
6.5、Docker 可视化工具 —— portainer
直接在dockerHub上面拉取镜像启动服务
docker pull portainer/portainer:1.24.2 docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer:1.24.2 直接访问远程虚拟机的9000端口，注册一个账号，链接到本地虚拟机的服务，就可以看到所提供的web可视化页面了。
同样的我们还可以将这个服务的启动加到docker-componse当中进行启动：
# 加入服务 portainer: container_name: portainer image: portainer/portainer:1.24.2 ports: - 8000:8000 - 9000:9000 volumes: - /var/run/docker.sock:/var/run/docker.sock - portainer_data:/data # 数据卷 volumes: portainer_data:</content></entry><entry><title>Seata四种模式（XA、AT、TCC、SAGA）</title><url>https://codingroam.github.io/post/seata%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8Fxaattccsaga/</url><categories><category>分布式</category><category>中间件</category></categories><tags><tag>Learning</tag><tag>中间件</tag><tag>分布式事务</tag></tags><content type="html"> Seata四种模式（XA、AT、TCC、SAGA）
Seata四种模式（XA、AT、TCC、SAGA） 1、XA模式 XA模式原理： XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA 规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对 XA 规范 提供了支持。
如果有失败的就会回滚事务
seata的XA模式 seata的XA模式做了一些调整，但大体相似：
RM一阶段的工作：
注册分支事务到TC 执行分支业务sql但不提交 报告执行状态到TC TC二阶段的工作：
TC检测各分支事务执行状态
如果都成功，通知所有RM提交事务
如果有失败，通知所有RM回滚事务
RM二阶段的工作：
接收TC指令，提交或回滚事务 xa模式的优点：
事务的强一致性，满足ACID原则。
常用数据库都支持，实现简单，并且没有代码侵入
xa模式的缺点：
因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差 依赖关系型数据库实现事务 实现XA模式
Seata的starter已经完成了XA模式的自动装配，实现非常简单，步骤如下： 修改application.yml文件（每个参与事务的微服务），开启XA模式：
配置seata的注册中心 seata: data-source-proxy-mode: XA # 选择XA模式 注意：是每一个微服务都需要
给发起全局事务的入口方法添加@GlobalTransactional注解，本例中是OrderServiceImpl中的create方法：
启动所有微服务，postman进行接口测试
先进行正确的测试，再继续错误的测试
错误设置，购买商品超出原来剩余的商品数就会让数据库报错测试是否可以事务回滚
注意：如果测试接口报错响应时间过长，那么就应该设置响应的时间大一点，如下图，然后重启seata
成功的可以查看seate的控制输出，可以看到事务回滚
IDEA输出
查看数据库的数据是否有被更新
2、AT模式 2.1、 AT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。 阶段一RM的工作：
注册分支事务 记录undo-log（数据快照，JSON格式） 执行业务sql并提交 报告事务状态
阶段二提交时RM的工作：
删除undo-log即可
阶段二回滚时RM的工作：
根据undo-log恢复数据到更新前
执行示例 例如，一个分支业务的SQL是这样的：update tb_account set money = money - 10 where id = 1
AT模式与XA模式最大的区别是什么
XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源。 XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚。 XA模式强一致；AT模式最终一致
2.2、AT模式的脏写问题 2.3、AT模式的写隔离 2.4、AT模式的优缺点 优点：
一阶段完成直接提交事务，释放数据库资源，性能比较好 利用全局锁实现读写隔离 没有代码侵入，框架自动完成回滚和提交 缺点：
两阶段之间属于软状态，属于最终一致 框架的快照功能会影响性能，但比XA模式要好很多 2.5、实现AT模式 AT模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。
1、创建相关的数据库文件
lock_table（全局锁）表导入到TC服务关联的数据库
DROP TABLE IF EXISTS `lock_table`; CREATE TABLE `lock_table` ( `row_key` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `xid` varchar(96) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `transaction_id` bigint(20) NULL DEFAULT NULL, `branch_id` bigint(20) NOT NULL, `resource_id` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `table_name` varchar(32) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `pk` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `gmt_create` datetime NULL DEFAULT NULL, `gmt_modified` datetime NULL DEFAULT NULL, PRIMARY KEY (`row_key`) USING BTREE, INDEX `idx_branch_id`(`branch_id`) USING BTREE ) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact; undo_log（记录快照）表导入到微服务关联的数据库
DROP TABLE IF EXISTS `undo_log`; CREATE TABLE `undo_log` ( `branch_id` bigint(20) NOT NULL COMMENT 'branch transaction id', `xid` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'global transaction id', `context` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'undo_log context,such as serialization', `rollback_info` longblob NOT NULL COMMENT 'rollback info', `log_status` int(11) NOT NULL COMMENT '0:normal status,1:defense status', `log_created` datetime(6) NOT NULL COMMENT 'create datetime', `log_modified` datetime(6) NOT NULL COMMENT 'modify datetime', UNIQUE INDEX `ux_undo_log`(`xid`, `branch_id`) USING BTREE ) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = 'AT transaction mode undo table' ROW_FORMAT = Compact; 2、修改application.yml文件，将事务模式修改为AT模式即可：
#配置seata的注册中心 seata: data-source-proxy-mode: AT #选择XA模式 3、重启并测试
查看当前的数据库库存数量继续超库存创建订单进行执行错误回滚测试
查看IDEA的错误日志
3、TCC模式 3.1、tcc模式原理 TCC模式与AT模式非常相似，每阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复。需要实现三个方法：
Try：资源的检测和预留； Confirm：完成资源操作业务；要求 Try 成功 Confirm 一定要能成功。 Cancel：预留资源释放，可以理解为try的反向操作。 举例，一个扣减用户余额的业务。假设账户A原来余额是100，需要余额扣减30元。
TCC的工作模型图：
3.2、小结 TCC模式的优点：
一阶段完成直接提交事务，释放数据库资源，性能好 相比AT模型，无需生成快照，无需使用全局锁，性能最强 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库 TCC模式的缺点：
有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦 软状态，事务是最终一致 需要考虑Confirm和Cancel的失败情况，做好幂等处理 3.2、TCC模式实现案例 不是所有的业务都适合TCC模式，如库存，金额等就比较适合
改造account-service服务，利用TCC实现分布式事务
需求如下：
修改account-service，编写try、confirm、cancel逻辑：
try业务：添加冻结金额，扣减可用金额 confirm业务：删除冻结金额 cancel业务：删除冻结金额，恢复可用金额 保证confirm、cancel接口的幂等性允许空回滚拒绝业务悬挂
TCC的空回滚和业务悬挂
当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执
行了cancel操作，这时cancel不能做回滚，就是空回滚。
对于已经空回滚的业务，如果以后继续执行try，就永远不可能confirm或cancel，这就是业务悬挂。应当阻止
执行空回滚后的try操作，避免悬挂
业务分析
为了实现空回滚、防止业务悬挂，以及幂等性要求。我们必须在数据库记录冻结金额的同时，记录当前事务id和执行状态，为此我们设计了一张表（添加在微服务的数据库seata-demo中）：
DROP TABLE IF EXISTS `account_freeze_tbl`; CREATE TABLE `account_freeze_tbl` ( `xid` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `user_id` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `freeze_money` int(11) UNSIGNED NULL DEFAULT 0, `state` int(1) NULL DEFAULT NULL COMMENT '事务状态，0:try，1:confirm，2:cancel', PRIMARY KEY (`xid`) USING BTREE ) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = COMPACT; Try业务：记录冻结金额和事务状态到account_freeze表，扣减account表可用金额
Confirm业务：根据xid删除account_freeze表的冻结记录
Cancel业务：修改account_freeze表，冻结金额为0，state为2，修改account表，恢复可用金额
如何判断是否空回滚：cancel业务中，根据xid查询account_freeze，如果为null则说明try还没做，需要空回
滚
如何避免业务悬挂：try业务中，根据xid查询account_freeze ，如果已经存在则证明Cancel已经执行，拒绝执
行try业务
业务实现
1、声明TCC接口 @BusinessActionContextParameter()注解的参数才可以被BusinessActionContext获取到 package cn.itcast.account.service; import io.seata.rm.tcc.api.BusinessActionContext; import io.seata.rm.tcc.api.BusinessActionContextParameter; import io.seata.rm.tcc.api.LocalTCC; import io.seata.rm.tcc.api.TwoPhaseBusinessAction; /** * 项目名称：seata-demo * 描述：TCC实现接口 * * @author zhong * @date 2022-06-08 16:05 */ @LocalTCC public interface AccountTCCService { /** * 定义try,注释里面的值必须是与方法同名 * @param userId * @param money */ @TwoPhaseBusinessAction(name = "deduct",commitMethod = "confirm",rollbackMethod = "cancel") void deduct(@BusinessActionContextParameter(paramName = "userId") String userId, @BusinessActionContextParameter(paramName = "money")int money); /** * 定义Confirm * @param ctx 获取事务类型参数 * @return */ boolean confirm(BusinessActionContext ctx); /** * 定义Cancel * @param ctx * @return */ boolean cancel(BusinessActionContext ctx); } 说明：对于account_freeze_tbl数据库表的操作与其他业务一样的，使用MP的CURD进行快速开发，需要实体类、mapper
2.、创建接口实现类
package cn.itcast.account.service.impl; import cn.itcast.account.entity.AccountFreeze; import cn.itcast.account.mapper.AccountFreezeMapper; import cn.itcast.account.mapper.AccountMapper; import cn.itcast.account.service.AccountTCCService; import io.seata.core.context.RootContext; import io.seata.rm.tcc.api.BusinessActionContext; import lombok.extern.slf4j.Slf4j; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Transactional; /** * 项目名称：seata-demo * 描述： * * @author zhong * @date 2022-06-08 16:25 */ @Slf4j @Service public class AccountTCCServiceImpl implements AccountTCCService { /** * 注入可用余额dao */ @Autowired private AccountMapper accountMapper; /** * 注入冻结表dao */ @Autowired private AccountFreezeMapper freezeMapper; /** * 资源检测和预留 * 在数据库中设置了非负数字段限定，这里可以直接简化一步，如果为负数就会报错 * @param userId * @param money */ @Override @Transactional public void deduct(String userId, int money) { // 获取全局事务id String xid = RootContext.getXID(); // 业务悬挂判断，如果freeze中有冻结记录，一定是CANCEL执行过，我要拒绝业务 AccountFreeze accountFreeze = freezeMapper.selectById(xid); if(accountFreeze != null){ // 一定是CANCEL执行过，我要拒绝业务 return; } // 1、扣减可用余额 accountMapper.deduct(userId,money); // 2、记录冻结金额，事务状态 AccountFreeze freeze = new AccountFreeze(); freeze.setXid(xid); freeze.setUserId(userId); freeze.setState(AccountFreeze.State.TRY); freeze.setFreezeMoney(money); freezeMapper.insert(freeze); } @Override public boolean confirm(BusinessActionContext ctx) { // 1、获取事务id String xid = ctx.getXid(); // 2、根据事务id删除冻结数据 int count = freezeMapper.deleteById(xid); return count == 1; } @Override public boolean cancel(BusinessActionContext ctx) { // 0、查询冻结记录 String xid = ctx.getXid(); // 获取用户id String userId = ctx.getActionContext("userId").toString(); AccountFreeze freeze = freezeMapper.selectById(xid); // 空回滚的判断，为null代表没有try没有执行 if(freeze == null){ freeze = new AccountFreeze(); freeze.setXid(xid); freeze.setUserId(userId); freeze.setState(AccountFreeze.State.CANCEL); freeze.setFreezeMoney(0); freezeMapper.insert(freeze); } // 幂等判断 if(freeze.getState() == AccountFreeze.State.CANCEL){ // 已经执行过了一次CANCEL，无需重复处理 return true; } // 1、恢复可用余额 accountMapper.refund(freeze.getUserId(), freeze.getFreezeMoney()); // 2、将冻结金额清零，修改状态 freeze.setFreezeMoney(0); freeze.setState(AccountFreeze.State.CANCEL); int count = freezeMapper.updateById(freeze); return count == 1; } } 3、修改业务层连接方式 改为上面定义的TCCM业务实现
4、测试数据 直接测试错误的信息
当出现错误的时候会将会数据保存到数据库，如果是成功的那么就会删除
IDEA的报错输出
4、SAGA模式
Saga模式是SEATA提供的长事务解决方案。也分为两个阶段：
一阶段：直接提交本地事务 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚 Saga模式优点：
事务参与者可以基于事件驱动实现异步调用，吞吐高
一阶段直接提交事务，无锁，性能好
不用编写TCC中的三个阶段，实现简单
缺点：
软状态持续时间不确定，时效性差 没有锁，没有事务隔离，会有脏写</content></entry><entry><title>分布式事务：两阶段提交与三阶段提交</title><url>https://codingroam.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E4%B8%8E%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/</url><categories><category>分布式</category></categories><tags><tag>分布式事务</tag><tag>CAP理论</tag><tag>Learning</tag></tags><content type="html"> 分布式事务：两阶段提交与三阶段提交
分布式事务：两阶段提交与三阶段提交 在分布式系统中著有 CAP 理论，该理论由加州大学伯克利分校的 Eric Brewer 教授提出，阐述了在一个分布式系统中不可能同时满足 一致性（Consistency）、可用性（Availability），以及 分区容错性（Partition tolerance）。
C：一致性 在分布式系统中数据往往存在多个副本，一致性描述的是这些副本中的数据在内容和组织上的一致。
A：可用性 可用性描述了系统对用户的服务能力，所谓可用是指在用户容忍的时间范围内返回用户期望的结果。
P：分区容错性 分布式系统通常由多个节点构成，由于网络是不可靠的，所以存在分布式集群中的节点因为网络通信故障导致被孤立成一个个小集群的可能性，即网络分区，分区容错性要求在出现网络分区时系统仍然能够对外提供一致性的可用服务。
对于一个分布式系统而言，我们要始终假设网络是不可靠的，因此分区容错性是对一个分布式系统最基本的要求，我们的切入点更多的是尝试在可用性和一致性之间寻找一个平衡点，但这也并非要求我们在系统设计时一直建立在网络出现分区的场景之上，然后对一致性和可用性在选择时非此即彼。实际上 Eric Brewer 在 2012 年就曾指出 CAP 理论证明不能同时满足一致性、可用性，以及分区容错性的观点在实际系统设计指导上存在一定的误导性。传统对于 CAP 理论的理解认为在设计分布式系统时必须满足 P，然后在 C 和 A 之间进行取舍，这是片面的，实际中网络出现分区的可能性还是比较小的，尤其是目前网络环境正在变得越来越好，甚至许多系统都拥有专线支持，所以在网络未出现分区时，还是应该兼顾 A 和 C；另外就是对于一致性、可用性，以及分区容错性三者在度量上也应该有一个评定范围，最简单的以可用性来说，当有多少占比请求出现响应超时才可以被认为是不满足可用性，而不是一出现超时就认为是不可用的；最后我们需要考虑的一点就是分布式系统一般都是一个比较大且复杂的系统，我们应该从更小的粒度上对各个子系统进行评估和设计，而不是简单的从整体上认为需要满足 P，而在 A 和 C 之间做取舍，一些子系统可能需要尽可能同时满足三者。
让分布式集群始终对外提供可用的一致性服务一直是富有挑战和趣味的一项任务。暂且抛开可用性，拿一致性来说，对于关系型数据库我们通常利用事务来保证数据的强一致性，当我们的数据量越来越大，大到单库已经无法承担时，我们不得不采取分库分表的策略对数据库实现水平拆分，构建分布式数据库集群，这样可以将一个数据库的压力分摊到多个数据库，极大的提升了数据库的存储和响应能力，但是拆分之后也为我们使用数据库带来了许多的限制，比如主键的全局唯一、联表查询、数据聚合等等，另外一个相当棘手的问题就是数据库的事务由原先的单库事务变成了现在的分布式事务。
分布式事务的实现并不是很难，比如下文要展开的两阶段提交（2PC：Two-Phrase Commit）和三阶段提交（3PC：Three-Phrase Commit）都给我们提供了思路，但是如果要保证数据的强一致性，并要求对外提供可用的服务，就变成了一个几乎不可能的任务（至少目前是），因此很多分布式系统对于数据强一致性都敬而远之。
两阶段提交协议（2PC：Two-Phrase Commit） 两阶段提交协议的目标在于在分布式系统中保证数据的一致性，许多分布式系统采用该协议提供对分布式事务的支持（提供但不一定有人用，呵呵~）。顾名思义，该协议将一个分布式的事务过程拆分成两个阶段：投票阶段和事务提交阶段。为了让整个数据库集群能够正常的运行，该协议指定了一个“协调者”单点，用于协调整个数据库集群的运行，为了简化描述，我们将数据库里面的各个节点称为“参与者”，三阶段提交协议中同样包含“协调者”和“参与者”这两个定义。
第一阶段：投票阶段
该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下：
协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果。 事务参与者收到请求之后，执行事务，但不提交，并记录事务日志。 参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。 第二阶段：事务提交阶段
在第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在三种可能：
所有的参与者回复能够正常执行事务 一个或多个参与者回复事务执行失败 协调者等待超时。 对于第一种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：
协调者向各个参与者发送commit通知，请求提交事务。 参与者收到事务提交通知之后，执行commit操作，然后释放占有的资源。 参与者向协调者返回事务commit结果信息。 对于第二、三种情况，协调者均认为参与者无法正常成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下：
协调者向各个参与者发送事务rollback通知，请求回滚事务。 参与者收到事务回滚通知之后，执行rollback操作，然后释放占有的资源。 参与者向协调者返回事务rollback结果信息。 两阶段提交协议解决的是分布式数据库数据强一致性问题，其原理简单，易于实现，但是缺点也是显而易见的，主要缺点如下：
单点问题 协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，那么就会影响整个数据库集群的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。
同步阻塞 两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率及其低下。
数据不一致性 两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。
三阶段提交协议（2PC：Three-Phrase Commit） 针对两阶段提交存在的问题，三阶段提交协议通过引入一个“预询盘”阶段，以及超时策略来减少整个集群的阻塞时间，提升系统性能。三阶段提交的三个阶段分别为：can_commit，pre_commit，do_commit。
第一阶段：can_commit
该阶段协调者会去询问各个参与者是否能够正常执行事务，参与者根据自身情况回复一个预估值，相对于真正的执行事务，这个过程是轻量的，具体步骤如下：
协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复 各个参与者依据自身状况回复一个预估值，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否则返回否定信息 第二阶段：pre_commit
本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有三种：
所有的参与者都返回确定信息 一个或多个参与者返回否定信息 协调者等待超时 针对第一种情况，协调者会向所有参与者发送事务执行请求，具体步骤如下：
协调者向所有的事务参与者发送事务执行通知 参与者收到通知后，执行事务，但不提交 参与者将事务执行情况返回给客户端 在上面的步骤中，如果参与者等待超时，则会中断事务。 针对第二、三种情况，协调者认为事务无法正常执行，于是向各个参与者发出abort通知，请求退出预备状态，具体步骤如下：
协调者向所有事务参与者发送abort通知 参与者收到通知后，中断事务 第三阶段：do_commit
如果第二阶段事务未中断，那么本阶段协调者将会依据事务执行返回的结果来决定提交或回滚事务，分为三种情况：
所有的参与者都能正常执行事务 一个或多个参与者执行事务失败 协调者等待超时 针对第一种情况，协调者向各个参与者发起事务提交请求，具体步骤如下：
协调者向所有参与者发送事务commit通知 所有参与者在收到通知之后执行commit操作，并释放占有的资源 参与者向协调者反馈事务提交结果 针对第二、三种情况，协调者认为事务无法正常执行，于是向各个参与者发送事务回滚请求，具体步骤如下：
协调者向所有参与者发送事务rollback通知 所有参与者在收到通知之后执行rollback操作，并释放占有的资源 参与者向协调者反馈事务提交结果 在本阶段如果因为协调者或网络问题，导致参与者迟迟不能收到来自协调者的commit或rollback请求，那么参与者将不会如两阶段提交中那样陷入阻塞，而是等待超时后继续commit。相对于两阶段提交虽然降低了同步阻塞，但仍然无法避免数据的不一致性。
在分布式数据库中，如果期望达到数据的强一致性，那么服务基本没有可用性可言，这也是为什么许多分布式数据库提供了跨库事务，但也只是个摆设的原因，在实际应用中我们更多追求的是数据的弱一致性或最终一致性，为了强一致性而丢弃可用性是不可取的。</content></entry><entry><title>IO模型</title><url>https://codingroam.github.io/post/io%E6%A8%A1%E5%9E%8B/</url><categories><category>计算机原理</category><category>Linux/Unix</category></categories><tags><tag>IO模型</tag><tag>计算机原理</tag><tag>Linux/Unix</tag><tag>Learning</tag></tags><content type="html"> IO模型
IO模型 1. 引言 同步异步I/O，阻塞非阻塞I/O是程序员老生常谈的话题了，也是自己一直以来懵懵懂懂的一个话题。比如：何为同步异步？何为阻塞与非阻塞？二者的区别在哪里？阻塞在何处？为什么会有多种IO模型，分别用来解决问题？常用的框架采用的是何种I/O模型？各种IO模型的优劣势在哪里，适用于何种应用场景？
简而言之，对于I/O的认知，不能仅仅停留在字面上认识，了解内部玄机，才能深刻理解I/O，才能看清I/O相关问题的本质。
2. I/O 的定义 I/O 的全称是Input/Output。虽常谈及I/O，但想必你也一时不能给出一个完整的定义。搜索了谷歌，发现也尽是些冗长的论述。要想厘清I/O这个概念，我们需要从不同的视角去理解它。
2.1. 计算机视角 冯•诺伊曼计算机的基本思想中有提到计算机硬件组成应为五大部分：控制器，运算器，存储器，输入和输出。其中输入是指将数据输入到计算机的设备，比如键盘鼠标；输出是指从计算机中获取数据的设备，比如显示器；以及既是输入又是输出设备，硬盘，网卡等。
用户通过操作系统才能完成对计算机的操作。计算机启动时，第一个启动的程序是操作系统的内核，它将负责计算机的资源管理和进程的调度。换句话说：操作系统负责从输入设备读取数据并将数据写入到输出设备。
所以I/O之于计算机，有两层意思：
I/O设备 对I/O设备的数据读写 对于一次I/O操作，必然涉及2个参与方，一个输入端，一个输出端，而又根据参与双方的设备类型，我们又可以分为磁盘I/O，网络I/O（一次网络的请求响应，网卡）等。
2.2. 程序视角 应用程序作为一个文件保存在磁盘中，只有加载到内存到成为一个进程才能运行。应用程序运行在计算机内存中，必然会涉及到数据交换，比如读写磁盘文件，访问数据库，调用远程API等等。但我们编写的程序并不能像操作系统内核一样直接进行I/O操作。
因为为了确保操作系统的安全稳定运行，操作系统启动后，将会开启保护模式：将内存分为内核空间（内核对应进程所在内存空间）和用户空间，进行内存隔离。我们构建的程序将运行在用户空间，用户空间无法操作内核空间，也就意味着用户空间的程序不能直接访问由内核管理的I/O，比如：硬盘、网卡等。
但操作系统向外提供API，其由各种类型的系统调用（System Call）组成，以提供安全的访问控制。 所以应用程序要想访问内核管理的I/O，必须通过调用内核提供的系统调用(system call）进行间接访问。
所以I/O之于应用程序来说，强调的通过向内核发起系统调用完成对I/O的间接访问。换句话说应用程序发起的一次IO操作实际包含两个阶段：
IO调用阶段：应用程序进程向内核发起系统调用 IO执行阶段：内核执行IO操作并返回 2.1. 准备数据阶段：内核等待I/O设备准备好数据 2.2. 拷贝数据阶段：将数据从内核缓冲区拷贝到用户空间缓冲区 怎么理解准备数据阶段呢？ 对于写请求：等待系统调用的完整请求数据，并写入内核缓冲区； 对于读请求：等待系统调用的完整请求数据；（若请求数据不存在于内核缓冲区）则将外围设备的数据读入到内核缓冲区。
而应用程序进程在发起IO调用至内核执行IO返回之前，应用程序进程/线程所处状态，就是我们下面要讨论的第二个话题阻塞IO与非阻塞IO。
3. IO 模型之阻塞I/O(BIO) 应用程序中进程在发起IO调用后至内核执行IO操作返回结果之前，若发起系统调用的线程一直处于等待状态，则此次IO操作为阻塞IO。阻塞IO简称BIO，Blocking IO。其处理流程如下图所示：
从上图可知当用户进程发起IO系统调用后，内核从准备数据到拷贝数据到用户空间的两个阶段期间用户调用线程选择阻塞等待数据返回。
因此BIO带来了一个问题：如果内核数据需要耗时很久才能准备好，那么用户进程将被阻塞，浪费性能。为了提升应用的性能，虽然可以通过多线程来提升性能，但线程的创建依然会借助系统调用，同时多线程会导致频繁的线程上下文的切换，同样会影响性能。所以要想解决BIO带来的问题，我们就得看到问题的本质，那就是阻塞二字。
4. IO 模型之非阻塞I/O(NIO) 那解决方案自然也容易想到，将阻塞变为非阻塞，那就是用户进程在发起系统调用时指定为非阻塞，内核接收到请求后，就会立即返回，然后用户进程通过轮询的方式来拉取处理结果。也就是如下图所示：
应用程序中进程在发起IO调用后至内核执行IO操作返回结果之前，若发起系统调用的线程不会等待而是立即返回，则此次IO操作为非阻塞IO模型。非阻塞IO简称NIO，Non-Blocking IO。
然而，非阻塞IO虽然相对于阻塞IO大幅提升了性能，但依旧不是完美的解决方案，其依然存在性能问题，也就是频繁的轮询导致频繁的系统调用，会耗费大量的CPU资源。比如当并发很高时，假设有1000个并发，那么单位时间循环内将会有1000次系统调用去轮询执行结果，而实际上可能只有2个请求结果执行完毕，这就会有998次无效的系统调用，造成严重的性能浪费。有问题就要解决，那NIO问题的本质就是频繁轮询导致的无效系统调用。
5. IO模型之IO多路复用 解决NIO的思路就是降解无效的系统调用，如何降解呢？我们一起来看看以下几种IO多路复用的解决思路。
5.1. IO多路复用之select/poll 系统调用 select()或Poll()会一直阻塞，直到一个或多个文件描述符集合成为就绪态。
select()函数：
poll()函数：
系统调用 poll()执行的任务同 select()很相似。两者间主要的区别在于我们要如何指定待检查的文件描述符。在 select()中，我们提供三个集合，在每个集合中标明我们感兴趣的文件描述符。而在 poll()中我们提供一列文件描述符，并在每个文件描述符上标明我们感兴趣的事件
select()和 poll()之间的一些区别：
select()所使用的数据类型 fd_set 对于被检查的文件描述符数量有一个上限，在 Linux 下，这个上限值默认为 1024，修改这个上限需要重新编译。与之相反，poll()对于被检查的文件描述符数量本质上是没有限制的
select()的参数 fd_set 同时也是保存调用结果的地方，如果要在循环中重复调用select()的话，我们必须每次都要重新初始化 fd_set。而 poll()通过独立的两个字段 events（针对输入）和 revents（针对输出）来处理，从而避免每次都要重新初始化参数
Select是内核提供的系统调用，它支持一次查询多个系统调用的可用状态，当任意一个结果状态可用时就会返回，用户进程再发起一次系统调用进行数据读取。换句话说，就是NIO中N次的系统调用，借助Select，只需要发起一次系统调用就够了。其IO流程如下所示：
select/epoll 虽然解决了NIO重复无效系统调用用的问题，但同时又引入了新的问题。问题是：
每次调用 select()或 poll()，内核都必须检查所有被指定的文件描述符，看它们是否处于就绪态。当检查大量处于密集范围内的文件描述符时，该操作耗费的时间将大大超过接下来的操作
每次调用 select()或 poll()时，程序都必须传递一个表示所有需要被检查的文件描述符的数据结构到内核，内核检查过描述符后，修改这个数据结构并返回给程序。内核与用户进程数据传递较多
select()或 poll()调用完成后，程序必须检查返回的数据结构中的每个元素，以此查明
哪个文件描述符处于就绪态了
换句话说，select/poll虽然减少了用户进程的发起的系统调用，但内核的工作量只增不减。在高并发的情况下，内核的性能问题依旧。所以select/poll的问题本质是：内核存在无效的循环遍历。
5.2. IO多路复用之epoll 针对select/pool引入的问题，我们把解决问题的思路转回到内核上，如何减少内核重复无效的循环遍历呢？变主动为被动，基于事件驱动来实现。其流程图如下所示：
epoll API 由以下 3 个系统调用组成：
epoll_create()创建一个 epoll 实例，返回代表该实例的文件描述符。
epoll_ctl()操作同 epoll 实例相关联的兴趣列表。通过 epoll_ctl()，我们可以增加新的描述符到列表中，将已有的文件描述符从该列表中移除，以及修改代表文件描述符上事件类型的位掩码。
epoll_wait()返回与 epoll 实例相关联的就绪列表中的成员。单个 epoll_wait()调用能返回多个就绪态文件描述符的信息。
epoll与select/poll比较：
每次调用 select()和 poll()时，内核必须检查所有在调用中指定的文件描述符。与之相反，当通过 epoll_ctl()指定了需要监视的文件描述符时，内核会在与打开的文件描述上下文相关联的列表中记录该描述符。之后每当执行 I/O 操作使得文件描述符成为就绪态时，内核就在 epoll 描述符的就绪列表中添加一个元素。（单个打开的文件描述上下文中的一次 I/O 事件可能导致与之相关的多个文件描述符成为就绪态。）之后的epoll_wait()调用从就绪列表中简单地取出这些元素。
每次调用 select()或 poll()时，我们传递一个标记了所有待监视的文件描述符的数据结构给内核，调用返回时，内核将所有标记为就绪态的文件描述符的数据结构再传回给我们。与之相反，在 epoll 中我们使用 epoll_ctl()在内核空间中建立一个数据结构，该数据结构会将待监视的文件描述符都记录下来。一旦这个数据结构建立完成，稍后每次调用 epoll_wait()时就不需要再传递任何与文件描述符有关的信息给内核了，而调用返回的信息中只包含那些已经处于就绪态的描述符
epoll，已经大大优化了IO的执行效率，但在IO执行的第一阶段：数据准备阶段都还是被阻塞的。所以这是一个可以继续优化的点。
6. IO 模型之信号驱动IO(SIGIO) 信号驱动IO与BIO和NIO最大的区别就在于，在IO执行的数据准备阶段，不会阻塞用户进程。 如下图所示：当用户进程需要等待数据的时候，会向内核发送一个信号，告诉内核我要什么数据，然后用户进程就继续做别的事情去了，而当内核中的数据准备好之后，内核立马发给用户进程一个信号，说”数据准备好了，快来查收“，用户进程收到信号之后，立马调用recvfrom，去查收数据。
乍一看，信号驱动式I/O模型有种异步操作的感觉，但是在IO执行的第二阶段，也就是将数据从内核空间复制到用户空间这个阶段，用户进程还是被阻塞的。
综上，你会发现，不管是BIO还是NIO还是SIGIO，它们最终都会被阻塞在IO执行的第二阶段。 那如果能将IO执行的第二阶段变成非阻塞，那就完美了。
7. IO 模型之异步IO(AIO) 异步IO真正实现了IO全流程的非阻塞。用户进程发出系统调用后立即返回，内核等待数据准备完成，然后将数据拷贝到用户进程缓冲区，然后发送信号告诉用户进程IO操作执行完毕（与SIGIO相比，一个是发送信号告诉用户进程数据准备完毕，一个是IO执行完毕）。其流程如下：
所以，之所以称为异步IO，取决于IO执行的第二阶段是否阻塞。因此前面讲的BIO，NIO和SIGIO均为同步IO。</content></entry><entry><title>NIO的阻塞IO模式、非阻塞IO模式、IO多路复用模式的使用</title><url>https://codingroam.github.io/post/nio%E7%9A%84%E9%98%BB%E5%A1%9Eio%E6%A8%A1%E5%BC%8F%E9%9D%9E%E9%98%BB%E5%A1%9Eio%E6%A8%A1%E5%BC%8Fio%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E6%A8%A1%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url><categories><category>Java</category></categories><tags><tag>IO模型</tag><tag>Java</tag><tag>NIO</tag></tags><content type="html"> NIO的阻塞IO模式、非阻塞IO模式、IO多路复用模式的使用
NIO的阻塞IO模式、非阻塞IO模式、IO多路复用模式的使用 NIO虽然称为Non-Blocking IO（非阻塞IO），但它支持阻塞IO、非阻塞IO和IO多路复用模式这几种方式的使用。
同步阻塞模式 NIO服务器端
@Slf4j public class NIOBlockingServer { public static void main(String[] args) throws IOException { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(true); // 设置SocketChannel为阻塞模式（默认就是阻塞模式） serverSocketChannel.bind(new InetSocketAddress(8080)); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); while (true) { // 如果没有接收到新的线程，这里会阻塞，无法及时处理其他已连接Channel的请求 SocketChannel socketChannel = serverSocketChannel.accept(); log.info("receive connection from client. client:{}",socketChannel.getRemoteAddress()); socketChannel.configureBlocking(true); // 设置SocketChannel为阻塞模式（默认就是阻塞模式） // 如果读不到数据，这里会阻塞，无法及时处理其他Channel的请求 int length = socketChannel.read(byteBuffer); log.info("receive message from client. client:{} message:{}",socketChannel.getRemoteAddress(),new String(byteBuffer.array(),0,length,"UTF-8")); byteBuffer.clear(); } } } NIO客户端
@Slf4j public class NIOClient { @SneakyThrows public static void main(String[] args) { SocketChannel socketChannel=SocketChannel.open(); try { socketChannel.connect(new InetSocketAddress("127.0.0.1", 8080)); log.info("client connect finished"); ByteBuffer writeBuffer=ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)); socketChannel.write(writeBuffer); log.info("client send finished"); } catch (Exception e) { e.printStackTrace(); } finally { socketChannel.close(); } } } NIO阻塞模式的使用，乍一看怎么跟BIO的使用方法很像？不是很像，简直是一模一样~
以Run模式启动NIO服务端 在客户端的 socketChannel.write(writeBuffer);处打上断点，以Debug模式运行一个客户端A，执行到断点时，服务端已经接收到客户端A的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:64334 ） 再以Debug模式运行一个客户端B，服务端没反应，因为这时客户端A还没发送数据，所以服务端目前是在 int length = socketChannel.read(byteBuffer) 的地方阻塞了（还在等着接收客户端A发送数据） 再以Debug模式运行一个客户端C，服务端同样没反应 让客户端A继续运行完，发现服务端读取到客户端A的数据（打印了receive message from client. client:/127.0.0.1:64334 message:hello ）后，才能接收到客户端B的连接（打印了receive connection from client. client:/127.0.0.1:64358 ） 让客户端B继续运行完，发现服务端读取到客户端B的数据（打印了receive message from client. client:/127.0.0.1:64358 message:hello ）后，才能接收到客户端C的连接（打印了receive connection from client. client:/127.0.0.1:64369 ） 因此，NIO的阻塞IO模式跟BIO一样，最大的缺点就是阻塞。
同步非阻塞模式 通过前面的学习我们知道，异步IO和同步IO最大的区别就是： 同步IO在做完一件事（比如：处理客户端连接请求+写请求）之前，只能等待，无法做其他事情； 而异步是在客户端某个事件没有就绪时，我服务端可以先处理其他的客户端请求，不用一直等着。
NIO服务端
@Slf4j public class NIONonBlockingServer { public static void main(String[] args) throws IOException, InterruptedException { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(8080)); List&lt;SocketChannel> socketChannelList = new ArrayList&lt;>(); while (true) { // 如果没有接收到新的线程，这里不会阻塞，会返回null，可以让线程继续处理其他Channel的请求 SocketChannel socketChannel = serverSocketChannel.accept(); if (Objects.nonNull(socketChannel)) { log.info("receive connection from client. client:{}", socketChannel.getRemoteAddress()); socketChannel.configureBlocking(false); socketChannelList.add(socketChannel); } for (SocketChannel channel : socketChannelList) { // 如果没有读到数据，这里也不会阻塞，会返回0，表示没有读到数据，可以让线程继续处理其他Channel的请求 ByteBuffer byteBuffer = ByteBuffer.allocate(10); int length = channel.read(byteBuffer); if (length > 0) { log.info("receive message from client. client:{} message:{}", channel.getRemoteAddress() , new String(byteBuffer.array(), 0, length, "UTF-8")); } byteBuffer.clear(); } // 为了避免没有客户端请求时循环过于频繁，把所有就绪的事件循环处理完后，停顿1秒再继续执行 Thread.sleep(1000); } } } NIO客户端
@Slf4j public class NIOClient { @SneakyThrows public static void main(String[] args) { SocketChannel socketChannel=SocketChannel.open(); try { socketChannel.connect(new InetSocketAddress("127.0.0.1", 8080)); log.info("client connect finished"); ByteBuffer writeBuffer=ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)); socketChannel.write(writeBuffer); log.info("client send finished"); } catch (Exception e) { e.printStackTrace(); } finally { socketChannel.close(); } } } 以Run模式启动NIO服务端
在客户端的 socketChannel.write(writeBuffer);处打上断点，以Debug模式运行一个客户端A，执行到断点时，服务端已经接收到客户端A的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:53004 ）
再以Debug模式运行一个客户端B，服务端也接收到客户端B的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:53032 ）
再以Debug模式运行一个客户端C，服务端也接收到客户端B的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:53032 ） 如下图：
继续运行客户端A、B、C，可以看到服务端也可以正常接收它们发来的数据：
2022-07-30 16:31:07.987 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive connection from client. client:/127.0.0.1:53004 2022-07-30 16:31:13.014 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive connection from client. client:/127.0.0.1:53032 2022-07-30 16:31:18.039 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive connection from client. client:/127.0.0.1:53060 2022-07-30 16:33:12.919 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive message from client. client:/127.0.0.1:53004 message:hello 2022-07-30 16:33:18.940 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive message from client. client:/127.0.0.1:53032 message:hello 2022-07-30 16:33:19.942 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive message from client. client:/127.0.0.1:53060 message:hello NIO非阻塞模式这种用法跟 BIO多线程处理请求的方式类似，让服务端可以同时处理多个客户端请求，即使某一个客户端的读/写事件未就绪也不会阻塞线程（比如上面服务端执行serverSocketChannel.accept()时如果没有客户端连接不会阻塞而是会返回null；执行channel.read(byteBuffer)时如果读不到数据不会阻塞而是会返回0），而是会继续处理其他客户端的请求。
需要注意的是，这里的非阻塞，是指serverSocketChannel执行accept()、socketChannel执行read()时是非阻塞的（会立刻返回结果）。但是在客户端有就绪事件，处理客户端的请求时，比如服务端接收客户端连接请求的过程、服务端读取数据（数据拷贝）的过程，是阻塞的。
IO多路复用模式 看完NIO非阻塞模式的使用方法你是不是就觉得万无一失了？No！这种方式也有一个很大的缺点就是，当一直没有客户端事件就绪时，服务端线程就会一直循环，白白占用了CPU资源，所以上面代码中为了减小CPU消耗，在每次处理完所有Channel的就绪事件后，会调用Thread.sleep(1000);让服务端线程休息1秒再执行。那有没有什么方法可以在没有客户端事件就绪时，服务端线程等待，当有了请求再继续工作呢？
有，那就是IO多路复用模式，相对于上面的非阻塞模式，IO多路复用模式主要是引入了Selector选择器，且需要把Channel设置为非阻塞模式（默认是阻塞的）。
Selector可以作为一个观察者，可以把已知的Channel（无论是服务端用来监听客户端连接的ServerSocketChannel，还是服务端和客户端用来读写数据的SocketChannel）及其感兴趣的事件（READ、WRITE、CONNECT、ACCEPT）包装成一个SelectionKey，注册到Selector上，Selector就会监听这些Channel注册的事件（监听的时候如果没有事件就绪，Selector所在线程会被阻塞），一旦有事件就绪，就会返回这些事件的列表，继而服务端线程可以依次处理这些事件。
服务端例子如下：
@Slf4j public class NioSelectorServer { public static void main(String[] args) throws Exception { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress("127.0.0.1", 8080), 50); Selector selector = Selector.open(); SelectionKey serverSocketKey = serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { // 从Selector中获取事件（客户端连接、客户端发送数据……），如果没有事件发生，会阻塞 int count = selector.select(); log.info("select event count:" + count); Set&lt;SelectionKey> selectionKeys = selector.selectedKeys(); // Iterator&lt;SelectionKey> iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); // 有客户端请求建立连接 if (selectionKey.isAcceptable()) { handleAccept(selectionKey); } // 有客户端发送数据 else if (selectionKey.isWritable()) { handleRead(selectionKey); } // select 在事件发生后，就会将相关的 key 放入 Selector 中的 selectedKeys 集合，但不会在处理完后从 selectedKeys 集合中移除，需要我们自己手动删除 iterator.remove(); } } } private static void handleAccept(SelectionKey selectionKey) throws IOException { ServerSocketChannel serverSocketChannel = (ServerSocketChannel) selectionKey.channel(); SocketChannel socketChannel = serverSocketChannel.accept(); if (Objects.nonNull(socketChannel)) { log.info("receive connection from client. client:{}", socketChannel.getRemoteAddress()); // 设置客户端Channel为非阻塞模式，否则在执行socketChannel.read()时会阻塞 socketChannel.configureBlocking(false); Selector selector = selectionKey.selector(); socketChannel.register(selector, SelectionKey.OP_READ); } } private static void handleRead(SelectionKey selectionKey) throws IOException { SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(8); int length = socketChannel.read(readBuffer); if (length > 0) { log.info("receive message from client. client:{} message:{}", socketChannel.getRemoteAddress() , new String(readBuffer.array(), 0, length, "UTF-8")); } else if (length == -1) { // 客户端正常断开(socketChannel.close())时，在服务端也会产生读事件，且读到的数据长度为-1 socketChannel.close(); return; } } } SelectionKey表示一对Selector和Channel的关系，从SelectionKey中可以获得已经准备好数据的Channel。 SelectionKey.OP_ACCEPT —— 针对服务端，接收连接就绪事件，表示服务器监听到了客户连接 SelectionKey.OP_CONNECT —— 针对客户端，连接就绪事件，表示客户与服务器的连接已经建立就绪 SelectionKey.OP_READ —— 读就绪事件，表示通道中已经有了可读的数据，可以执行读操作 SelectionKey.OP_WRITE —— 写就绪事件，表示已经可以向通道写数据了（通道目前可以用于写操作）
以Debug模式启动服务端，初始化完ServerSocketChannel后，手动设置了ServerSocketChannel的阻塞模式为非阻塞，并且为ServerSocketChannel在Selector上注册了一个ACCEPT事件，当有客户端向服务端请求连接时会触发该事件。当执行到int count = selector.select();时，服务端阻塞，等待客户端连接 以Debug模式运行一个客户端A，当执行完socketChannel.connect(new InetSocketAddress("127.0.0.1", 8080));时，服务端selector.select()方法返回了就绪的IO事件数量为1（就是客户端A的请求连接事件） 当服务端接收到客户端A的连接后，把客户端连接——SocketChannel设置为非阻塞，并且在Selector实例上注册一个读事件，这时客户端连接SocketChannel会对读事件感兴趣，当这个客户端发送数据时，会唤醒Selector。当服务端下一次循环再次执行到int count = selector.select();时，会再次阻塞，等待客户端的IO事件 客户端A继续执行完socketChannel.write(writeBuffer);后，服务端selector.select()方法返回了就绪的IO事件数量为1（就是客户端A的写数据事件） 当服务端在读取客户端A的数据时（下次执行selector.select()之前），同时启动客户端B、客户端C（或者再多开几个线程，否则可能模拟不出来），等服务端下次执行selector.select()时，返回的就绪的IO事件数量可能有多个，然后可以根据 selectionKey.isAcceptable()、 selectionKey.isReadable()、selectionKey.isWritable()来分别处理对应的事件。 但是，如果客户端连接或读写时间过长，也只能一个一个处理。NIO只是把BIO中等待的时间（比如socket.getInputStream().read()）充分利用，为在多核CPU机器上的运行提高了效率，可以用多线程+NIO的IO多路复用模式来处理。</content></entry><entry><title>装饰者模式</title><url>https://codingroam.github.io/post/%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/</url><categories><category>设计模式</category></categories><tags><tag>设计模式</tag></tags><content type="html"> 装饰者模式
装饰者模式 ​ 装饰者模式的核心思想是通过创建一个装饰对象（即装饰者），动态扩展目标对象的功能，并且不会改变目标对象的结构，提供了一种比继承更灵活的替代方案。
​ 我们在进行软件开发时要想实现可维护、可扩展，就需要尽量复用代码，并且降低代码的耦合度，而设计模式就是一种可以提高代码可复用性、可维护性、可扩展性以及可读性的解决方案。
大家熟知的23种设计模式，可以分为创建型模式、结构型模式和行为型模式三大类。其中，结构型模式用于设计类或对象的组合方式，以便实现更加灵活的结构。结构型模式又可划分为类结构型模式和对象结构型模式，前者通过继承来组合接口或类，后者通过组合或聚合来组合对象
装饰者模式的核心思想是通过创建一个装饰对象（即装饰者），动态扩展目标对象的功能，并且不会改变目标对象的结构，提供了一种比继承更灵活的替代方案。需要注意的是，装饰对象要与目标对象实现相同的接口，或继承相同的抽象类；另外装饰对象需要持有目标对象的引用作为成员变量，而具体的赋能任务往往通过带参构造方法来完成。
▐ 结构 装饰者模式包含四种类，分别是抽象构件类、具体构件类、抽象装饰者类、具体装饰者类，它们各自负责完成特定任务，并且相互之间存在紧密联系。
▐ 使用 有了上述的基本概念，我们将装饰者模式的使用步骤概括为：
step1：创建抽象构件类，定义目标对象的抽象类、将要扩展的功能定义成抽象方法；
step2：创建具体构件类，定义目标对象的实现类，实现抽象构件中声明的抽象方法；
step3：创建抽象装饰者类，维护一个指向抽象构件的引用，并传入构造函数以调用具体构件的实现方法，给具体构件增加功能；
step4：创建具体装饰者类，可以调用抽象装饰者类中定义的方法，并定义若干个新的方法，扩展目标对象的功能。
我们在淘宝上购物时，经常会遇到很多平台和商家的优惠活动：满减、聚划算站内的百亿补贴券、店铺折扣等等。那么在商品自身原价的基础上，叠加了多种优惠活动后，后台应该怎样计算最终的下单结算金额呢？下面就以这种优惠叠加结算的场景为例，简单分析装饰者模式如何使用。
// 定义抽象构件：抽象商品 public interface ItemComponent { // 商品价格 public double checkoutPrice(); } // 定义具体构件：具体商品 public class ConcreteItemCompoment implements ItemComponent { // 原价 @Override public double checkoutPrice() { return 200.0; } } // 定义抽象装饰者：创建传参(抽象构件)构造方法，以便给具体构件增加功能 public abstract class ItemAbsatractDecorator implements ItemComponent { protected ItemComponent itemComponent; public ItemAbsatractDecorator(ItemComponent myItem) { this.itemComponent = myItem; } @Overrid public double checkoutPrice() { return this.itemComponent.checkoutPrice(); } } // 定义具体装饰者A：增加店铺折扣八折 public class ShopDiscountDecorator extends ItemAbsatractDecorator { public ShopDiscountDecorator(ItemComponent myItem) { super(myItem); } @Override public double checkoutPrice() { return 0.8 * super.checkoutPrice(); } } // 定义具体装饰者B：增加满200减20功能，此处忽略判断逻辑 public class FullReductionDecorator extends ItemAbsatractDecorator { public FullReductionDecorator(ItemComponent myItem) { super(myItem); } @Override public double checkoutPrice() { return super.checkoutPrice() - 20; } } // 定义具体装饰者C：增加百亿补贴券50 public class BybtCouponDecorator extends ItemAbsatractDecorator { public BybtCouponDecorator(ItemComponent myItem) { super(myItem); } @Override public double checkoutPrice() { return super.checkoutPrice() - 50; } } //客户端调用 public class userPayForItem() { public static void main(String[] args) { ItemCompoment item = new ConcreteItemCompoment(); System.out.println("宝贝原价：" + item.checkoutPrice() + " 元"）; item = new ShopDiscountDecorator(item); System.out.println("使用店铺折扣后需支付：" + item.checkoutPrice() + " 元"）; item = new FullReductionDecorator(item); System.out.println("使用满200减20后需支付：" + item.checkoutPrice() + " 元"）; item = new BybtCouponDecorator(item); System.out.println("使用百亿补贴券后需支付：" + item.checkoutPrice() + " 元"）; } } ▐ 结果输出 宝贝原价：200.0 元 使用店铺折扣后需支付：160.0 元 使用满200减20后需支付：140.0 元 使用百亿补贴券后需支付：90.0 元 ▐ UML图 ▐ 比较分析
VS 继承 装饰者模式和继承关系都是要对目标类进行功能扩展，但装饰模式可以提供比继承更多的灵活性：继承是静态添加功能，在系统运行前就会确定下来；装饰者模式是动态添加、删除功能。
比如，一个对象需要具备 10 种功能，但客户端可能要求分阶段使用对象功能：在第一阶段只执行第 1-8 项功能，第二阶段执行第 3-10 项功能，这种场景下只需先定义好第 3-8 项功能方法。在程序运行的第一个阶段，使用具体装饰者 A 添加 1、2 功能；在第二个运行阶段，使用具体装饰者 B 添加 9、10 功能。而继承关系难以实现这种需求，它必须在编译期就定义好要使用的功能。
VS 代理模式 装饰者模式常常被拿来和代理模式比较，两者都要实现目标类的相同接口、声明一个目标对象，并且都可以在不修改目标类的前提下进行方法扩展，整体设计思路非常相似。那么两者的区别是什么呢？
首先，装饰者模式的重点在于增强目标对象功能，而代理模式的重点在于保护和隐藏目标对象。其中，装饰者模式需要客户端明确知道目标类，才能对其功能进行增强；代理模式要求客户端对目标类进行透明访问，借助代理类来完成相关控制功能（如日志记录、缓存设置等），隐藏目标类的具体信息。可见，代理类与目标类的关系往往在编译时就确定下来，而装饰者类在运行时动态构造而成。
其次，两者获取目标类的方式不同。装饰者模式是将目标对象作为参数传给构造方法，而代理模式是通过在代理类中创建目标对象的一个实例。
最后，通过上述示例可发现，装饰者模式会使用一系列具体装饰者类来增强目标对象的功能，产生了一种连续、叠加的效应；而代理模式是在代理类中一次性为目标对象添加功能。
﻿
VS 适配器模式 两者都属于包装式行为，即当一个类不能满足需求时，创建辅助类进行包装以满足变化的需求。但是装饰者模式的装饰者类和被装饰类都要实现相同接口，或者装饰类是被装饰类的子类；而适配器模式中，适配器和被适配的类可以有不同接口，并且可能会有部分接口重合。
▐ JDK源码赏析 Java I/O标准库是装饰者模式在Java语言中非常经典的应用实例。
如下图所示，InputStream 相当于抽象构件，FilterInputStream 类似于抽象装饰者，它的四个子类等同于具体装饰者。其中，FilterInputStream 中含有被装饰类 InputStream 的引用，其具体装饰者及各自功能为：PushbackInputStream 能弹出一个字节的缓冲区，可将输入流放到回退流中；DataInputStream 与 DataOutputStream搭配使用，用来装饰其它输入流，允许应用程序以一种与机器无关的方式从底层输入流中读取基本 Java 数据类型；BufferedInputStream 使用缓冲数组提供缓冲输入流功能，在每次调用 read() 方法时优先从缓冲区读取数据，比直接从物理数据源读取数据的速度更快；LineNumberInputStream 提供输入流过滤功能，可以跟踪输入流中的行号（以回车符、换行符标记换行）。
FilterInputStream 是所有装饰器类的抽象类，提供特殊的输入流控制。下面源码省略了 skip、available、mark、reset、markSupported 方法，这些方法也都委托给了 InputStream 类。其中， InputStream 提供装饰器类的接口，因而此类并没有对 InputStream 的功能做任何扩展，其扩展主要交给其子类来实现。
public class FilterInputStream extends InputStream { //维护一个 InputStream 对象 protected volatile InputStream in; //构造方法参数需要一个 inputStream protected FilterInputStream(InputStream in) { this.in = in; } //委托给 InputStream public int read() throws IOException { return in.read(); } //委托给 InputStream public void close() throws IOException { in.close(); } ....... } 由于源码太长，这里先以 PushbackInputStream 为例，展示 FilterInputStream 的具体装饰者的底层实现，大家感兴趣的话可以自行查阅其它源码哦。PushbackInputStream 内部维护了一个 pushback buf 缓冲区，可以帮助我们试探性地读取数据流，对于不想要的数据也可以返还回去。
public class PushbackInputStream extends FilterInputStream { //缓冲区 protected byte[] buf; protected int pos; private void ensureOpen() throws IOException { if (in == null) throw new IOException("Stream closed"); } //构造函数可以指定返回的字节个数 public PushbackInputStream(InputStream in, int size) { super(in); if (size &lt;= 0) { throw new IllegalArgumentException("size &lt;= 0"); } //初始化缓冲区的大小 this.buf = new byte[size]; //设置读取的位置 this.pos = size; } //默认回退一个 public PushbackInputStream(InputStream in) { this(in, 1); } public int read() throws IOException { //确保流存在 ensureOpen(); //如果要读取的位置在缓冲区里面 if (pos &lt; buf.length) { //返回缓冲区中的内容 return buf[pos++] &amp; 0xff; } //否则调用超类的读函数 return super.read(); } //读取指定的长度 public int read(byte[] b, int off, int len) throws IOException { ensureOpen(); if (b == null) { throw new NullPointerException(); } else if (off &lt; 0 || len &lt; 0 || len > b.length - off) { throw new IndexOutOfBoundsException(); } else if (len == 0) { return 0; } //缓冲区长度减去读取位置 int avail = buf.length - pos; //如果大于0，表明部分数据可以从缓冲区读取 if (avail > 0) { //如果要读取的长度小于可从缓冲区读取的字符 if (len &lt; avail) { //修改可读取值为实际要读的长度 avail = len; } //将buf中的数据复制到b中 System.arraycopy(buf, pos, b, off, avail); //修改pos的值 pos += avail; //修改off偏移量的值 off += avail; //修改len的值 len -= avail; } //如果从缓冲区读取的数据不够 if (len > 0) { //从流中读取 len = super.read(b, off, len); if (len == -1) { return avail == 0 ? -1 : avail; } return avail + len; } return avail; } //不读字符b public void unread(int b) throws IOException { ensureOpen(); if (pos == 0) { throw new IOException("Push back buffer is full"); } //实际就是修改缓冲区中的值，同时pos后退 buf[--pos] = (byte)b; } public void unread(byte[] b, int off, int len) throws IOException { ensureOpen(); if (len > pos) { throw new IOException("Push back buffer is full"); } //修改缓冲区中的值，pos后退多个 pos -= len; System.arraycopy(b, off, buf, pos, len); } public void unread(byte[] b) throws IOException { unread(b, 0, b.length); } } ▐ 优点
提供比继承更加灵活的扩展功能，通过叠加不同的具体装饰者的方法，动态地增强目标类的功能。
装饰者和被装饰者可以独立发展，不会相互耦合，比如说我们想再加一个炒河粉只需创建一个炒河粉类继承FastFood即可，而想要增加火腿肠配料就增加一个类去继承 Garnish 抽象装饰者。
▐ 缺点
使用装饰模式，可以比使用继承关系创建更少的类，使设计比较易于进行。然而，多层装饰会产生比继承更多的对象，使查错更加困难，尤其是这些对象都很相似。而且，当目标类被多次动态装饰后，程序的复杂性也会大大提升，难以维护。
▐ 适用场景
﻿
继承关系不利于系统维护，甚至不能使用继承关系的场景。比如，当继承导致类爆炸时、目标类被 final 修饰时，都不宜通过创建目标类的子类来扩展功能。
要求不影响其他对象，为特定目标对象添加功能。
要求动态添加、撤销对象的功能。
▐ 总结
装饰者模式也是一种比较容易理解和上手的设计模式，它可以对多个装饰者类进行花式排列组合，适应多变的用户需求。同时，装饰者模式也是符合开闭原则的，被装饰的对象和装饰者类互相独立、互不干扰。
在介绍装饰者模式的适用场景时，我们可以发现上述场景在实际工程中也比较常见，因此装饰者模式同样应用广泛。除了本文提到的 Java I/O，装饰者模式的典型应用实例还有：Spring cache 中的 TransactionAwareCacheDecorator 类、 Spring session 中的 ServletRequestWrapper 类、Mybatis 缓存中的 decorators 包等等。</content></entry><entry><title>git上传代码报错ssh: connect to host github.com port 22: Connection timed out解决办法</title><url>https://codingroam.github.io/post/git%E4%B8%8A%E4%BC%A0%E4%BB%A3%E7%A0%81%E6%8A%A5%E9%94%99ssh-connect-to-host-github.com-port-22-connection-timed-out%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url><categories><category>Git</category></categories><tags><tag>Git</tag><tag>Problem-Solving</tag></tags><content type="html"> git上传代码报错ssh: connect to host github.com port 22: Connection timed out解决办法
git上传代码报错ssh: connect to host github.com port 22: Connection timed out解决办法 当在远程库上设置了SSH 之后还是报错连接超时，问题如下
$ git push 报错：
ssh: connect to host github.com port 22: Connection timed out fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个时候需要检查一下SSH是否能够连接成功，输入以下命令
ssh -T git@github.com 稍等片刻如果继续报错，如下：
ssh: connect to host github.com port 22: Connection timed out则，可以使用一下解决办法
打开存放ssh的目录
cd ~/.ssh ls 查看是否存在 id_rsa id_rsa.pun known_hosts 三个文件，如果没有请查看Git设置SSH方式
，如果有请按照以下方式设置
1. vim config (进入到vim编辑界面，如果是windows系统powershell，可以设置使用vim,参考文章powershell设置vim
）
2. insert 编辑模式
Host github.com User YourEmail@163.com Hostname ssh.github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa Port 443 其中， YourEmail 为绑定的邮箱。
保存之后再次执行"ssh -T git@github.com
"时，会出现如下提示，回车"yes"即可</content></entry><entry><title>Java伪共享</title><url>https://codingroam.github.io/post/java%E4%BC%AA%E5%85%B1%E4%BA%AB/</url><categories><category>Java</category><category>计算机原理</category></categories><tags><tag>Java</tag><tag>计算机原理</tag><tag>Learning</tag></tags><content type="html"> Java伪共享
Java伪共享 维基百科对伪共享的定义如下：
In computer science, false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that will never be altered by another party, but those data shares a cache block with data that are altered, the caching protocol may force the first participant to reload the whole unit despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource
其大致意思是： CPU的缓存是以缓存行(cache line)为单位进行缓存的，当多个线程修改不同变量，而这些变量又处于同一个缓存行时就会影响彼此的性能。例如：线程1和线程2共享一个缓存行，线程1只读取缓存行中的变量1，线程2修改缓存行中的变量2，虽然线程1和线程2操作的是不同的变量，由于变量1和变量2同处于一个缓存行中，当变量2被修改后，缓存行失效，线程1要重新从主存(或者低级cache)中读取，因此导致缓存失效，从而产生性能问题。为了更深入一步理解伪共享，我们先看一下CPU缓存
1、Cpu三级缓存 CPU的速度要远远大于内存的速度，为了解决这个问题，CPU引入了三级缓存：L1，L2和L3三个级别，L1最靠近CPU，L2次之，L3离CPU最远，L3之后才是主存。速度是L1>L2>L3>主存。越靠近CPU的容量越小。CPU获取数据会依次从三级缓存中查找，如果找不到再从主存中加载。
当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。一般来说，每级缓存的命中率大概都在80%左右，也就是说全部数据量的80%都可以在一级缓存中找到，只剩下20%的总数据量才需要从二级缓存、三级缓存或内存中读取，由此可见一级缓存是整个CPU缓存架构中最为重要的部分
2、MESI协议（高速缓存一致性协议） 缓存行状态 CPU的缓存是以缓存行(cache line)为单位的，MESI协议描述了多核处理器中一个缓存行的状态。在MESI协议中，每个缓存行有4个状态，分别是：
M（修改，Modified）：本地处理器已经修改缓存行，即是脏行，它的内容与内存中的内容不一样，并且此 cache 只有本地一个拷贝(专有)； E（专有，Exclusive）：缓存行内容和内存中的一样，而且其它处理器都没有这行数据； S（共享，Shared）：缓存行内容和内存中的一样, 有可能其它处理器也存在此缓存行的拷贝； I（无效，Invalid）：缓存行失效, 不能使用 缓存行状态转换 在MESI协议中，每个Cache的Cache控制器不仅知道自己的读写操作，而且也监听(snoop)其它Cache的读写操作。每个Cache line所处的状态根据本核和其它核的读写操作在4个状态间进行迁移。MESI协议状态迁移图如下：
MESI协议-状态转换
初始：一开始时，缓存行没有加载任何数据，所以它处于 I 状态。 本地写（Local Write）：如果本地处理器写数据至处于 I 状态的缓存行，则缓存行的状态变成 M。 本地读（Local Read）：如果本地处理器读取处于 I 状态的缓存行，很明显此缓存没有数据给它。此时分两种情况：(1)其它处理器的缓存里也没有此行数据，则从内存加载数据到此缓存行后，再将它设成 E 状态，表示只有我一家有这条数据，其它处理器都没有；(2)其它处理器的缓存有此行数据，则将此缓存行的状态设为 S 状态。（备注：如果处于M状态的缓存行，再由本地处理器写入/读出，状态是不会改变的） 远程读（Remote Read）：假设我们有两个处理器 c1 和 c2，如果 c2 需要读另外一个处理器 c1 的缓存行内容，c1 需要把它缓存行的内容通过内存控制器 (Memory Controller) 发送给 c2，c2 接到后将相应的缓存行状态设为 S。在设置之前，内存也得从总线上得到这份数据并保存。 远程写（Remote Write）：其实确切地说不是远程写，而是 c2 得到 c1 的数据后，不是为了读，而是为了写。也算是本地写，只是 c1 也拥有这份数据的拷贝，这该怎么办呢？c2 将发出一个 RFO (Request For Owner) 请求，它需要拥有这行数据的权限，其它处理器的相应缓存行设为 I，除了它自已，谁不能动这行数据。这保证了数据的安全，同时处理 RFO 请求以及设置I的过程将给写操作带来很大的性能消耗。 缓存行 CPU缓存是以缓存行（cache line）为单位存储的。缓存行通常是 64 字节，并且它有效地引用主内存中的一块地址。一个 Java 的 long 类型是 8 字节，因此在一个缓存行中可以存 8 个 long 类型的变量。所以，如果你访问一个 long 数组，当数组中的一个值被加载到缓存中，它会额外加载另外 7 个，以致你能非常快地遍历这个数组。事实上，你可以非常快速的遍历在连续的内存块中分配的任意数据结构。而如果你在数据结构中的项在内存中不是彼此相邻的（如链表），你将得不到免费缓存加载所带来的优势，并且在这些数据结构中的每一个项都可能会出现缓存未命中。下图是一个CPU缓存行的示意图：
上图中，一个运行在处理器 core1上的线程想要更新变量 X 的值，同时另外一个运行在处理器 core2 上的线程想要更新变量 Y 的值。但是，这两个频繁改动的变量都处于同一条缓存行。两个线程就会轮番发送 RFO 消息，占得此缓存行的拥有权。当 core1 取得了拥有权开始更新 X，则 core2 对应的缓存行需要设为 I 状态。当 core2 取得了拥有权开始更新 Y，则 core1 对应的缓存行需要设为 I 状态(失效态)。轮番夺取拥有权不但带来大量的 RFO 消息，而且如果某个线程需要读此行数据时，L1 和 L2 缓存上都是失效数据，只有 L3 缓存上是同步好的数据。从前一篇我们知道，读 L3 的数据非常影响性能。更坏的情况是跨槽读取，L3 都要 miss，只能从内存上加载。
表面上 X 和 Y 都是被独立线程操作的，而且两操作之间也没有任何关系。只不过它们共享了一个缓存行，但所有竞争冲突都是来源于共享。
3、java中的伪共享 解决伪共享最直接的方法就是填充（padding），例如下面的VolatileLong，一个long占8个字节，Java的对象头占用8个字节（32位系统）或者12字节（64位系统，默认开启对象头压缩，不开启占16字节）。一个缓存行64字节，那么我们可以填充6个long（6 * 8 = 48 个字节）。这样就能避免多个VolatileLong共享缓存行。
public class VolatileLong { private volatile long v; // private long v0, v1, v2, v3, v4, v5 // 去掉注释，开启填充，避免缓存行共享 } 这是最简单直接的方法，Java 8中引入了一个更加简单的解决方案：@Contended注解：
@Retention(RetentionPolicy.RUNTIME) @Target({ElementType.FIELD, ElementType.TYPE}) public @interface Contended { String value() default ""; } Contended注解可以用于类型上和属性上，加上这个注解之后虚拟机会自动进行填充，从而避免伪共享。这个注解在Java8 ConcurrentHashMap、ForkJoinPool和Thread等类中都有应用。我们来看一下Java8中ConcurrentHashMap中如何运用Contended这个注解来解决伪共享问题。以下说的ConcurrentHashMap都是Java8版本。
ConcurrentHashMap中伪共享解决方案 ConcurrentHashMap的size操作通过CounterCell来计算，哈希表中的每个节点都对用了一个CounterCell，每个CounterCell记录了对应Node的键值对数目。这样每次计算size时累加各个CounterCell就可以了。ConcurrentHashMap中CounterCell以数组形式保存，而数组在内存中是连续存储的，CounterCell中只有一个long类型的value属性，这样CPU会缓存CounterCell临近的CounterCell，于是就形成了伪共享。
ConcurrentHashMap中用Contended注解自动对CounterCell来进行填充：
/** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; // CounterCell数组，CounterCell在内存中连续 public int size() { long n = sumCount(); return ((n &lt; 0L) ? 0 : (n > (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } // 计算size时直接对各个CounterCell的value进行累加 final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } // 使用Contended注解自动进行填充避免伪共享 @sun.misc.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } } 需要注意的是@sun.misc.Contended注解在user classpath中是不起作用的，需要通过一个虚拟机参数来开启：-XX:-RestrictContended
4、总结 CPU缓存是以缓存行为单位进行操作的。产生伪共享问题的根源在于不同的核同时操作同一个缓存行。 可以通过填充来解决伪共享问题，Java8 中引入了@sun.misc.Contended注解来自动填充。 并不是所有的场景都需要解决伪共享问题，因为CPU缓存是有限的，填充会牺牲掉一部分缓存</content></entry><entry><title>PowerShell设置使用vim</title><url>https://codingroam.github.io/post/powershell%E8%AE%BE%E7%BD%AE%E4%BD%BF%E7%94%A8vim/</url><categories><category>PowerShell</category></categories><tags><tag>PowerShell</tag><tag>Vim</tag><tag>Hands-on</tag></tags><content type="html"> PowerShell设置使用vim
PowerShell设置使用vim 如果系统安装了Git，Git本身有自带的vim，位置为%GITHOME%\usr\bin\vim.exe，如果没有安装Git,可以去github下载vim，Github vim下载
。
打开C:\Windows\System32\WindowsPowerShell\v1.0文件夹新建profile.ps1文件，内容如下，重点是$VIMPATH变量，值为第一步中vim.exe路径
# There's usually much more than this in my profile! $SCRIPTPATH = "C:\Program Files\Git\usr\share\vim" # 此行根据$VIMPATH寻找相应vim路径即可,此行不用更改 $VIMPATH = "D:\DevelopmentSoftware\Git\usr\bin\vim.exe" # 此行为1中vim.exe路径 Set-Alias vi $VIMPATH Set-Alias vim $VIMPATH # for editing your PowerShell profile Function Edit-Profile { vim $profile } # for editing your Vim settings Function Edit-Vimrc { vim $home\_vimrc } 以管理员身份运行PowerShell，运行Set-ExecutionPolicy RemoteSigned，然后输入Y即可
重新打开PowerShell即可使用vim命令</content></entry><entry><title>ThreadLocal与FastThreadLocal</title><url>https://codingroam.github.io/post/threadlocal%E4%B8%8Efastthreadlocal/</url><categories><category>Java</category></categories><tags><tag>ThreadLocal</tag><tag>Java Base</tag><tag>Learning</tag></tags><content type="html"> ThreadLocal与FastThreadLocal
ThreadLocal与FastThreadLocal FastThreadLocal是Netty中常用的一个工具类，他的基本功能与JDK自带的ThreadLocal
一样，但是性能优于ThreadLocal。在讲解FastThreadLocal之前，先大致讲一下ThreadLocal的原理。
1、ThreadLocal 如果想要在线程中保存一个变量，这个变量是该线程所独有的，其他线程不能对该变量进行访问和修改，那么我们可以使用ThreadLocal实现这一功能.
ThreadLocal的功能主要是通过ThreadLocalMap来实现的，每个线程都会有一个ThreadLocalMap类型的成员变量，ThreadLocalMap本质上就是一个哈希表，key为ThreadLocal，value为该ThreadLocal为该线程保存的变量，如下图所示： ThreadLocalMap中使用一个Entry类型的数组来作为哈希表，并用线性探测法解决哈希冲突，数组的大小永远是2的n次方。Entry是一个静态内部类，且继承了WeakReference&lt;ThreadLocal<?>>，表示其保存的ThreadLocal是一个弱引用，这是为了防止内存泄漏，因为当所有指向ThreadLocal的强引用都为null时，该Entry也就没有必要再指向这个ThreadLocal了，这样下一次gc时该ThreadLoca就会被回收掉。Entry的成员变量Object value就是T和read Local为线程保存的独有的变量。
set方法将ThreadLocal和相应的value保存为一个Entry，再根据ThreadLocal的哈希值将Entry放入哈希表中的相应的位置，并用线性探测法解决哈希冲突。setEntry方法中，如果在查找哈希表的过程中发现某个Entry所保存的ThreadLocal被垃圾回收了，那么会将该Entry清除掉
private ThreadLocal.ThreadLocalMap.Entry getEntry(ThreadLocal&lt;?> key) { int i = key.threadLocalHashCode &amp; (table.length - 1); ThreadLocal.ThreadLocalMap.Entry e = table[i]; //找到了相应的entry，直接返回 if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); } private ThreadLocal.ThreadLocalMap.Entry getEntryAfterMiss(ThreadLocal&lt;?> key, int i, ThreadLocal.ThreadLocalMap.Entry e) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal&lt;?> k = e.get(); if (k == key) return e; //该threadLocal已经为null，那么entry也就没有存在的必要了，因此需要被清除 if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null; } private void set(ThreadLocal&lt;?> key, Object value) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; //将threadlocal的哈希值与哈希表的长度取余，放入哈希表中相应的位置 int i = key.threadLocalHashCode &amp; (len-1); for (ThreadLocal.ThreadLocalMap.Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal&lt;?> k = e.get(); //哈希表中保存了这个threadLocal，因此直接修改value值 if (k == key) { e.value = value; return; } //entry不为null，但entry保存threadLocal为null，说明该threadLocal已经被垃圾回收了，需要替换掉这个entry if (k == null) { replaceStaleEntry(key, value, i); return; } } tab[i] = new ThreadLocal.ThreadLocalMap.Entry(key, value); int sz = ++size; //如果哈希表的装载因子超过阈值，则需要对哈希表扩容，并对所有的entry重新做哈希 if (!cleanSomeSlots(i, sz) &amp;&amp; sz >= threshold) rehash(); } 2、FastThreadLocal 从 InternalThreadLocalMap 内部实现来看，与 ThreadLocalMap 一样都是采用数组的存储方式，但是 InternalThreadLocalMap 并没有使用线性探测法来解决 Hash 冲突，而是在 FastThreadLocal 初始化的时候分配一个数组索引 index，index 的值采用原子类 AtomicInteger 保证顺序递增，通过调用 InternalThreadLocalMap.nextVariableIndex() 方法获得。然后在读写数据的时候通过数组下标 index 直接定位到 FastThreadLocal 的位置，时间复杂度为 O(1)。如果数组下标递增到非常大，那么数组也会比较大，所以 FastThreadLocal 是通过空间换时间的思想提升读写性能
public FastThreadLocal() { index = InternalThreadLocalMap.nextVariableIndex(); } public static int nextVariableIndex() { //nextIndex是一个静态变量，每次调用nextVariableIndex()都会自增1，让后赋给FastThreadLocal的index属性 int index = nextIndex.getAndIncrement(); if (index &lt; 0) { nextIndex.decrementAndGet(); throw new IllegalStateException("too many thread-local indexed variables"); } return index; } static final AtomicInteger nextIndex = new AtomicInteger(); 通过上面 FastThreadLocal 的内部结构图可知，FastThreadLocal 使用 Object 数组替代了 Entry 数组，Object[0] 存储的是一个Set&lt;FastThreadLocal<?>> 集合，从数组下标 1 开始都是直接存储的 value 数据，不再采用 ThreadLocal 的键值对形式进行存储。
假设现在我们有一批数据需要添加到数组中，分别为 value1、value2、value3、value4，对应的 FastThreadLocal 在初始化的时候生成的数组索引分别为 1、2、3、4。如下图所示。
使用代码示例 public class FastThreadLocalTest { private static final FastThreadLocal&lt;;String> THREAD_NAME_LOCAL = new FastThreadLocal&lt;;>(); private static final FastThreadLocal&lt;;TradeOrder> TRADE_THREAD_LOCAL = new FastThreadLocal&lt;;>(); public static void main(String[] args) { for (int i = 0; i &lt;; 2; i++) { int tradeId = i; String threadName = "thread-" + i; new FastThreadLocalThread(() -> { THREAD_NAME_LOCAL.set(threadName); TradeOrder tradeOrder = new TradeOrder(tradeId, tradeId % 2 == 0 ? "已支付" : "未支付"); TRADE_THREAD_LOCAL.set(tradeOrder); System.out.println("threadName: " + THREAD_NAME_LOCAL.get()); System.out.println("tradeOrder info：" + TRADE_THREAD_LOCAL.get()); }, threadName).start(); } } } FastThreadLocal 的使用方法几乎和 ThreadLocal 保持一致，只需要把代码中 Thread、ThreadLocal 替换为 FastThreadLocalThread 和 FastThreadLocal 即可。
FastThreadLocal源码分析 FastThreadLocal.set()
public final void set(V value) { if (value != InternalThreadLocalMap.UNSET) { // 1. value 是否为缺省值 InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.get(); // 2. 获取当前线程的 InternalThreadLocalMap setKnownNotUnset(threadLocalMap, value); // 3. 将 InternalThreadLocalMap 中数据替换为新的 value } else { remove(); } } set() 的过程主要分为三步：
判断 value 是否为缺省值(UNSET)，如果等于缺省值，那么直接调用 remove() 方法。这里我们还不知道缺省值和 remove() 之间的联系是什么，我们暂且把 remove() 放在最后分析。 如果 value 不等于缺省值，接下来会获取当前线程的 InternalThreadLocalMap。 然后将 InternalThreadLocalMap 中对应数据替换为新的 value。 FastThreadLocal使用字节填充解决伪共享 FastThreadLocal也是用在多线程场景，所以FastThreadLocal需要解决伪共享问题，FastThreadLocal使用字节填充解决伪共享，详情请移步java伪共享
缓存行 Cache是由很多个cache line组成的。每个cache line通常是64字节，并且它有效地引用主内存中的一块儿地址。一个Java的long类型变量是8字节，因此在一个缓存行中可以存8个long类型的变量。 CPU每次从主存中拉取数据时，会把相邻的数据也存入同一个cache line。 在访问一个long数组的时候，如果数组中的一个值被加载到缓存中，它会自动加载另外7个。因此你能非常快的遍历这个数组。事实上，你可以非常快速的遍历在连续内存块中分配的任意数据结构。
伪共享 由于多个线程同时操作同一缓存行的不同变量，但是这些变量之间却没有啥关联，但是每次修改，都会导致缓存的数据变成无效，从而明明没有任何修改的内容，还是需要去主存中读（CPU读取主存中的数据会比从L1中读取慢了近2个数量级）但是其实这块内容并没有任何变化，由于缓存的最小单位是一个缓存行，这就是伪共享。
如果让多线程频繁操作的并且没有关系的变量在不同的缓存行中，那么就不会因为缓存行的问题导致没有关系的变量的修改去影响另外没有修改的变量去读主存了（那么从L1中取是从主存取快2个数量级的）那么性能就会好很多很多。
FastThreadLocal 的一些思考 FastThreadLocal 真的一定比 ThreadLocal 快吗？答案是不一定的，只有使用FastThreadLocalThread 类型的线程才会更快，如果是普通线程反而会更慢。
FastThreadLocal 会浪费很大的空间吗？虽然 FastThreadLocal 采用的空间换时间的思路，但是在 FastThreadLocal 设计之初就认为不会存在特别多的 FastThreadLocal 对象，而且在数据中没有使用的元素只是存放了同一个缺省对象的引用，并不会占用太多内存空间。
FastThreadLocal vs ThreadLocal 高效查找。FastThreadLocal 在定位数据的时候可以直接根据数组下标 index 获取，时间复杂度 O(1)。而 JDK 原生的 ThreadLocal 在数据较多时哈希表很容易发生 Hash 冲突，线性探测法在解决 Hash 冲突时需要不停地向下寻找，效率较低。此外，FastThreadLocal 相比 ThreadLocal 数据扩容更加简单高效，FastThreadLocal 以 index 为基准向上取整到 2 的次幂作为扩容后容量，然后把原数据拷贝到新数组。而 ThreadLocal 由于采用的哈希表，所以在扩容后需要再做一轮 rehash。 安全性更高。JDK 原生的 ThreadLocal 使用不当可能造成内存泄漏，只能等待线程销毁。在使用线程池的场景下，ThreadLocal 只能通过主动检测的方式防止内存泄漏，从而造成了一定的开销。然而 FastThreadLocal 不仅提供了 remove() 主动清除对象的方法，而且在线程池场景中 Netty 还封装了 FastThreadLocalRunnable，FastThreadLocalRunnable 最后会执行 FastThreadLocal.removeAll() 将 Set 集合中所有 FastThreadLocal 对象都清理掉</content></entry><entry><title>ThreadLocal父子线程传递问题</title><url>https://codingroam.github.io/post/threadlocal%E7%88%B6%E5%AD%90%E7%BA%BF%E7%A8%8B%E4%BC%A0%E9%80%92%E9%97%AE%E9%A2%98/</url><categories><category>Java</category></categories><tags><tag>ThreadLocal</tag><tag>Java</tag><tag>Learning</tag></tags><content type="html"> ThreadLocal父子线程传递问题
ThreadLocal父子线程传递问题 1、父子线程传递问题 ThreadLocal是线程上下文，如果主线程开启子线程，还可以顺利获得本地变量吗？答案是否定的，以下是实验过程
public class TTLTest{ public static void main(String[] args) { ThreadLocal&lt;String> threadLocal = new ThreadLocal&lt;>(); InheritableThreadLocal&lt;String> inheritableThreadLocal = new InheritableThreadLocal&lt;>(); threadLocal.set("threadLocal-value"); inheritableThreadLocal.set("inheritableThreadLocal-value"); // 验证父子线程传递 new Thread(() -> { System.out.println(threadLocal.get()); // null System.out.println(inheritableThreadLocal.get()); //inheritableThreadLocal-value }).start(); threadLocal.remove(); inheritableThreadLocal.remove(); } } 可以看到，开启的子线程是无法获得父线程的本地变量的，所以java引入了InheritableThreadLocal,在子线程初始化时，会将父线程的本地变量传递到子线程
2、线程池环境的本地变量传递问题 在大多数实际项目中，为了节省线程开启关闭的开销，常常使用线程池来提高线程的复用，线程复用的同时，对本地变量的传递带来了新的影响，上文提到InheritableThreadLocal实现父子线程变量传递是在子线程初始化过程中，而池化的线程是不会重新初始化的，所以InheritableThreadLocal不能在线程池开辟的子线程中传递，或者说，只会在子线程初始化时传递一次，而后在线程池中未被销毁之前，无法再次接受父线程的变量传递。
public class TTLTest{ public static void main(String[] args) { InheritableThreadLocal&lt;String> inheritableThreadLocal = new InheritableThreadLocal&lt;>(); // 验证线程池传递 ExecutorService executorService = Executors.newFixedThreadPool(1); for (int i = 0; i &lt; 10; i++) { inheritableThreadLocal.set("inheritableThreadLocal-value->" + i); executorService.submit(() -> { System.out.println(inheritableThreadLocal.get()); }); } executorService.shutdown(); inheritableThreadLocal.remove(); } } 结果： inheritableThreadLocal-value->0 inheritableThreadLocal-value->0 inheritableThreadLocal-value->0 inheritableThreadLocal-value->0 ... 可以看出子线程只保留了最初获得的本地变量 解决方案是阿里巴巴开源框架TTL: transmittable-thread-local
&lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>transmittable-thread-local&lt;/artifactId> &lt;/dependency> public class TTLTest { public static void main(String[] args) { InheritableThreadLocal&lt;String> inheritableThreadLocal = new InheritableThreadLocal&lt;>(); TransmittableThreadLocal&lt;String> transmittableThreadLocal = new TransmittableThreadLocal&lt;>(); // 验证线程池传递2 ExecutorService executorService2 = TtlExecutors.getTtlExecutorService(Executors.newFixedThreadPool(1)); for (int i = 0; i &lt; 10; i++) { inheritableThreadLocal.set("inheritableThreadLocal-value->" + i); transmittableThreadLocal.set("transmittableThreadLocal-value->" + i); executorService2.submit(() -> { System.out.println(inheritableThreadLocal.get()); System.out.println(transmittableThreadLocal.get()); }); } executorService.shutdown(); inheritableThreadLocal.remove(); transmittableThreadLocal.remove(); } } 结果： inheritableThreadLocal-value->0 transmittableThreadLocal-value->1 inheritableThreadLocal-value->0 transmittableThreadLocal-value->2 inheritableThreadLocal-value->0 transmittableThreadLocal-value->3 inheritableThreadLocal-value->0 transmittableThreadLocal-value->4 ... 注意这句代码TtlExecutors.getTtlExecutorService 通过对线程池进行包装，从而实现TTL所具备的功能，使用阿里开源的TransmittableThreadLocal 优雅的实现父子线程的数据传递。</content></entry><entry><title>ThreadLocal详解</title><url>https://codingroam.github.io/post/threadlocal/</url><categories><category>Java</category></categories><tags><tag>ThreadLocal</tag><tag>Java Base</tag><tag>Java</tag><tag>Learning</tag></tags><content type="html"> ThreadLocal详解
ThreadLocal详解 1 ThreadLocal有什么缺陷？为什么会导致内存泄漏？ 2 Entry的key为什么要用弱引用? 四种引用有什么区别说一下？为什么使用弱引用就可以解决内存泄漏问题? 3 ThreadLocalMap的Key是什么？ 4 ThreadLocalMap如何解决value冲突问题?跟HashMap有什么区别？ 5 进阶：ThreadLocal能否做到子线程中共享父线程中的数据?InheritableThreadLocal了解过吗？有什么局限性？ 6 进阶：FastThreadLocal有了解过吗？ 1、ThreadLocal简介 该类提供线程局部变量。这些变量与普通变量的不同之处在于，每个访问一个变量(通过其get或set方法)的线程都有自己的、独立初始化的变量副本。ThreadLocal实例通常是类中的私有静态字段，希望将状态与线程关联(例如，用户ID或事务ID)。
​ ThreadLocal叫做线程变量，意思是ThreadLocal中填充的变量属于当前线程，该变量对其他线程而言是隔离的，也就是说该变量是当前线程独有的变量。ThreadLocal为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量。
通俗的描述一下threadlocal的存取set()/get()方法大体逻辑如下：
set方法：threadlocal在set数据时，首先调用Thread.currentThread()获取当前操作线程，拿到线程之后获取线程的ThreadLocalMap变量(细节还包括首次获取时为null，为当前线程初始化ThreadLocalMap并返回)，然后向ThreadLocalMap中put数据，key是this(threadlocal)，value即为当前线程要保存的值。
get方法：逻辑与set类似，先获取当前线程，拿到线程的ThreadLocalMap，调用ThreadLocalMap的get方法，key即为this(当前threadlocal实例对象)，返回当前线程保存的值。
总的来说，ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景
图示：
2、ThreadLocal简单使用 public class ThreadLocaDemo { private static ThreadLocal&lt;String> localVar = new ThreadLocal&lt;String>(); static void print(String str) { //打印当前线程中本地内存中本地变量的值 System.out.println(str + " :" + localVar.get()); //清除本地内存中的本地变量 localVar.remove(); } public static void main(String[] args) throws InterruptedException { new Thread(new Runnable() { public void run() { ThreadLocaDemo.localVar.set("local_A"); print("A"); //打印本地变量 System.out.println("after remove : " + localVar.get()); } },"A").start(); Thread.sleep(1000); new Thread(new Runnable() { public void run() { ThreadLocaDemo.localVar.set("local_B"); print("B"); System.out.println("after remove : " + localVar.get()); } },"B").start(); } } A :local_A after remove : null B :local_B after remove : null 3、ThreadLocal源码原理 ThreadLocal的set()方法： public void set(T value) { //1、获取当前线程 Thread t = Thread.currentThread(); //2、获取线程中的属性 threadLocalMap ,如果threadLocalMap 不为空， //则直接更新要保存的变量值，否则创建threadLocalMap，并赋值 ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else // 初始化thradLocalMap 并赋值 createMap(t, value); } 从上面的代码可以看出，ThreadLocal set赋值的时候首先会获取当前线程thread,并获取thread线程中的ThreadLocalMap属性。如果map属性不为空，则直接更新value值，如果map为空，则实例化threadLocalMap,并将value值初始化。
那么ThreadLocalMap又是什么呢，还有createMap又是怎么做的
static class ThreadLocalMap { /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as "stale entries" in the code that follows. */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?>> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?> k, Object v) { super(k); value = v; } } } 可看出ThreadLocalMap是ThreadLocal的内部静态类，而它的构成主要是用Entry来保存数据 ，而且还是继承的弱引用。在Entry内部使用ThreadLocal作为key，使用我们设置的value作为value。
//这个是threadlocal 的内部方法 void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } //ThreadLocalMap 构造方法 ThreadLocalMap(ThreadLocal&lt;?> firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } ThreadLocal的get方法 public T get() { //1、获取当前线程 Thread t = Thread.currentThread(); //2、获取当前线程的ThreadLocalMap ThreadLocalMap map = getMap(t); //3、如果map数据不为空， if (map != null) { //3.1、获取threalLocalMap中存储的值 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings("unchecked") T result = (T)e.value; return result; } } //如果是数据为null，则初始化，初始化的结果，TheralLocalMap中存放key值为threadLocal，值为null return setInitialValue(); } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } ThreadLocal的remove方法
public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); } remove方法，直接将ThrealLocal 对应的值从当前相差Thread中的ThreadLocalMap中删除。为什么要删除，这涉及到内存泄露的问题。
实际上 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，弱引用的特点是，如果这个对象只存在弱引用，那么在下一次垃圾回收的时候必然会被清理掉。
所以如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会被清理掉的，这样一来 ThreadLocalMap中使用这个 ThreadLocal 的 key 也会被清理掉。但是，value 是强引用，不会被清理，这样一来就会出现 key 为 null 的 value。
ThreadLocal其实是与线程绑定的一个变量，如此就会出现一个问题：如果没有将ThreadLocal内的变量删除（remove）或替换，它的生命周期将会与线程共存。通常线程池中对线程管理都是采用线程复用的方法，在线程池中线程很难结束甚至于永远不会结束，这将意味着线程持续的时间将不可预测，甚至与JVM的生命周期一致。举个例字，如果ThreadLocal中直接或间接包装了集合类或复杂对象，每次在同一个ThreadLocal中取出对象后，再对内容做操作，那么内部的集合类和复杂对象所占用的空间可能会开始持续膨胀。
ThreadLocal与Thread，ThreadLocalMap之间的关系
4、ThreadLocal 常见使用场景 ThreadLocal 适用于如下两种场景：
每个线程需要有自己单独的实例
实例需要在多个方法中共享，但不希望被多线程共享
对于第一点，每个线程拥有自己实例，实现它的方式很多。例如可以在线程内部构建一个单独的实例。ThreadLoca 可以以非常方便的形式满足该需求。
对于第二点，可以在满足第一点（每个线程有自己的实例）的条件下，通过方法间引用传递的形式实现。ThreadLocal 使得代码耦合度更低，且实现更优雅。
场景
1）存储用户Session
一个简单的用ThreadLocal来存储Session的例子：
private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException { Session s = (Session) threadSession.get(); try { if (s == null) { s = getSessionFactory().openSession(); threadSession.set(s); } } catch (HibernateException ex) { throw new InfrastructureException(ex); } return s; } 场景二、数据库连接，处理数据库事务
场景三、数据跨层传递（controller,service, dao）
每个线程内需要保存类似于全局变量的信息（例如在拦截器中获取的用户信息），可以让不同方法直接使用，避免参数传递的麻烦却不想被多线程共享（因为不同线程获取到的用户信息不一样）。
例如，用 ThreadLocal 保存一些业务内容（用户权限信息、从用户系统获取到的用户名、用户ID 等），这些信息在同一个线程内相同，但是不同的线程使用的业务内容是不相同的。
在线程生命周期内，都通过这个静态 ThreadLocal 实例的 get() 方法取得自己 set 过的那个对象，避免了将这个对象（如 user 对象）作为参数传递的麻烦。
比如说我们是一个用户系统，那么当一个请求进来的时候，一个线程会负责执行这个请求，然后这个请求就会依次调用service-1()、service-2()、service-3()、service-4()，这4个方法可能是分布在不同的类中的。这个例子和存储session有些像。
package com.kong.threadlocal; public class ThreadLocalDemo05 { public static void main(String[] args) { User user = new User("jack"); new Service1().service1(user); } } class Service1 { public void service1(User user){ //给ThreadLocal赋值，后续的服务直接通过ThreadLocal获取就行了。 UserContextHolder.holder.set(user); new Service2().service2(); } } class Service2 { public void service2(){ User user = UserContextHolder.holder.get(); System.out.println("service2拿到的用户:"+user.name); new Service3().service3(); } } class Service3 { public void service3(){ User user = UserContextHolder.holder.get(); System.out.println("service3拿到的用户:"+user.name); //在整个流程执行完毕后，一定要执行remove UserContextHolder.holder.remove(); } } class UserContextHolder { //创建ThreadLocal保存User对象 public static ThreadLocal&lt;User> holder = new ThreadLocal&lt;>(); } class User { String name; public User(String name){ this.name = name; } } 执行的结果： service2拿到的用户:jack service3拿到的用户:jack 场景四、Spring使用ThreadLocal解决线程安全问题
我们知道在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全的“状态性对象”采用ThreadLocal进行封装，让它们也成为线程安全的“状态性对象”，因此有状态的Bean就能够以singleton的方式在多线程中正常工作了。
一般的Web应用划分为展现层、服务层和持久层三个层次，在不同的层中编写对应的逻辑，下层通过接口向上层开放功能调用。在一般情况下，从接收请求到返回响应所经过的所有程序调用都同属于一个线程，如图9-2所示。 这样用户就可以根据需要，将一些非线程安全的变量以ThreadLocal存放，在同一次请求响应的调用线程中，所有对象所访问的同一ThreadLocal变量都是当前线程所绑定的。
5、ThreadLocal内存泄漏 ThreadLocalMap实际上是以Entry作为数据存储载体，Entry将ThreadLocal作为Key，值作为value保存，它继承自WeakReference，注意构造函数里的第一行代码super(k)，这意味着它的Key(ThreadLocal对象)是一个「弱引用」
static class Entry extends WeakReference&lt;ThreadLocal&lt;?>> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?> k, Object v) { super(k); value = v; } } 主要两个原因 1 . 没有手动删除这个 Entry 2 . CurrentThread 当前线程依然运行
第一点很好理解，只要在使用完下 ThreadLocal ，调用其 remove 方法删除对应的 Entry(找到对应的entry并将entry=null) ，就能避免内存泄漏。 第二点稍微复杂一点，由于ThreadLocalMap 是 Thread 的一个属性，被当前线程所引用，所以ThreadLocalMap的生命周期跟 Thread 一样长。如果threadlocal变量被回收，那么当前线程的threadlocal 变量副本指向的就是key=null, 也即entry(null,value),那这个entry对应的value永远无法访问到。实际私用ThreadLocal场景都是采用线程池，而线程池中的线程都是复用的，这样就可能导致非常多的entry(null,value)出现，从而导致内存泄露。 综上， ThreadLocal 内存泄漏的根源是： 由于ThreadLocalMap 的生命周期跟 Thread 一样长，对于重复利用的线程来说，如果没有手动删除（remove()方法）对应 key 就会导致entry(null，value)的对象越来越多，从而导致内存泄漏。
**key为弱引用的好处**：**ThreadLocal 没有被外部强引用的情况下，key就会被回收变成null，在 ThreadLocalMap 中的set/getEntry 方法中，会对 key 为 null（也即是 ThreadLocal 为 null ）进行判断，如果为 null 的话，那么会把 value 置为 null 的．这就意味着使用threadLocal , CurrentThread 依然运行的前提下．就算忘记调用 remove 方法，弱引用比强引用可以多一层保障：弱引用的 ThreadLocal 会被回收．对应value在下一次 ThreadLocaI 调用 get()/set()/remove() 中的任一方法的时候会被清除，从而避免内存泄漏．**
6、ThreadLocalMap 和 HashMap 区别 ThreadLocalMap 和 HashMap 最大的区别在于解决 hash 冲突。
HashMap 使用的链地址法，而 ThreadLocalMap 使用的线性探测法/开放地址法。
当我们通过 int i = key.threadLocalHashCode &amp; (len-1) 计算出 hash 值，如果出现冲突，顺序查看表中下一单元，直到找出一个空单元或查遍全表。</content></entry><entry><title>GitHub提交设置代理</title><url>https://codingroam.github.io/post/github%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/</url><categories><category>Git</category></categories><tags><tag>Git</tag><tag>Problem-Solving</tag></tags><content type="html"> GitHub提交443错误可以设置代理（科学上网）解决
GitHub提交443错误设置代理 https方式提交可以通过设置代理解决，但是更好的办法是设置SSH方式来提交代码，详情请查看笔记《Git设置SSH方式》
错误描述 OpenSSL SSL_connect: Connection was reset in connection to github.com:443 看错误描述就标识ssl连接不到443端口。 可以设置代理解决。先检查git的全局配置
查看全局配置 git config --global -l 检查是否有https.proxy及http.proxy项
设置全局代理设置 示例10808端口是代理软件端口，按个人情况修改。
git config --global http.proxy 127.0.0.1:10808 git config --global https.proxy 127.0.0.1:10808 git config --global http.proxy 127.0.0.1:10818 git config --global https.proxy 127.0.0.1:10818 已有设置情况修改代理项 git config --global --unset http.proxy git config --global --unset https.proxy 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</content></entry><entry><title>B树与B+树</title><url>https://codingroam.github.io/post/b%E6%A0%91%E4%B8%8Eb+%E6%A0%91/</url><categories><category>MySQL</category></categories><tags><tag>数据结构</tag><tag>MySQL</tag><tag>Learning</tag></tags><content type="html"> B树、B+树 mysql存储引擎
一，b树 b树（balance tree）和b+树应用在数据库索引，可以认为是m叉的多路平衡查找树，但是从理论上讲，二叉树查找速度和比较次数都是最小的，为什么不用二叉树呢？
​ 平衡二叉树的查找效率是非常高的，并可以通过降低树的深度来提高查找的效率。但是当数据量非常大，树的存储的元素数量是有限的，这样会导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下。另外数据量过大会导致内存空间不够容纳平衡二叉树所有结点的情况。B树是解决这个问题的很好的结构。数据库索引是存储在磁盘上的，当数据量大时，就不能把整个索引全部加载到内存了，只能逐一加载每一个磁盘页（对应索引树的节点）。所以我们要减少IO次数，对于树来说，IO次数就是树的高度，而“矮胖”就是b树的特征之一，它的每个节点最多包含m个孩子，m称为b树的阶，m的大小取决于磁盘页的大小。
​ b树在查询时的比较次数并不比二叉树少，尤其是节点中的数非常多时，但是内存的比较速度非常快，耗时可以忽略，所以只要树的高度低，IO少，就可以提高查询性能，这是b树的优势之一。
二，b+树 b+树，是b树的一种变体，查询性能更好。m阶的b+树的特征：
有n棵子树的非叶子结点中含有n个关键字（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据）。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非叶子结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 通常在b+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。 同一个数字会在不同节点中重复出现，根节点的最大元素就是b+树的最大元素。
三、b+树相比于b树的查询优势： b+树的中间节点不保存数据，所以磁盘页能容纳更多节点元素，更“矮胖”； b+树查询必须查找到叶子节点，b树只要匹配到即可不用管元素位置，因此b+树查找更稳定（并不慢）； 对于范围查找来说，b+树只需遍历叶子节点链表即可，b树却需要重复地中序遍历。 四、图示 B+树 InnoDB
MyISAM</content></entry><entry><title>Git设置SSH方式</title><url>https://codingroam.github.io/post/git%E8%AE%BE%E7%BD%AEssh%E6%96%B9%E5%BC%8F/</url><categories><category>Git</category></categories><tags><tag>Git</tag><tag>Linux</tag><tag>Hands-on</tag><tag>SSH</tag></tags><content type="html"> Linux环境Git上传出现：The requested URL returned error: 403解决办法小记
Git设置SSH方式 一、Gitee场景 ​ 阿里云环境push代码失败，代码托管为Gitee。切换https方式到ssh后提交成功，方式如下：
1、在linux控制台(powershell也是如此)生成sshkey $ ssh-keygen -t rsa -C &ldquo;xxxxx@xxxxx.com
&rdquo; 按照提示完成三次回车，即可生成 ssh key cat ~/.ssh/id_rsa.pub 2、将ssh key添加到gitee仓库 通过仓库主页 「管理」->「部署公钥管理」 点击黄色方框中添加个人公钥超链接 3、在linux终端输入ssh -T git@gitee.com
首次使用需要确认并添加主机到本机SSH可信列表。若返回 Hi XXX! You've successfully authenticated, but Gitee.com does not provide shell access. 内容，则证明添加成功。
4、修改原先https的下载方式 git config &ndash;list 查看配置得到 remote.origin.url=https://gitee.com/wk_acme/git_study.git 修改 git config remote.origin.url git@gitee.com
:wk_acme/git_study.git 5、push代码成功 二、Github场景 1、首先需要检查你电脑是否已经有 SSH key 运行 git Bash 客户端，输入如下代码：
$ cd ~/.ssh $ ls 这两个命令就是检查是否已经存在 id_rsa.pub 或 id_dsa.pub 文件，如果文件已经存在，那么你可以跳过步骤2，直接进入步骤3。
2、创建一个 SSH key $ ssh-keygen -t rsa -C "your_email@example.com" 代码参数含义：
-t 指定密钥类型，默认是 rsa ，可以省略。 -C 设置注释文字，比如邮箱。 -f 指定密钥文件存储文件名。
以上代码省略了 -f 参数，因此，运行上面那条命令后会让你输入一个文件名，用于保存刚才生成的 SSH key 代码，如：
Generating public/private rsa key pair. # Enter file in which to save the key (/c/Users/you/.ssh/id_rsa): [Press enter] 当然，你也可以不输入文件名，使用默认文件名（推荐），那么就会生成 id_rsa 和 id_rsa.pub 两个秘钥文件。
接着又会提示你输入两次密码（该密码是你push文件的时候要输入的密码，而不是github管理者的密码），
当然，你也可以不输入密码，直接按回车。那么push的时候就不需要输入密码，直接提交到github上了，如：
Enter passphrase (empty for no passphrase): # Enter same passphrase again: 接下来，就会显示如下代码提示，如：
Your identification has been saved in /c/Users/you/.ssh/id_rsa. # Your public key has been saved in /c/Users/you/.ssh/id_rsa.pub. # The key fingerprint is: # 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com 当你看到上面这段代码的收，那就说明，你的 SSH key 已经创建成功，你只需要添加到github的SSH key上就可以了。
3、添加你的 SSH key 到 github上面去 首先你需要拷贝 id_rsa.pub 文件的内容，你可以用编辑器打开文件复制，也可以用git命令复制该文件的内容，如：
$ clip &lt; ~/.ssh/id_rsa.pub Window 使用 clip 命令复制，Mac 则使用 pbcopy 命令
登录你的github账号，从又上角的设置（ Account Settings
）进入，然后点击菜单栏的 SSH key 进入页面添加 SSH key。
点击 Add SSH key 按钮添加一个 SSH key 。把你复制的 SSH key 代码粘贴到 key 所对应的输入框中，记得 SSH key 代码的前后不要留有空格或者回车。当然，上面的 Title 所对应的输入框你也可以输入一个该 SSH key 显示在 github 上的一个别名。默认的会使用你的邮件名称。
4、测试一下该SSH key 在git Bash 中输入以下代码
$ ssh -T git@github.com 当你输入以上代码时，会有一段警告代码，如：
The authenticity of host 'github.com (207.97.227.239)' can't be established. # RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48. # Are you sure you want to continue connecting (yes/no)? 这是正常的，你输入 yes 回车既可。如果你创建 SSH key 的时候设置了密码，接下来就会提示你输入密码，如：
Enter passphrase for key '/c/Users/Administrator/.ssh/id_rsa': 当然如果你密码输错了，会再要求你输入，知道对了为止。
注意：输入密码时如果输错一个字就会不正确，使用删除键是无法更正的。
密码正确后你会看到下面这段话，如：
Hi username! You've successfully authenticated, but GitHub does not # provide shell access. 成功！</content></entry><entry><title>Spring基础汇总</title><url>https://codingroam.github.io/post/spring%E5%9F%BA%E7%A1%80%E6%B1%87%E6%80%BB/</url><categories><category>Spring</category></categories><tags><tag>Spring</tag><tag>java</tag><tag>Learning</tag></tags><content type="html"> Spring基础汇总
Spring基础汇总 1.简介 1.1.简介 简介
Spring : 春天 —>给软件行业带来了春天
2002年，Rod Jahnson首次推出了Spring框架
雏形interface21框架。
2004年3月24日，Spring框架以interface21框架为基础，经过重新设计，发布了1.0正式版。
很难想象Rod Johnson的学历 , 他是悉尼大学的博士，然而他的专业不是计算机，而是音乐学。
Spring理念 : 使现有技术更加实用 . 本身就是一个大杂烩 , 整合现有的框架技术
官网 : http://spring.io/
官方下载地址 : https://repo.spring.io/libs-release-local/org/springframework/spring/
GitHub : https://github.com/spring-projects
&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-webmvc --> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>5.2.0.RELEASE&lt;/version> &lt;/dependency> &lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-jdbc --> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-jdbc&lt;/artifactId> &lt;version>5.2.0.RELEASE&lt;/version> &lt;/dependency> 1.2.优点 优点
1、Spring是一个开源免费的框架 , 容器 .
2、Spring是一个轻量级的框架 , 非侵入式的 .
3、控制反转 IoC , 面向切面 Aop
4、对事物的支持 , 对框架的支持
…
一句话概括：
Spring是一个轻量级的控制反转(IoC)和面向切面(AOP)的容器（框架）。
1.3.组成 组成
Spring 框架是一个分层架构，由 7 个定义良好的模块组成。Spring 模块构建在核心容器之上，核心容器定义了创建、配置和管理 bean 的方式 .
组成 Spring 框架的每个模块（或组件）都可以单独存在，或者与其他一个或多个模块联合实现。每个模块的功能如下：
核心容器：核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转（IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。 Spring 上下文：Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、校验和调度功能。 Spring AOP：通过配置管理特性，Spring AOP 模块直接将面向切面的编程功能 , 集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理任何支持 AOP的对象。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖组件，就可以将声明性事务管理集成到应用程序中。 Spring DAO：JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构。 Spring ORM：Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。 Spring Web 模块：Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Struts 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。 Spring MVC 框架：MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。 1.4.扩展 拓展
Spring Boot与Spring Cloud
Spring Boot 是 Spring 的一套快速配置脚手架，可以基于Spring Boot 快速开发单个微服务; Spring Cloud是基于Spring Boot实现的； Spring Boot专注于快速、方便集成的单个微服务个体，Spring Cloud关注全局的服务治理框架； Spring Boot使用了约束优于配置的理念，很多集成方案已经帮你选择好了，能不配置就不配置 , Spring Cloud很大的一部分是基于Spring Boot来实现，Spring Boot可以离开Spring Cloud独立使用开发项目，但是Spring Cloud离不开Spring Boot，属于依赖的关系。 SpringBoot在SpringClound中起到了承上启下的作用，如果你要学习SpringCloud必须要学习SpringBoot。 2.IOC理论推导 IoC基础
新建一个空白的maven项目
2.1.分析实现 分析实现
我们先用我们原来的方式写一段代码 .
1、先写一个UserDao接口
public interface UserDao { public void getUser(); } 123 2、再去写Dao的实现类
public class UserDaoImpl implements UserDao { @Override public void getUser() { System.out.println("获取用户数据"); } } 3、然后去写UserService的接口
public interface UserService { public void getUser(); } 4、最后写Service的实现类
public class UserServiceImpl implements UserService { private UserDao userDao = new UserDaoImpl(); @Override public void getUser() { userDao.getUser(); } } 5、测试一下
@Test public void test(){ UserService service = new UserServiceImpl(); service.getUser(); } 这是我们原来的方式 , 开始大家也都是这么去写的对吧 . 那我们现在修改一下 .
把Userdao的实现类增加一个 .
public class UserDaoMySqlImpl implements UserDao { @Override public void getUser() { System.out.println("MySql获取用户数据"); } } 123456 紧接着我们要去使用MySql的话 , 我们就需要去service实现类里面修改对应的实现
public class UserServiceImpl implements UserService { private UserDao userDao = new UserDaoMySqlImpl(); @Override public void getUser() { userDao.getUser(); } } 在假设, 我们再增加一个Userdao的实现类 .
public class UserDaoOracleImpl implements UserDao { @Override public void getUser() { System.out.println("Oracle获取用户数据"); } } 那么我们要使用Oracle , 又需要去service实现类里面修改对应的实现 . 假设我们的这种需求非常大 , 这种方式就根本不适用了, 甚至反人类对吧 , 每次变动 , 都需要修改大量代码 . 这种设计的耦合性太高了, 牵一发而动全身 .
那我们如何去解决呢 ?
我们可以在需要用到他的地方 , 不去实现它 , 而是留出一个接口 , 利用set , 我们去代码里修改下 .
public class UserServiceImpl implements UserService { private UserDao userDao; // 利用set实现 public void setUserDao(UserDao userDao) { this.userDao = userDao; } @Override public void getUser() { userDao.getUser(); } } 现在去我们的测试类里 , 进行测试 ;
@Test public void test(){ UserServiceImpl service = new UserServiceImpl(); service.setUserDao( new UserDaoMySqlImpl() ); service.getUser(); //那我们现在又想用Oracle去实现呢 service.setUserDao( new UserDaoOracleImpl() ); service.getUser(); } 大家发现了区别没有 ? 可能很多人说没啥区别 . 但是同学们 , 他们已经发生了根本性的变化 , 很多地方都不一样了 . 仔细去思考一下 , 以前所有东西都是由程序去进行控制创建 , 而现在是由我们自行控制创建对象 , 把主动权交给了调用者 . 程序不用去管怎么创建,怎么实现了 . 它只负责提供一个接口 .
这种思想 , 从本质上解决了问题 , 我们程序员不再去管理对象的创建了 , 更多的去关注业务的实现 . 耦合性大大降低 . 这也就是IOC的原型 !
2.2.IOC本质 IOC本质
控制反转IoC(Inversion of Control)，是一种设计思想，DI(依赖注入)是实现IoC的一种方法，也有人认为DI只是IoC的另一种说法。没有IoC的程序中 , 我们使用面向对象编程 , 对象的创建与对象间的依赖关系完全硬编码在程序中，对象的创建由程序自己控制，控制反转后将对象的创建转移给第三方，个人认为所谓控制反转就是：获得依赖对象的方式反转了。
IoC是Spring框架的核心内容，使用多种方式完美的实现了IoC，可以使用XML配置，也可以使用注解，新版本的Spring也可以零配置实现IoC。
Spring容器在初始化时先读取配置文件，根据配置文件或元数据创建与组织对象存入容器中，程序使用时再从Ioc容器中取出需要的对象。
采用XML方式配置Bean的时候，Bean的定义信息是和实现分离的，而采用注解的方式可以把两者合为一体，Bean的定义信息直接以注解的形式定义在实现类中，从而达到了零配置的目的。
控制反转是一种通过描述（XML或注解）并通过第三方去生产或获取特定对象的方式。在Spring中实现控制反转的是IoC容器，其实现方法是依赖注入（Dependency Injection,DI）。
3.HelloSpring 导入Jar包
注 : spring 需要导入commons-logging进行日志记录 . 我们利用maven , 他会自动下载对应的依赖项 .
&lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>5.1.10.RELEASE&lt;/version> &lt;/dependency> 编写代码
1、编写一个Hello实体类
public class Hello { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } public void show(){ System.out.println("Hello,"+ name ); } } 2、编写我们的spring文件 , 这里我们命名为beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;!--bean就是java对象 , 由Spring创建和管理--> &lt;bean id="hello" class="com.kuang.pojo.Hello"> &lt;property name="name" value="Spring"/> &lt;/bean> &lt;/beans> 3、我们可以去进行测试了 .
@Test public void test(){ //解析beans.xml文件 , 生成管理相应的Bean对象 ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); //getBean : 参数即为spring配置文件中bean的id . Hello hello = (Hello) context.getBean("hello"); hello.show(); } 思考
Hello 对象是谁创建的 ? hello 对象是由Spring创建的 Hello 对象的属性是怎么设置的 ? hello 对象的属性是由Spring容器设置的 这个过程就叫控制反转 :
控制 : 谁来控制对象的创建 , 传统应用程序的对象是由程序本身控制创建的 , 使用Spring后 , 对象是由Spring来创建的 反转 : 程序本身不创建对象 , 而变成被动的接收对象 . 依赖注入 : 就是利用set方法来进行注入的.
IOC是一种编程思想，由主动的编程变成被动的接收
可以通过newClassPathXmlApplicationContext去浏览一下底层源码 .
修改案例一
我们在案例一中， 新增一个Spring配置文件beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;bean id="MysqlImpl" class="com.kuang.dao.impl.UserDaoMySqlImpl"/> &lt;bean id="OracleImpl" class="com.kuang.dao.impl.UserDaoOracleImpl"/> &lt;bean id="ServiceImpl" class="com.kuang.service.impl.UserServiceImpl"> &lt;!--注意: 这里的name并不是属性 , 而是set方法后面的那部分 , 首字母小写--> &lt;!--引用另外一个bean , 不是用value 而是用 ref--> &lt;property name="userDao" ref="OracleImpl"/> &lt;/bean> &lt;/beans> 测试！
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserServiceImpl serviceImpl = (UserServiceImpl) context.getBean("ServiceImpl"); serviceImpl.getUser(); } 123456 OK , 到了现在 , 我们彻底不用再程序中去改动了 , 要实现不同的操作 , 只需要在xml配置文件中进行修改 , 所谓的IoC,一句话搞定 : 对象由Spring 来创建 , 管理 , 装配 !
4.IOC创建对象方式 4.1.通过无参构造方法来创建 通过无参构造方法来创建
1、User.java
public class User { private String name; public User() { System.out.println("user无参构造方法"); } public void setName(String name) { this.name = name; } public void show(){ System.out.println("name="+ name ); } } 1234567891011121314151617 2、beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;bean id="user" class="com.kuang.pojo.User"> &lt;property name="name" value="kuangshen"/> &lt;/bean> &lt;/beans> 3、测试类
@Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); //在执行getBean的时候, user已经创建好了 , 通过无参构造 User user = (User) context.getBean("user"); //调用对象的方法 . user.show(); } 结果可以发现，在调用show方法之前，User对象已经通过无参构造初始化了！
4.2.通过有参构造方法来创建 通过有参构造方法来创建
1、UserT . java
public class UserT { private String name; public UserT(String name) { this.name = name; } public void setName(String name) { this.name = name; } public void show(){ System.out.println("name="+ name ); } } 2、beans.xml 有三种方式编写
&lt;!-- 第一种根据index参数下标设置 --> &lt;bean id="userT" class="com.kuang.pojo.UserT"> &lt;!-- index指构造方法 , 下标从0开始 --> &lt;constructor-arg index="0" value="kuangshen2"/> &lt;/bean> &lt;!-- 第二种根据参数名字设置 --> &lt;bean id="userT" class="com.kuang.pojo.UserT"> &lt;!-- name指参数名 --> &lt;constructor-arg name="name" value="kuangshen2"/> &lt;/bean> &lt;!-- 第三种根据参数类型设置 --> &lt;bean id="userT" class="com.kuang.pojo.UserT"> &lt;constructor-arg type="java.lang.String" value="kuangshen2"/> &lt;/bean> 3、测试
@Test public void testT(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserT user = (UserT) context.getBean("userT"); user.show(); } 123456 结论：在配置文件加载的时候。其中管理的对象都已经初始化了！
5.Spring配置 5.1.别名 别名
alias 设置别名 , 为bean设置别名 , 可以设置多个别名
&lt;!--设置别名：在获取Bean的时候可以使用别名获取--> &lt;alias name="userT" alias="userNew"/> 5.2.Bean的配置 Bean的配置
&lt;!--bean就是java对象,由Spring创建和管理--> &lt;!-- id 是bean的标识符,要唯一,如果没有配置id,name就是默认标识符 如果配置id,又配置了name,那么name是别名 name可以设置多个别名,可以用逗号,分号,空格隔开 如果不配置id和name,可以根据applicationContext.getBean(.class)获取对象; class是bean的全限定名=包名+类名 --> &lt;bean id="hello" name="hello2 h2,h3;h4" class="com.kuang.pojo.Hello"> &lt;property name="name" value="Spring"/> &lt;/bean> 5.3.import import
团队的合作通过import来实现 .
&lt;import resource="{path}/beans.xml"/> 1 6.依赖注入（DI） Dependency Injection
概念
依赖注入（Dependency Injection,DI）。 依赖 : 指Bean对象的创建依赖于容器 . Bean对象的依赖资源 . 注入 : 指Bean对象所依赖的资源 , 由容器来设置和装配 . 6.1.构造器注入 构造器注入
我们在之前的案例已经讲过了
6.2.Set 注入 （重点） Set 注入 （重点）
要求被注入的属性 , 必须有set方法 , set方法的方法名由set + 属性首字母大写 , 如果属性是boolean类型 , 没有set方法 , 是 is .
测试pojo类 :
Address.java
public class Address { private String address; public String getAddress() { return address; } public void setAddress(String address) { this.address = address; } } Student.java
package com.kuang.pojo; import java.util.List; import java.util.Map; import java.util.Properties; import java.util.Set; public class Student { private String name; private Address address; private String[] books; private List&lt;String> hobbys; private Map&lt;String,String> card; private Set&lt;String> games; private String wife; private Properties info; public void setName(String name) { this.name = name; } public void setAddress(Address address) { this.address = address; } public void setBooks(String[] books) { this.books = books; } public void setHobbys(List&lt;String> hobbys) { this.hobbys = hobbys; } public void setCard(Map&lt;String, String> card) { this.card = card; } public void setGames(Set&lt;String> games) { this.games = games; } public void setWife(String wife) { this.wife = wife; } public void setInfo(Properties info) { this.info = info; } public void show(){ System.out.println("name="+ name + ",address="+ address.getAddress() + ",books=" ); for (String book:books){ System.out.print("&lt;&lt;"+book+">>\t"); } System.out.println("\n爱好:"+hobbys); System.out.println("card:"+card); System.out.println("games:"+games); System.out.println("wife:"+wife); System.out.println("info:"+info); } } 6.3.扩展的注入 1、常量注入
&lt;bean id="student" class="com.kuang.pojo.Student"> &lt;property name="name" value="小明"/> &lt;/bean> 测试：
@Test public void test01(){ ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); Student student = (Student) context.getBean("student"); System.out.println(student.getName()); } 2、Bean注入
注意点：这里的值是一个引用，ref
&lt;bean id="addr" class="com.kuang.pojo.Address"> &lt;property name="address" value="重庆"/> &lt;/bean> &lt;bean id="student" class="com.kuang.pojo.Student"> &lt;property name="name" value="小明"/> &lt;property name="address" ref="addr"/> &lt;/bean> 3、数组注入
&lt;bean id="student" class="com.kuang.pojo.Student"> &lt;property name="name" value="小明"/> &lt;property name="address" ref="addr"/> &lt;property name="books"> &lt;array> &lt;value>西游记&lt;/value> &lt;value>红楼梦&lt;/value> &lt;value>水浒传&lt;/value> &lt;/array> &lt;/property> &lt;/bean> 4、List注入
&lt;property name="hobbys"> &lt;list> &lt;value>听歌&lt;/value> &lt;value>看电影&lt;/value> &lt;value>爬山&lt;/value> &lt;/list> &lt;/property> 5、Map注入
&lt;property name="card"> &lt;map> &lt;entry key="中国邮政" value="456456456465456"/> &lt;entry key="建设" value="1456682255511"/> &lt;/map> &lt;/property> 6、set注入
&lt;property name="games"> &lt;set> &lt;value>LOL&lt;/value> &lt;value>BOB&lt;/value> &lt;value>COC&lt;/value> &lt;/set> &lt;/property> 7、Null注入
&lt;property name="wife">&lt;null/>&lt;/property> 8、Properties注入
&lt;property name="info"> &lt;props> &lt;prop key="学号">20190604&lt;/prop> &lt;prop key="性别">男&lt;/prop> &lt;prop key="姓名">小明&lt;/prop> &lt;/props> &lt;/property> 测试结果： 9、p命名和c命名注入
p命名和c命名注入
User.java ：【注意：这里没有有参构造器！】
public class User { private String name; private int age; public void setName(String name) { this.name = name; } public void setAge(int age) { this.age = age; } @Override public String toString() { return "User{" + "name='" + name + '\'' + ", age=" + age + '}'; } } 1、P命名空间注入 : 需要在头文件中加入约束文件
导入约束 : xmlns:p="http://www.springframework.org/schema/p" &lt;!--P(属性: properties)命名空间 , 直接注入属性--> &lt;bean id="user" class="com.kuang.pojo.User" p:name="狂神" p:age="18"/> 2、c 命名空间注入 : 需要在头文件中加入约束文件
导入约束 : xmlns:c="http://www.springframework.org/schema/c" &lt;!--C(构造: Constructor)命名空间 , 使用构造器注入--> &lt;bean id="user" class="com.kuang.pojo.User" c:name="狂神" c:age="18"/> 发现问题：爆红了，刚才我们没有写有参构造！
解决：把有参构造器加上，这里也能知道，c 就是所谓的构造器注入！
测试代码：
@Test public void test02(){ ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); User user = (User) context.getBean("user"); System.out.println(user); } 6.4.Bean的作用域 Bean的作用域
在Spring中，那些组成应用程序的主体及由Spring IoC容器所管理的对象，被称之为bean。简单地讲，bean就是由IoC容器初始化、装配及管理的对象 . 几种作用域中，request、session作用域仅在基于web的应用中使用（不必关心你所采用的是什么web应用框架），只能用在基于web的Spring ApplicationContext环境。
Singleton(单例模式) 当一个bean的作用域为Singleton，那么Spring IoC容器中只会存在一个共享的bean实例，并且所有对bean的请求，只要id与该bean定义相匹配，则只会返回bean的同一实例。Singleton是单例类型，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，他都存在了，每次获取到的对象都是同一个对象。注意，Singleton作用域是Spring中的缺省作用域。要在XML中将bean定义成singleton，可以这样配置：
&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" scope="singleton"> 测试：
@Test public void test03(){ ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); User user = (User) context.getBean("user"); User user2 = (User) context.getBean("user"); System.out.println(user==user2); } Prototype(原型模式) 当一个bean的作用域为Prototype，表示一个bean定义对应多个对象实例。Prototype作用域的bean会导致在每次对该bean请求（将其注入到另一个bean中，或者以程序的方式调用容器的getBean()方法）时都会创建一个新的bean实例。Prototype是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。根据经验，对有状态的bean应该使用prototype作用域，而对无状态的bean则应该使用singleton作用域。在XML中将bean定义成prototype，可以这样配置：
&lt;bean id="account" class="com.foo.DefaultAccount" scope="prototype"/> 或者 &lt;bean id="account" class="com.foo.DefaultAccount" singleton="false"/> Request 当一个bean的作用域为Request，表示在一次HTTP请求中，一个bean定义对应一个实例；即每个HTTP请求都会有各自的bean实例，它们依据某个bean定义创建而成。该作用域仅在基于web的Spring ApplicationContext情形下有效。考虑下面bean定义：
&lt;bean id="loginAction" class=cn.csdn.LoginAction" scope="request"/> 1 针对每次HTTP请求，Spring容器会根据loginAction bean的定义创建一个全新的LoginAction bean实例，且该loginAction bean实例仅在当前HTTP request内有效，因此可以根据需要放心的更改所建实例的内部状态，而其他请求中根据loginAction bean定义创建的实例，将不会看到这些特定于某个请求的状态变化。当处理请求结束，request作用域的bean实例将被销毁。
Session 当一个bean的作用域为Session，表示在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。考虑下面bean定义：
&lt;bean id="userPreferences" class="com.foo.UserPreferences" scope="session"/> 1 针对某个HTTP Session，Spring容器会根据userPreferences bean定义创建一个全新的userPreferences bean实例，且该userPreferences bean仅在当前HTTP Session内有效。与request作用域一样，可以根据需要放心的更改所创建实例的内部状态，而别的HTTP Session中根据userPreferences创建的实例，将不会看到这些特定于某个HTTP Session的状态变化。当HTTP Session最终被废弃的时候，在该HTTP Session作用域内的bean也会被废弃掉。
7.Bean的自动装配 自动装配说明
自动装配是使用spring满足bean依赖的一种方法 spring会在应用上下文中为某个bean寻找其依赖的bean。 Spring中bean有三种装配机制，分别是：
在xml中显式配置； 在java中显式配置； 隐式的bean发现机制和自动装配。 这里我们主要讲第三种：自动化的装配bean。
Spring的自动装配需要从两个角度来实现，或者说是两个操作：
组件扫描(component scanning)：spring会自动发现应用上下文中所创建的bean； 自动装配(autowiring)：spring自动满足bean之间的依赖，也就是我们说的IoC/DI； 组件扫描和自动装配组合发挥巨大威力，使得显示的配置降低到最少。
推荐不使用自动装配xml配置 , 而使用注解 .
** **
测试环境搭建
1、新建一个项目
2、新建两个实体类，Cat Dog 都有一个叫的方法
public class Cat { public void shout() { System.out.println("miao~"); } } public class Dog { public void shout() { System.out.println("wang~"); } } 3、新建一个用户类 User
public class User { private Cat cat; private Dog dog; private String str; } 4、编写Spring配置文件
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User"> &lt;property name="cat" ref="cat"/> &lt;property name="dog" ref="dog"/> &lt;property name="str" value="qinjiang"/> &lt;/bean> &lt;/beans> 5、测试
public class MyTest { @Test public void testMethodAutowire() { ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); User user = (User) context.getBean("user"); user.getCat().shout(); user.getDog().shout(); } } 结果正常输出，环境OK
7.1.byName byName
autowire byName (按名称自动装配)
由于在手动配置xml过程中，常常发生字母缺漏和大小写等错误，而无法对其进行检查，使得开发效率降低。
采用自动装配将避免这些错误，并且使配置简单化。
测试：
1、修改bean配置，增加一个属性 autowire=“byName”
&lt;bean id="user" class="com.kuang.pojo.User" autowire="byName"> &lt;property name="str" value="qinjiang"/> &lt;/bean> 2、再次测试，结果依旧成功输出！
3、我们将 cat 的bean id修改为 catXXX
4、再次测试， 执行时报空指针java.lang.NullPointerException。因为按byName规则找不对应set方法，真正的setCat就没执行，对象就没有初始化，所以调用时就会报空指针错误。
小结：
当一个bean节点带有 autowire byName的属性时。
将查找其类中所有的set方法名，例如setCat，获得将set去掉并且首字母小写的字符串，即cat。 去spring容器中寻找是否有此字符串名称id的对象。 如果有，就取出注入；如果没有，就报空指针异常。 7.2.byType byType
autowire byType (按类型自动装配)
使用autowire byType首先需要保证：同一类型的对象，在spring容器中唯一。如果不唯一，会报不唯一的异常。
NoUniqueBeanDefinitionException 测试：
1、将user的bean配置修改一下 ： autowire=“byType”
2、测试，正常输出
3、在注册一个cat 的bean对象！
&lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat" class="com.kuang.pojo.Cat"/> &lt;bean id="cat2" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User" autowire="byType"> &lt;property name="str" value="qinjiang"/> &lt;/bean> 1234567 4、测试，报错：NoUniqueBeanDefinitionException
5、删掉cat2，将cat的bean名称改掉！测试！因为是按类型装配，所以并不会报异常，也不影响最后的结果。甚至将id属性去掉，也不影响结果。
这就是按照类型自动装配！
7.3.使用注解 使用注解
jdk1.5开始支持注解，spring2.5开始全面支持注解。
准备工作：利用注解的方式注入属性。
1、在spring配置文件中引入context文件头
xmlns:context="http://www.springframework.org/schema/context" http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd 2、开启属性注解支持！
&lt;context:annotation-config/> @Autowired @Autowired是按类型自动转配的，不支持id匹配。 需要导入 spring-aop的包！ 测试：
1、将User类中的set方法去掉，使用@Autowired注解
public class User { @Autowired private Cat cat; @Autowired private Dog dog; private String str; public Cat getCat() { return cat; } public Dog getDog() { return dog; } public String getStr() { return str; } } 2、此时配置文件内容
&lt;context:annotation-config/> &lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User"/> 3、测试，成功输出结果！
【小狂神科普时间】
@Autowired(required=false) 说明：false，对象可以为null；true，对象必须存对象，不能为null。
//如果允许对象为null，设置required = false,默认为true @Autowired(required = false) private Cat cat; @Qualifier @Autowired是根据类型自动装配的，加上@Qualifier则可以根据byName的方式自动装配 @Qualifier不能单独使用。 测试实验步骤：
1、配置文件修改内容，保证类型存在对象。且名字不为类的默认名字！
&lt;bean id="dog1" class="com.kuang.pojo.Dog"/> &lt;bean id="dog2" class="com.kuang.pojo.Dog"/> &lt;bean id="cat1" class="com.kuang.pojo.Cat"/> &lt;bean id="cat2" class="com.kuang.pojo.Cat"/> 2、没有加Qualifier测试，直接报错
3、在属性上添加Qualifier注解
@Autowired @Qualifier(value = "cat2") private Cat cat; @Autowired @Qualifier(value = "dog2") private Dog dog; 测试，成功输出！
@Resource @Resource如有指定的name属性，先按该属性进行byName方式查找装配； 其次再进行默认的byName方式进行装配； 如果以上都不成功，则按byType的方式自动装配。 都不成功，则报异常。 实体类：
public class User { //如果允许对象为null，设置required = false,默认为true @Resource(name = "cat2") private Cat cat; @Resource private Dog dog; private String str; } beans.xml
&lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat1" class="com.kuang.pojo.Cat"/> &lt;bean id="cat2" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User"/> 测试：结果OK
配置文件2：beans.xml ， 删掉cat2
&lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat1" class="com.kuang.pojo.Cat"/> 实体类上只保留注解
@Resource private Cat cat; @Resource private Dog dog; 结果：OK
结论：先进行byName查找，失败；再进行byType查找，成功。
小结
@Autowired与@Resource异同：
1、@Autowired与@Resource都可以用来装配bean。都可以写在字段上，或写在setter方法上。
2、@Autowired默认按类型装配（属于spring规范），默认情况下必须要求依赖对象必须存在，如果要允许null 值，可以设置它的required属性为false，如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用
3、@Resource（属于J2EE复返），默认按照名称进行装配，名称可以通过name属性进行指定。如果没有指定name属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。
它们的作用相同都是用注解方式注入对象，但执行顺序不同。@Autowired先byType，@Resource先byName。
8.使用注解开发 说明
在spring4之后，想要使用注解形式，必须得要引入aop的包
在配置文件当中，还得要引入一个context约束
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"> &lt;/beans> 8.1.Bean的实现 我们之前都是使用 bean 的标签进行bean注入，但是实际开发中，我们一般都会使用注解！
1、配置扫描哪些包下的注解 &lt;!--指定注解扫描包--> &lt;context:component-scan base-package="com.kuang.pojo"/> 2、在指定包下编写类，增加注解
@Component("user") // 相当于配置文件中 &lt;bean id="user" class="当前注解的类"/> public class User { public String name = "秦疆"; } 3、测试
@Test public void test(){ ApplicationContext applicationContext = new ClassPathXmlApplicationContext("beans.xml"); User user = (User) applicationContext.getBean("user"); System.out.println(user.name); } 8.2.属性注入 使用注解注入属性
1、可以不用提供set方法，直接在直接名上添加@value(“值”)
@Component("user") // 相当于配置文件中 &lt;bean id="user" class="当前注解的类"/> public class User { @Value("秦疆") // 相当于配置文件中 &lt;property name="name" value="秦疆"/> public String name; } 2、如果提供了set方法，在set方法上添加@value(“值”);
@Component("user") public class User { public String name; @Value("秦疆") public void setName(String name) { this.name = name; } } 8.3.衍生注解 我们这些注解，就是替代了在配置文件当中配置步骤而已！更加的方便快捷！
@Component三个衍生注解
为了更好的进行分层，Spring可以使用其它三个注解，功能一样，目前使用哪一个功能都一样。
@Controller：controller层 @Service：service层 @Repository：dao层 写上这些注解，就相当于将这个类交给Spring管理装配了！
8.4.自动装配注解 在Bean的自动装配已经讲过了，可以回顾！
8.5.作用域 @scope
singleton：默认的，Spring会采用单例模式创建这个对象。关闭工厂 ，所有的对象都会销毁。 prototype：多例模式。关闭工厂 ，所有的对象不会销毁。内部的垃圾回收机制会回收 @Controller("user") @Scope("prototype") public class User { @Value("秦疆") public String name; } 8.6.小结 XML与注解比较
XML可以适用任何场景 ，结构清晰，维护方便 注解不是自己提供的类使用不了，开发简单方便 xml与注解整合开发 ：推荐最佳实践
xml管理Bean 注解完成属性注入 使用过程中， 可以不用扫描，扫描是为了类上的注解 &lt;context:annotation-config/> 作用：
进行注解驱动注册，从而使注解生效 用于激活那些已经在spring容器里注册过的bean上面的注解，也就是显示的向Spring注册 如果不扫描包，就需要手动配置bean 如果不加注解驱动，则注入的值为null！ 9.基于Java类进行配置 JavaConfig 原来是 Spring 的一个子项目，它通过 Java 类的方式提供 Bean 的定义信息，在 Spring4 的版本， JavaConfig 已正式成为 Spring4 的核心功能 。
测试：
1、编写一个实体类，Dog
@Component //将这个类标注为Spring的一个组件，放到容器中！ public class Dog { public String name = "dog"; } 2、新建一个config配置包，编写一个MyConfig配置类
@Configuration //代表这是一个配置类 public class MyConfig { @Bean //通过方法注册一个bean，这里的返回值就Bean的类型，方法名就是bean的id！ public Dog dog(){ return new Dog(); } } 3、测试
@Test public void test2(){ ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MyConfig.class); Dog dog = (Dog) applicationContext.getBean("dog"); System.out.println(dog.name); } 4、成功输出结果！
导入其他配置如何做呢？
1、我们再编写一个配置类！
@Configuration //代表这是一个配置类 public class MyConfig2 { } 2、在之前的配置类中我们来选择导入这个配置类
@Configuration @Import(MyConfig2.class) //导入合并其他配置类，类似于配置文件中的 inculde 标签 public class MyConfig { @Bean public Dog dog(){ return new Dog(); } } 关于这种Java类的配置方式，我们在之后的SpringBoot 和 SpringCloud中还会大量看到，我们需要知道这些注解的作用即可！
10.代理模式 为什么要学习代理模式，因为AOP的底层机制就是动态代理！
代理模式：
静态代理 动态代理 学习aop之前 , 我们要先了解一下代理模式！
10.1静态代理 静态代理角色分析
抽象角色 : 一般使用接口或者抽象类来实现 真实角色 : 被代理的角色 代理角色 : 代理真实角色 ; 代理真实角色后 , 一般会做一些附属的操作 . 客户 : 使用代理角色来进行一些操作 . 代码实现
Rent . java 即抽象角色
//抽象角色：租房 public interface Rent { public void rent(); } Host . java 即真实角色
//真实角色: 房东，房东要出租房子 public class Host implements Rent{ public void rent() { System.out.println("房屋出租"); } } Proxy . java 即代理角色
//代理角色：中介 public class Proxy implements Rent { private Host host; public Proxy() { } public Proxy(Host host) { this.host = host; } //租房 public void rent(){ seeHouse(); host.rent(); fare(); } //看房 public void seeHouse(){ System.out.println("带房客看房"); } //收中介费 public void fare(){ System.out.println("收中介费"); } Client . java 即客户
//客户类，一般客户都会去找代理！ public class Client { public static void main(String[] args) { //房东要租房 Host host = new Host(); //中介帮助房东 Proxy proxy = new Proxy(host); //你去找中介！ proxy.rent(); } } 分析：在这个过程中，你直接接触的就是中介，就如同现实生活中的样子，你看不到房东，但是你依旧租到了房东的房子通过代理，这就是所谓的代理模式，程序源自于生活，所以学编程的人，一般能够更加抽象的看待生活中发生的事情。
静态代理的好处:
可以使得我们的真实角色更加纯粹 . 不再去关注一些公共的事情 . 公共的业务由代理来完成 . 实现了业务的分工 , 公共业务发生扩展时变得更加集中和方便 . 缺点 :
类多了 , 多了代理类 , 工作量变大了 . 开发效率降低 . 我们想要静态代理的好处，又不想要静态代理的缺点，所以 , 就有了动态代理 !
10.2.静态代理再理解 同学们练习完毕后，我们再来举一个例子，巩固大家的学习！
练习步骤：
1、创建一个抽象角色，比如咋们平时做的用户业务，抽象起来就是增删改查！
//抽象角色：增删改查业务 public interface UserService { void add(); void delete(); void update(); void query(); } 2、我们需要一个真实对象来完成这些增删改查操作
//真实对象，完成增删改查操作的人 public class UserServiceImpl implements UserService { public void add() { System.out.println("增加了一个用户"); } public void delete() { System.out.println("删除了一个用户"); } public void update() { System.out.println("更新了一个用户"); } public void query() { System.out.println("查询了一个用户"); } } 3、需求来了，现在我们需要增加一个日志功能，怎么实现！
思路1 ：在实现类上增加代码 【麻烦！】 思路2：使用代理来做，能够不改变原来的业务情况下，实现此功能就是最好的了！ 4、设置一个代理类来处理日志！代理角色
//代理角色，在这里面增加日志的实现 public class UserServiceProxy implements UserService { private UserServiceImpl userService; public void setUserService(UserServiceImpl userService) { this.userService = userService; } public void add() { log("add"); userService.add(); } public void delete() { log("delete"); userService.delete(); } public void update() { log("update"); userService.update(); } public void query() { log("query"); userService.query(); } public void log(String msg){ System.out.println("执行了"+msg+"方法"); } } 5、测试访问类：
public class Client { public static void main(String[] args) { //真实业务 UserServiceImpl userService = new UserServiceImpl(); //代理类 UserServiceProxy proxy = new UserServiceProxy(); //使用代理类实现日志功能！ proxy.setUserService(userService); proxy.add(); } } OK，到了现在代理模式大家应该都没有什么问题了，重点大家需要理解其中的思想；
我们在不改变原来的代码的情况下，实现了对原有功能的增强，这是AOP中最核心的思想
聊聊AOP：纵向开发，横向开发
10.3动态代理 动态代理的角色和静态代理的一样 . 动态代理的代理类是动态生成的 . 静态代理的代理类是我们提前写好的 动态代理分为两类 : 一类是基于接口动态代理 , 一类是基于类的动态代理 基于接口的动态代理&mdash;-JDK动态代理 基于类的动态代理–cglib 现在用的比较多的是 javasist 来生成动态代理 . 百度一下javasist 我们这里使用JDK的原生代码来实现，其余的道理都是一样的！、 JDK的动态代理需要了解两个类
核心 : InvocationHandler 和 Proxy ， 打开JDK帮助文档看看
【InvocationHandler：调用处理程序】
Object invoke(Object proxy, 方法 method, Object[] args)； //参数 //proxy - 调用该方法的代理实例 //method -所述方法对应于调用代理实例上的接口方法的实例。方法对象的声明类将是该方法声明的接口，它可以是代理类继承该方法的代理接口的超级接口。 //args -包含的方法调用传递代理实例的参数值的对象的阵列，或null如果接口方法没有参数。原始类型的参数包含在适当的原始包装器类的实例中，例如java.lang.Integer或java.lang.Boolean 。 【Proxy : 代理】
//生成代理类 public Object getProxy(){ return Proxy.newProxyInstance(this.getClass().getClassLoader(), rent.getClass().getInterfaces(),this); } 代码实现
抽象角色和真实角色和之前的一样！
Rent . java 即抽象角色
//抽象角色：租房 public interface Rent { public void rent(); } 1234 Host . java 即真实角色
//真实角色: 房东，房东要出租房子 public class Host implements Rent{ public void rent() { System.out.println("房屋出租"); } } 123456 ProxyInvocationHandler. java 即代理角色
public class ProxyInvocationHandler implements InvocationHandler { private Rent rent; public void setRent(Rent rent) { this.rent = rent; } //生成代理类，重点是第二个参数，获取要代理的抽象角色！之前都是一个角色，现在可以代理一类角色 public Object getProxy(){ return Proxy.newProxyInstance(this.getClass().getClassLoader(), rent.getClass().getInterfaces(),this); } // proxy : 代理类 method : 代理类的调用处理程序的方法对象. // 处理代理实例上的方法调用并返回结果 @Override public Object invoke(Object proxy, Method method, Object[] args) throwsThrowable { seeHouse(); //核心：本质利用反射实现！ Object result = method.invoke(rent, args); fare(); return result; } //看房 public void seeHouse(){ System.out.println("带房客看房"); } //收中介费 public void fare(){ System.out.println("收中介费"); } } 12345678910111213141516171819202122232425262728293031323334 Client . java
//租客 public class Client { public static void main(String[] args) { //真实角色 Host host = new Host(); //代理实例的调用处理程序 ProxyInvocationHandler pih = new ProxyInvocationHandler(); pih.setRent(host); //将真实角色放置进去！ Rent proxy = (Rent)pih.getProxy(); //动态生成对应的代理类！ proxy.rent(); } } 核心：一个动态代理 , 一般代理某一类业务 , 一个动态代理可以代理多个类，代理的是接口！、
10.4深化理解 我们来使用动态代理实现代理我们后面写的UserService！
我们也可以编写一个通用的动态代理实现的类！所有的代理对象设置为Object即可！
public class ProxyInvocationHandler implements InvocationHandler { private Object target; public void setTarget(Object target) { this.target = target; } //生成代理类 public Object getProxy(){ return Proxy.newProxyInstance(this.getClass().getClassLoader(), target.getClass().getInterfaces(),this); } // proxy : 代理类 // method : 代理类的调用处理程序的方法对象. public Object invoke(Object proxy, Method method, Object[] args) throwsThrowable { log(method.getName()); Object result = method.invoke(target, args); return result; } public void log(String methodName){ System.out.println("执行了"+methodName+"方法"); } } 测试！
public class Test { public static void main(String[] args) { //真实对象 UserServiceImpl userService = new UserServiceImpl(); //代理对象的调用处理程序 ProxyInvocationHandler pih = new ProxyInvocationHandler(); pih.setTarget(userService); //设置要代理的对象 UserService proxy = (UserService)pih.getProxy(); //动态生成代理类！ proxy.delete(); } } 测试，增删改查，查看结果！
10.5动态代理的好处 静态代理有的它都有，静态代理没有的，它也有！
可以使得我们的真实角色更加纯粹 . 不再去关注一些公共的事情 . 公共的业务由代理来完成 . 实现了业务的分工 , 公共业务发生扩展时变得更加集中和方便 . 一个动态代理 , 一般代理某一类业务 一个动态代理可以代理多个类，代理的是接口！ 11.AOP 11.1.什么是AOP AOP（Aspect Oriented Programming）意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生范型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。
11.2.Aop在Spring中的作用 提供声明式事务；允许用户自定义切面
以下名词需要了解下：
横切关注点：跨越应用程序多个模块的方法或功能。即是，与我们业务逻辑无关的，但是我们需要关注的部分，就是横切关注点。如日志 , 安全 , 缓存 , 事务等等 … 切面（ASPECT）：横切关注点 被模块化 的特殊对象。即，它是一个类。 通知（Advice）：切面必须要完成的工作。即，它是类中的一个方法。 目标（Target）：被通知对象。 代理（Proxy）：向目标对象应用通知之后创建的对象。 切入点（PointCut）：切面通知 执行的 “地点”的定义。 连接点（JointPoint）：与切入点匹配的执行点。 SpringAOP中，通过Advice定义横切逻辑，Spring中支持5种类型的Advice:
即 Aop 在 不改变原有代码的情况下 , 去增加新的功能 .
11.3.使用Spring实现Aop 【重点】使用AOP织入，需要导入一个依赖包！
&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjweaver --> &lt;dependency> &lt;groupId>org.aspectj&lt;/groupId> &lt;artifactId>aspectjweaver&lt;/artifactId> &lt;version>1.9.4&lt;/version> &lt;/dependency> 11.3.1.通过 Spring API 实现 第一种方式
首先编写我们的业务接口和实现类
public interface UserService { public void add(); public void delete(); public void update(); public void search(); } public class UserServiceImpl implements UserService{ @Override public void add() { System.out.println("增加用户"); } @Override public void delete() { System.out.println("删除用户"); } @Override public void update() { System.out.println("更新用户"); } @Override public void search() { System.out.println("查询用户"); } } 然后去写我们的增强类 , 我们编写两个 , 一个前置增强 一个后置增强
public class Log implements MethodBeforeAdvice { //method : 要执行的目标对象的方法 //objects : 被调用的方法的参数 //Object : 目标对象 @Override public void before(Method method, Object[] objects, Object o) throws Throwable { System.out.println( o.getClass().getName() + "的" + method.getName() + "方法被执行了"); } } public class AfterLog implements AfterReturningAdvice { //returnValue 返回值 //method被调用的方法 //args 被调用的方法的对象的参数 //target 被调用的目标对象 @Override public void afterReturning(Object returnValue, Method method, Object[] args,Object target) throws Throwable { System.out.println("执行了" + target.getClass().getName() +"的"+method.getName()+"方法," +"返回值："+returnValue); } } 最后去spring的文件中注册 , 并实现aop切入实现 , 注意导入约束 .
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd"> &lt;!--注册bean--> &lt;bean id="userService" class="com.kuang.service.UserServiceImpl"/> &lt;bean id="log" class="com.kuang.log.Log"/> &lt;bean id="afterLog" class="com.kuang.log.AfterLog"/> &lt;!--aop的配置--> &lt;aop:config> &lt;!--切入点 expression:表达式匹配要执行的方法--> &lt;aop:pointcut id="pointcut" expression="execution(* com.kuang.service.UserServiceImpl.*(..))"/> &lt;!--执行环绕; advice-ref执行方法 . pointcut-ref切入点--> &lt;aop:advisor advice-ref="log" pointcut-ref="pointcut"/> &lt;aop:advisor advice-ref="afterLog" pointcut-ref="pointcut"/> &lt;/aop:config> &lt;/beans> 测试
public class MyTest { @Test public void test(){ ApplicationContext context = newClassPathXmlApplicationContext("beans.xml"); UserService userService = (UserService) context.getBean("userService"); userService.search(); } } Aop的重要性 : 很重要 . 一定要理解其中的思路 , 主要是思想的理解这一块 .
Spring的Aop就是将公共的业务 (日志 , 安全等) 和领域业务结合起来 , 当执行领域业务时 , 将会把公共业务加进来 . 实现公共业务的重复利用 . 领域业务更纯粹 , 程序猿专注领域业务 , 其本质还是动态代理 .
11.3.2.自定义类来实现Aop 第二种方式
目标业务类不变依旧是userServiceImpl
第一步 : 写我们自己的一个切入类
public class DiyPointcut { public void before(){ System.out.println("---------方法执行前---------"); } public void after(){ System.out.println("---------方法执行后---------"); } 去spring中配置
&lt;!--第二种方式自定义实现--> &lt;!--注册bean--> &lt;bean id="diy" class="com.kuang.config.DiyPointcut"/> &lt;!--aop的配置--> &lt;aop:config> &lt;!--第二种方式：使用AOP的标签实现--> &lt;aop:aspect ref="diy"> &lt;aop:pointcut id="diyPonitcut" expression="execution(* com.kuang.service.UserServiceImpl.*(..))"/> &lt;aop:before pointcut-ref="diyPonitcut" method="before"/> &lt;aop:after pointcut-ref="diyPonitcut" method="after"/> &lt;/aop:aspect> &lt;/aop:config> 测试：
public class MyTest { @Test public void test(){ ApplicationContext context = newClassPathXmlApplicationContext("beans.xml"); UserService userService = (UserService) context.getBean("userService"); userService.add(); } } 11.3.4.使用注解实现 第三种方式
第一步：编写一个注解实现的增强类
package com.kuang.config; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.After; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; @Aspect public class AnnotationPointcut { @Before("execution(* com.kuang.service.UserServiceImpl.*(..))") public void before(){ System.out.println("---------方法执行前---------"); } @After("execution(* com.kuang.service.UserServiceImpl.*(..))") public void after(){ System.out.println("---------方法执行后---------"); } @Around("execution(* com.kuang.service.UserServiceImpl.*(..))") public void around(ProceedingJoinPoint jp) throws Throwable { System.out.println("环绕前"); System.out.println("签名:"+jp.getSignature()); //执行目标方法proceed Object proceed = jp.proceed(); System.out.println("环绕后"); System.out.println(proceed); } } 第二步：在Spring配置文件中，注册bean，并增加支持注解的配置
&lt;!--第三种方式:注解实现--> &lt;bean id="annotationPointcut" class="com.kuang.config.AnnotationPointcut"/> &lt;aop:aspectj-autoproxy/> aop:aspectj-autoproxy：说明
通过aop命名空间的&lt;aop:aspectj-autoproxy />声明自动为spring容器中那些配置@aspectJ切面的bean创建代理，织入切面。当然，spring 在内部依旧采用AnnotationAwareAspectJAutoProxyCreator进行自动代理的创建工作，但具体实现的细节已经被&lt;aop:aspectj-autoproxy />隐藏起来了 &lt;aop:aspectj-autoproxy />有一个proxy-target-class属性，默认为false，表示使用jdk动态代理织入增强，当配为&lt;aop:aspectj-autoproxy poxy-target-class="true"/>时，表示使用CGLib动态代理技术织入增强。不过即使proxy-target-class设置为false，如果目标类没有声明接口，则spring将自动使用CGLib动态代理。 12.整合MyBatis 步骤 1、导入相关jar包
junit
&lt;dependency> &lt;groupId>junit&lt;/groupId> &lt;artifactId>junit&lt;/artifactId> &lt;version>4.12&lt;/version> &lt;/dependency> mybatis
&lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis&lt;/artifactId> &lt;version>3.5.2&lt;/version> &lt;/dependency> mysql-connector-java
&lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;version>5.1.47&lt;/version> &lt;/dependency> spring相关
&lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>5.1.10.RELEASE&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-jdbc&lt;/artifactId> &lt;version>5.1.10.RELEASE&lt;/version> &lt;/dependency> aspectJ AOP 织入器
&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjweaver --> &lt;dependency> &lt;groupId>org.aspectj&lt;/groupId> &lt;artifactId>aspectjweaver&lt;/artifactId> &lt;version>1.9.4&lt;/version> &lt;/dependency> mybatis-spring整合包 【重点】
&lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis-spring&lt;/artifactId> &lt;version>2.0.2&lt;/version> &lt;/dependency> 配置Maven静态资源过滤问题！
&lt;build> &lt;resources> &lt;resource> &lt;directory>src/main/java&lt;/directory> &lt;includes> &lt;include>**/*.properties&lt;/include> &lt;include>**/*.xml&lt;/include> &lt;/includes> &lt;filtering>true&lt;/filtering> &lt;/resource> &lt;/resources> &lt;/build> 2、编写配置文件
3、代码实现
12.1回忆MyBatis 编写pojo实体类
package com.kuang.pojo; public class User { private int id; //id private String name; //姓名 private String pwd; //密码 } 实现mybatis的配置文件
&lt;?xml version="1.0" encoding="UTF-8" ?> &lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"> &lt;configuration> &lt;typeAliases> &lt;package name="com.kuang.pojo"/> &lt;/typeAliases> &lt;environments default="development"> &lt;environment id="development"> &lt;transactionManager type="JDBC"/> &lt;dataSource type="POOLED"> &lt;property name="driver" value="com.mysql.jdbc.Driver"/> &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8"/> &lt;property name="username" value="root"/> &lt;property name="password" value="123456"/> &lt;/dataSource> &lt;/environment> &lt;/environments> &lt;mappers> &lt;package name="com.kuang.dao"/> &lt;/mappers> &lt;/configuration> UserDao接口编写
public interface UserMapper { public List&lt;User> selectUser(); } 接口对应的Mapper映射文件
&lt;?xml version="1.0" encoding="UTF-8" ?> &lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"> &lt;mapper namespace="com.kuang.dao.UserMapper"> &lt;select id="selectUser" resultType="User"> select * from user &lt;/select> &lt;/mapper> 测试类
@Test public void selectUser() throws IOException { String resource = "mybatis-config.xml"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = newSqlSessionFactoryBuilder().build(inputStream); SqlSession sqlSession = sqlSessionFactory.openSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); List&lt;User> userList = mapper.selectUser(); for (User user: userList){ System.out.println(user); } sqlSession.close(); } 12.2.MyBatis-Spring学习 引入Spring之前需要了解mybatis-spring包中的一些重要类；
http://www.mybatis.org/spring/zh/index.html
什么是 MyBatis-Spring？
MyBatis-Spring 会帮助你将 MyBatis 代码无缝地整合到 Spring 中。
知识基础
在开始使用 MyBatis-Spring 之前，你需要先熟悉 Spring 和 MyBatis 这两个框架和有关它们的术语。这很重要
MyBatis-Spring 需要以下版本：
MyBatis-Spring MyBatis Spring 框架 Spring Batch Java 2.0 3.5+ 5.0+ 4.0+ Java 8+ 1.3 3.4+ 3.2.2+ 2.1+ Java 6+ 如果使用 Maven 作为构建工具，仅需要在 pom.xml 中加入以下代码即可：
&lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis-spring&lt;/artifactId> &lt;version>2.0.2&lt;/version> &lt;/dependency> 要和 Spring 一起使用 MyBatis，需要在 Spring 应用上下文中定义至少两样东西：一个 SqlSessionFactory 和至少一个数据映射器类。
在 MyBatis-Spring 中，可使用SqlSessionFactoryBean来创建 SqlSessionFactory。要配置这个工厂 bean，只需要把下面代码放在 Spring 的 XML 配置文件中：
&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"> &lt;property name="dataSource" ref="dataSource" /> &lt;/bean> 注意：SqlSessionFactory需要一个 DataSource（数据源）。这可以是任意的 DataSource，只需要和配置其它 Spring 数据库连接一样配置它就可以了。
在基础的 MyBatis 用法中，是通过 SqlSessionFactoryBuilder 来创建 SqlSessionFactory 的。而在 MyBatis-Spring 中，则使用 SqlSessionFactoryBean 来创建。
在 MyBatis 中，你可以使用 SqlSessionFactory 来创建 SqlSession。一旦你获得一个 session 之后，你可以使用它来执行映射了的语句，提交或回滚连接，最后，当不再需要它的时候，你可以关闭 session。
SqlSessionFactory有一个唯一的必要属性：用于 JDBC 的 DataSource。这可以是任意的 DataSource 对象，它的配置方法和其它 Spring 数据库连接是一样的。
一个常用的属性是 configLocation，它用来指定 MyBatis 的 XML 配置文件路径。它在需要修改 MyBatis 的基础配置非常有用。通常，基础配置指的是 &lt; settings> 或 &lt; typeAliases>元素。
需要注意的是，这个配置文件并不需要是一个完整的 MyBatis 配置。确切地说，任何环境配置（），数据源（）和 MyBatis 的事务管理器（）都会被忽略。SqlSessionFactoryBean 会创建它自有的 MyBatis 环境配置（Environment），并按要求设置自定义环境的值。
SqlSessionTemplate 是 MyBatis-Spring 的核心。作为 SqlSession 的一个实现，这意味着可以使用它无缝代替你代码中已经在使用的 SqlSession。
模板可以参与到 Spring 的事务管理中，并且由于其是线程安全的，可以供多个映射器类使用，你应该总是用 SqlSessionTemplate 来替换 MyBatis 默认的 DefaultSqlSession 实现。在同一应用程序中的不同类之间混杂使用可能会引起数据一致性的问题。
可以使用 SqlSessionFactory 作为构造方法的参数来创建 SqlSessionTemplate 对象。
&lt;bean id="sqlSession" class="org.mybatis.spring.SqlSessionTemplate"> &lt;constructor-arg index="0" ref="sqlSessionFactory" /> &lt;/bean> 现在，这个 bean 就可以直接注入到你的 DAO bean 中了。你需要在你的 bean 中添加一个 SqlSession 属性，就像下面这样：
public class UserDaoImpl implements UserDao { private SqlSession sqlSession; public void setSqlSession(SqlSession sqlSession) { this.sqlSession = sqlSession; } public User getUser(String userId) { return sqlSession.getMapper...; } } 按下面这样，注入 SqlSessionTemplate：
&lt;bean id="userDao" class="org.mybatis.spring.sample.dao.UserDaoImpl"> &lt;property name="sqlSession" ref="sqlSession" /> &lt;/bean> 12.3.整合实现一 1、引入Spring配置文件beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> 2、配置数据源替换mybaits的数据源
&lt;!--配置数据源：数据源有非常多，可以使用第三方的，也可使使用Spring的--> &lt;bean id="dataSource"class="org.springframework.jdbc.datasource.DriverManagerDataSource"> &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"/> &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8"/> &lt;property name="username" value="root"/> &lt;property name="password" value="123456"/> &lt;/bean> 3、配置SqlSessionFactory，关联MyBatis
&lt;!--配置SqlSessionFactory--> &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"> &lt;property name="dataSource" ref="dataSource"/> &lt;!--关联Mybatis--> &lt;property name="configLocation" value="classpath:mybatis-config.xml"/> &lt;property name="mapperLocations" value="classpath:com/kuang/dao/*.xml"/> &lt;/bean> 4、注册sqlSessionTemplate，关联sqlSessionFactory；
&lt;!--注册sqlSessionTemplate , 关联sqlSessionFactory--> &lt;bean id="sqlSession" class="org.mybatis.spring.SqlSessionTemplate"> &lt;!--利用构造器注入--> &lt;constructor-arg index="0" ref="sqlSessionFactory"/> &lt;/bean> 5、增加Dao接口的实现类；私有化sqlSessionTemplate
public class UserDaoImpl implements UserMapper { //sqlSession不用我们自己创建了，Spring来管理 private SqlSessionTemplate sqlSession; public void setSqlSession(SqlSessionTemplate sqlSession) { this.sqlSession = sqlSession; } public List&lt;User> selectUser() { UserMapper mapper = sqlSession.getMapper(UserMapper.class); return mapper.selectUser(); } } 6、注册bean实现
&lt;bean id="userDao" class="com.kuang.dao.UserDaoImpl"> &lt;property name="sqlSession" ref="sqlSession"/> &lt;/bean> 7、测试
@Test public void test2(){ ApplicationContext context = newClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); } 结果成功输出！现在我们的Mybatis配置文件的状态！发现都可以被Spring整合！
&lt;?xml version="1.0" encoding="UTF-8" ?> &lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"> &lt;configuration> &lt;typeAliases> &lt;package name="com.kuang.pojo"/> &lt;/typeAliases> &lt;/configuration> 12.4.整合实现二 mybatis-spring1.2.3版以上的才有这个 .
官方文档截图 :
dao继承Support类 , 直接利用 getSqlSession() 获得 , 然后直接注入SqlSessionFactory . 比起方式1 , 不需要管理SqlSessionTemplate , 而且对事务的支持更加友好 . 可跟踪源码查看
测试：
1、将我们上面写的UserDaoImpl修改一下
public class UserDaoImpl extends SqlSessionDaoSupport implements UserMapper { public List&lt;User> selectUser() { UserMapper mapper = getSqlSession().getMapper(UserMapper.class); return mapper.selectUser(); } } 2、修改bean的配置
&lt;bean id="userDao" class="com.kuang.dao.UserDaoImpl"> &lt;property name="sqlSessionFactory" ref="sqlSessionFactory" /> &lt;/bean> 3、测试
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); } 总结 : 整合到spring以后可以完全不要mybatis的配置文件，除了这些方式可以实现整合之外，我们还可以使用注解来实现，这个等我们后面学习SpringBoot的时候还会测试整合！
13.声明式事务 13.1.回顾事务 事务在项目开发过程非常重要，涉及到数据的一致性的问题，不容马虎！ 事务管理是企业级应用程序开发中必备技术，用来确保数据的完整性和一致性。 事务就是把一系列的动作当成一个独立的工作单元，这些动作要么全部完成，要么全部不起作用。
事务四个属性ACID
原子性（atomicity） 事务是原子性操作，由一系列动作组成，事务的原子性确保动作要么全部完成，要么完全不起作用 一致性（consistency） 一旦所有事务动作完成，事务就要被提交。数据和资源处于一种满足业务规则的一致性状态中 隔离性（isolation） 可能多个事务会同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏 持久性（durability） 事务一旦完成，无论系统发生什么错误，结果都不会受到影响。通常情况下，事务的结果被写到持久化存储器中 测试 将上面的代码拷贝到一个新项目中
在之前的案例中，我们给userDao接口新增两个方法，删除和增加用户；
//添加一个用户 int addUser(User user); //根据id删除用户 int deleteUser(int id); mapper文件，我们故意把 deletes 写错，测试！
&lt;insert id="addUser" parameterType="com.kuang.pojo.User"> insert into user (id,name,pwd) values (#{id},#{name},#{pwd}) &lt;/insert> &lt;delete id="deleteUser" parameterType="int"> deletes from user where id = #{id} &lt;/delete> 编写接口的实现类，在实现类中，我们去操作一波
public class UserDaoImpl extends SqlSessionDaoSupport implements UserMapper { //增加一些操作 public List&lt;User> selectUser() { User user = new User(4,"小明","123456"); UserMapper mapper = getSqlSession().getMapper(UserMapper.class); mapper.addUser(user); mapper.deleteUser(4); return mapper.selectUser(); } //新增 public int addUser(User user) { UserMapper mapper = getSqlSession().getMapper(UserMapper.class); return mapper.addUser(user); } //删除 public int deleteUser(int id) { UserMapper mapper = getSqlSession().getMapper(UserMapper.class); return mapper.deleteUser(id); } } 测试
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); } 报错：sql异常，delete写错了
结果 ：插入成功！
没有进行事务的管理；我们想让他们都成功才成功，有一个失败，就都失败，我们就应该需要事务！
以前我们都需要自己手动管理事务，十分麻烦！
但是Spring给我们提供了事务管理，我们只需要配置即可；
13.2.Spring中的事务管理 Spring在不同的事务管理API之上定义了一个抽象层，使得开发人员不必了解底层的事务管理API就可以使用Spring的事务管理机制。Spring支持编程式事务管理和声明式的事务管理。
编程式事务管理
将事务管理代码嵌到业务方法中来控制事务的提交和回滚 缺点：必须在每个事务操作业务逻辑中包含额外的事务管理代码 声明式事务管理
一般情况下比编程式事务好用。 将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。 将事务管理作为横切关注点，通过aop方法模块化。Spring中通过Spring AOP框架支持声明式事务管理。 使用Spring管理事务，注意头文件的约束导入 : tx
xmlns:tx="http://www.springframework.org/schema/tx" http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"> 事务管理器
无论使用Spring的哪种事务管理策略（编程式或者声明式）事务管理器都是必须的。 就是 Spring的核心事务管理抽象，管理封装了一组独立于技术的方法。 JDBC事务
&lt;bean id="transactionManager"class="org.springframework.jdbc.datasource.DataSourceTransactionManager"> &lt;property name="dataSource" ref="dataSource" /> &lt;/bean> 配置好事务管理器后我们需要去配置事务的通知
&lt;!--配置事务通知--> &lt;tx:advice id="txAdvice" transaction-manager="transactionManager"> &lt;tx:attributes> &lt;!--配置哪些方法使用什么样的事务,配置事务的传播特性--> &lt;tx:method name="add" propagation="REQUIRED"/> &lt;tx:method name="delete" propagation="REQUIRED"/> &lt;tx:method name="update" propagation="REQUIRED"/> &lt;tx:method name="search*" propagation="REQUIRED"/> &lt;tx:method name="get" read-only="true"/> &lt;tx:method name="*" propagation="REQUIRED"/> &lt;/tx:attributes> &lt;/tx:advice> spring事务传播特性：
事务传播行为就是多个事务方法相互调用时，事务如何在这些方法间传播。spring支持7种事务传播行为：
propagation_requierd：如果当前没有事务，就新建一个事务，如果已存在一个事务中，加入到这个事务中，这是最常见的选择。 propagation_supports：支持当前事务，如果没有当前事务，就以非事务方法执行。 propagation_mandatory：使用当前事务，如果没有当前事务，就抛出异常。 propagation_required_new：新建事务，如果当前存在事务，把当前事务挂起。 propagation_not_supported：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 propagation_never：以非事务方式执行操作，如果当前事务存在则抛出异常。 propagation_nested：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与propagation_required类似的操作 Spring 默认的事务传播行为是 PROPAGATION_REQUIRED，它适合于绝大多数的情况。
假设 ServiveX#methodX() 都工作在事务环境下（即都被 Spring 事务增强了），假设程序中存在如下的调用链：Service1#method1()->Service2#method2()->Service3#method3()，那么这 3 个服务类的 3 个方法通过 Spring 的事务传播机制都工作在同一个事务中。
就好比，我们刚才的几个方法存在调用，所以会被放在一组事务当中！
配置AOP
导入aop的头文件！
&lt;!--配置aop织入事务--> &lt;aop:config> &lt;aop:pointcut id="txPointcut" expression="execution(* com.kuang.dao.*.*(..))"/> &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="txPointcut"/> &lt;/aop:config> 进行测试
删掉刚才插入的数据，再次测试！
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); }</content></entry><entry><title>精通Git</title><url>https://codingroam.github.io/post/%E7%B2%BE%E9%80%9Agit/</url><categories><category>Git</category></categories><tags><tag>Git</tag><tag>Learning</tag></tags><content type="html"> 精通Git
精通Git 1、Git基础 （1）三种状态 Git 有三种状态，你的文件可能处于其中之一：已提交（committed）、已修改（modified）和已暂存（staged）。已提交表示数据已经安全的保存在本地数据库中。已修改表示修改了文件，但还没保存到数据库中。已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中
由此引入 Git 项目的三个工作区域的概念：Git 仓库、工作目录以及暂存区域。
Git 仓库目录是 Git 用来保存项目的元数据和对象数据库的地方。这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。
工作目录是对项目的某个版本独立提取出来的内容。这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。
暂存区域是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。有时候也被称作`‘索引’&rsquo;，不过一般说法还是叫暂存区域。
总结：如果 Git 目录中保存着的特定版本文件，就属于已提交状态。如果作了修改并已放入暂存区域，就属于已暂存状态。如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。
（2）记录数据方式 Git直接记录快照，而非差异比较
Git 和其它版本控制系统（包括 Subversion 和近似工具）的主要差别在于 Git 对待数据的方法。概念上来区分，其它大部分系统以文件变更列表的方式存储信息。这类系统（CVS、Subversion、Perforce、Bazaar 等等）将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。
Git 不按照以上方式对待或保存数据。反之，Git 更像是把数据看作是对小型文件系统的一组快照。每次你提交
更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。Git 对待数据更像是一个 快照流。
Git 更像是一个小型的文件系统，提供了许多以此为基础构建的超强工具，而不只是一个简单的 VCS
（3）近乎所有操作都是本地执行和保证文件完整性** 2、配置信息 ​ &ndash;global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 &ndash;global 选项的命令来配置
git config &ndash;global user.name &ldquo;kw&rdquo;
git config &ndash;global user.email 1121034507@qq.com
git config &ndash;list &lt;!--命令来列出所有 Git 当时能找到的配置-->
git config &ndash;globalcredential.helper cache/store 缓存用户名/密码，cache-15分钟 store-永久
配置Git可以用git config &ndash;list查看配置项，再使用命令git config 配置项 设置值
3、git基础操作 git init &lt;!--在当前目录初始化git-->
**$ git add *.c ** &lt;!--暂存命令，添加一个或多个文件到暂存区（开始跟踪一个新文件，或者把已跟踪的文件放到暂存区，概括起来就是添加内容到下一次提交中）-->
$ git add LICENSE
$ git commit -m &lsquo;initial project version&rsquo; &lt;!--提交命令,将暂存区文件提交到本地仓库-->
git clone https://github.com/libgit2/libgit2
[别名]&lt;!--克隆远程仓库-->
**git status **&lt;!--检查当前文件状态-->
$ git status -s
M README &lt;!--右边的 M 表示该文件被修改了但是还没放入暂存区-->
M lib/simplegit.rb &lt;!--左边的 M文件被修改了并将修改后的文件放入了暂存区-->
MM Rakefile &lt;!--在工作区被修改并提交到暂存区后又在工作区中被修改了，所以在暂存区和工作区都有该文件被修改了的记录-->
**A lib/git.rb ** &lt;!--新添加到暂存区中的文件前面有 A 标记，修改过的文件前面有 M 标记-->
?? LICENSE.txt &lt;!--新添加的未跟踪文件前面有 ?? 标记-->
git diff &lt;!--**查看已暂存和未暂存的修改**-->
要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff： 要查看已暂存的将要添加到下次提交里的内容，可以用 git diff &ndash;cached/&ndash;staged git commit -m &ldquo;message&rdquo; &lt;!--将暂存区文件提交到本地仓库-->
git commit -a -m &ldquo;message&rdquo; &lt;!--将已追踪文件提交到本地仓库，省略git add .操作-->
**git rm 文件名 **&lt;!--从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。该命令连带从工作目录中删除文件-->
git rm &ndash;cached README&lt;!--从本地仓库删除，当然也包括暂存区和工作目录-->
**git rm log/\*.log **&lt;!--删除 log/ 目录下扩展名为 .log 的所有文件。星号 * 之前的反斜杠 \，因为 Git 有它自己的文件模式扩展匹配方式-->
git rm \*~ &lt;!--删除以 ~ 结尾的所有文件-->
git mv file_from file_to &lt;!--改名操作-->
**git log **&lt;!--查看提交历史-->
git log -p -2 &lt;!--（-p）查看提交差异，2表示最近两次提交-->**
**git log &ndash;stat **&lt;!--简略信息-->**
**git log &ndash;pretty=format:"%h - %an, %ar : %s" ** &lt;!-- --pretty。这个选项可以指定使用不同于默认格式的方式展示提交历史-->
ca82a6d - Scott Chacon, 6 years ago : changed the version number 085bb3b - Scott Chacon, 6 years ago : removed unnecessary test a11bef0 - Scott Chacon, 6 years ago : first commit ​ 用 &ndash;author 选项显示指定作者的提交，用 &ndash;grep 选项搜索提交说明中的关键字。（请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用 &ndash;all-match 选项。否则，满足任意一个条件的提交都会被匹配出来）。另一个非常有用的筛选选项是 -S，可以列出那些添加或移除了某些字符串的提交。比如说，你想找出添加或移除了某一个特定函数的引用的提交，你可以这样使用： $ git log -Sfunction_name
​
选项 说明 -(n) 仅显示最近的 n 条提交 &ndash;since, &ndash;after 仅显示指定时间之后的提交。 &ndash;until, &ndash;before 仅显示指定时间之前的提交 &ndash;author 仅显示指定作者相关的提交。 &ndash;committer 仅显示指定提交者相关的提交。 &ndash;grep 仅显示含指定关键字的提交 -S 仅显示添加或移除了某个关键字的提交 git tag 查看标签
git tag -a v1.4 -m &lsquo;my version 1.4&rsquo; 增加标签V1.4
git tag tagname 轻量标签
$ git log --pretty=oneline 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge branch 'experiment' a6b4c97498bd301d84096da251c98a07c7723e65 beginning write support 0d52aaab4479697da7686c15f77a3d64d9165190 one more thing $ git tag -a v1.2 0d52aaa //对历史提交打标签，如果不加提交校验和会对在最近一次的提交打tag git push origin [tagname] 推送tag到远程仓库（git push不会推送tag）
git push origin &ndash;tags 推送所有标签
git checkout -b [branchname] [tagname] 在特定的标签上创建一个新分支
Switched to a new branch &lsquo;version2&rsquo;
git config 设置别名
$ git config --global alias.co checkout $ git config --global alias.br branch $ git config --global alias.ci commit $ git config --global alias.st status 4、Git分支 ​ Git 保存的不是文件的变化或者差异，而是一系列不同时刻的文件快照。Git 的分支，其实本质上仅仅是指向提交对象的可变指针。Git 的默认分支名字是 master。在多次提交操作之后，其实已经有一个指向最后那个提交对象的 master 分支。它会在每次的提交操作中自动向前移动。Git 又是怎么知道当前在哪一个分支上呢？也很简单，它有一个名为 HEAD 的特殊指针
git branch testing 创建分支testing
git checkout testing 切换分支
$ vim test.txt $ git commit -a -m 'made a change' 如图所示，testing 分支向前移动了，但是 master 分支却没有，它仍然指向运行 git checkout 时所指的对象。分支切换会改变你工作目录中的文件在切换分支时，一定要注意你工作目录里的文件会被改变。如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。如果 Git 不能干净利落地完成这个任务，它将禁止切换分支，要留意工作目录和暂存区里那些还没有被提交的修改，它可能会和你即将检出的分支产生冲突从而阻止 Git 切换到该分支（Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样）
git checkout -b iss53 新建并检出分支iss53
//等于以下两条命令 $ git branch iss53 $ git checkout iss53 ​ 从一个远程跟踪分支检出一个本地分支会自动创建一个叫做 “跟踪分支”（有时候也叫做 “上游分支”）。跟踪分支是与远程分支有直接关系的本地分支。如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。
​ 当克隆一个仓库时，它通常会自动地创建一个跟踪 origin/master 的 master 分支。然而，如果你愿意的话可以设置其他的跟踪分支 - 其他远程仓库上的跟踪分支，或者不跟踪 master 分支。最简单的就是之前看到的例子，运行 git checkout -b [branch] [remotename]/[branch]。这是一个十分常用的操作所以 Git 提供了 &ndash;track 快捷方式：
$ git checkout --track origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' 如果想要将本地分支与远程分支设置为不同名字，你可以轻松地增加一个不同名字的本地分支的上一个命令：
$ git checkout -b sf origin/serverfix Branch sf set up to track remote branch serverfix from origin. Switched to a new branch 'sf' 现在，本地分支 sf 会自动从 origin/serverfix 拉取。
设置已有的本地分支跟踪一个刚刚拉取下来的远程分支，或者想要修改正在跟踪的上游分支，你可以在任意时间使用 -u 或 &ndash;set-upstream-to 选项运行 git branch 来显式地设置。
$ git branch -u origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. git merge 合并分支
$ git checkout master //检出master分支 $ git merge hotfix //合并testing分支到master 遇到冲突时的分支合并：如果在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法干净的合并它们，这就是合并冲突
​
$ git merge iss53 Auto-merging index.html CONFLICT (content): Merge conflict in index.html Automatic merge failed; fix conflicts and then commit the result. //查看具体冲突信息 $ git status On branch master You have unmerged paths. (fix conflicts and run "git commit") Unmerged paths: (use "git add &lt;file>..." to mark resolution) both modified: index.html no changes added to commit (use "git add" and/or "git commit -a") //解决冲突 手动修改冲突文件后，对每个文件使用 git add 命令来将其标记为冲突已解决 $ git add index.html //查看状态，已无冲突 $ git status On branch master All conflicts fixed but you are still merging. (use "git commit" to conclude merge) Changes to be committed: modified: index.html //合并提交 $ git commit 使用git commit提交合并 git branch 分支管理，不加任务参数情况下显示所有分支列表
$ git branch iss53 * master //当前检出分支 testing git branch -v 要查看每一个分支的最后一次提交
$ git branch -v iss53 93b412c fix javascript issue * master 7a98805 Merge branch 'iss53' testing 782fd34 add scott to the author list in the readmes git branch -vv 查看设置的所有跟踪分支
$ git branch -vv iss53 7e424c3 [origin/iss53: ahead 2] forgot the brackets master 1ae2a45 [origin/master] deploying index fix * serverfix f8674d9 [teamone/server-fix-good: ahead 3, behind 1] this should do it testing 5ea463a trying something new ​ 这里可以看到 iss53 分支正在跟踪 origin/iss53 并且 “ahead” 是 2，意味着本地有两个提交还没有推送到服务器上。也能看到 master 分支正在跟踪 origin/master 分支并且是最新的。接下来可以看到serverfix 分支正在跟踪 teamone 服务器上的 server-fix-good 分支并且领先 2 落后 1，意味着服务器上有一次提交还没有合并入同时本地有三次提交还没有推送。最后看到 testing 分支并没有跟踪任何远程分支。
​ 需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。可以像这样做：$ git fetch &ndash;all; git branch -vv
git branch [&ndash;no-merged/&ndash;merged] 查看已经合并或尚未合并到当前分支的分支
git branch -d branchname 删除分支 -d无法删除未合并的分支，-D可以强制删除
远程分支 git ls-remote (remote)或者git remote show (remote)来显示远程分支列表或者详细信息
​ Git 的 clone 命令会为你自动将其命名为 origin，拉取它的所有数据，创建一个指向它的 master 分支的指针，并且在本地将命名为 origin/master。Git 也会给你一个与 origin 的 master 分支在指向同一个地方的本地 master 分支，这样你就有工作的基础
​ 远程仓库名字 “origin” 与分支名字 “master” 一样，在 Git 中并没有任何特别的含义一样。同时 “master” 是当你运行 git init 时默认的起始分支名字，原因仅仅是它的广泛使用，“origin” 是当你运行 git clone 时默认的远程仓库名字。如果你运行 git clone -o booyah，那么你默认的远程分支名字将会是 booyah/master
​ ​ 如果你在本地的 master 分支做了一些工作，然而在同一时间，其他人推送提交到 git.ourcompany.com 并更新了它的 master 分支，那么你的提交历史将向不同的方向前进。也许，只要你不与 origin 服务器连接，你的 origin/master 指针就不会移动。
​ git fetch origin 在进行同步工作工作时，从远程仓库抓取本地没有的数据，更新本地数据库并将origin/master向后移动
​
git remote add 添加远程仓库分支
git remote add 别名 远程仓库
git fetch 拉取远程仓库
​ 当 git fetch 命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。它只会获取数据然后让你自己合并。然而，有一个命令叫作 git pull 在大多数情况下它的含义是一个 git fetch 紧接着一个git merge 命令。如果有一个像之前章节中演示的设置好的跟踪分支，不管它是显式地设置还是通过 clone 或checkout 命令为你创建的，git pull 都会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。
​ 运行 git fetch teamone 来抓取远程仓库 teamone 有而本地没有的数据。因为那台服务器上现
有的数据是 origin 服务器上的一个子集，所以 Git 并不会抓取数据而是会设置远程跟踪分支
teamone/master 指向 teamone 的 master 分支
git push 推送
git push (remote) (branch)
git push &ndash;all 推送到所有远程仓库
git push origin &ndash;delete serverfix 删除远程分支serverfix $ git push origin --delete serverfix To https://github.com/schacon/simplegit - [deleted] serverfix 变基
在 Git 中整合来自不同分支的修改主要有两种方法：merge 以及 rebase
​ ​ 之前介绍过，整合分支最容易的方法是 merge 命令。它会把两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交）。
使用rebase也可以达到目的，提取在 C4 中引入的补丁和修改，然后在 C3 的基础上再应用一次。在 Git 中，这种操作就叫做 变基。你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样
$ git checkout experiment $ git rebase master First, rewinding head to replay your work on top of it... Applying: added staged command 它的原理是首先找到这两个分支（即当前分支 experiment、变基操作的目标基底分支 master）的最近共同祖先 C2，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底 C3, 最后以此将之前另存为临时文件的修改依序应用。（译注：写明了 commit id，以便理解，下同）
现在回到 master 分支，进行一次快进合并。
$ git checkout master $ git merge experiment ​ 此时，C4&rsquo; 指向的快照就和上面使用 merge 命令的例子中 C5 指向的快照一模一样了。这两种整合方法的最终结果没有任何区别，但是变基使得提交历史更加整洁。你在查看一个经过变基的分支的历史记录时会发现，尽管实际的开发工作是并行的，但它们看上去就像是先后串行的一样，提交历史是一条直线没有分叉。
​ 一般我们这样做的目的是为了确保在向远程分支推送时能保持提交历史的整洁——例如向某个别人维护的项目贡献代码时。在这种情况下，你首先在自己的分支里进行开发，当开发完成时你需要先将你的代码变基到origin/master 上，然后再向主项目提交修改。这样的话，该项目的维护者就不再需要进行整合工作，只需要快进合并便可。
​ 请注意，无论是通过变基，还是通过三方合并，整合的最终结果所指向的快照始终是一样的，只不过提交历史不同罢了。变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。
变基的风险
呃，奇妙的变基也并非完美无缺，要用它得遵守一条准则：不要对在你的仓库外有副本的分支执行变基。</content></entry><entry><title>Spring学习记录</title><url>https://codingroam.github.io/post/spring%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url><categories><category>Spring</category></categories><tags><tag>Spring</tag><tag>源码</tag><tag>Learning</tag></tags><content type="html"> 根据雷神《spring注解驱动开发》整理记录知识点
Spring学习记录 本文是一篇spring(4.3.12.release版本)源码相关的文章。本文的源码解析就不按照传统的去贴代码的方式去讲解spring源码了，本文的源码解析以流程的方式来讲解spring在每一步都干了什么，所以谓之曰spring源码导读。。。。
在正式开始spring源码导读之前，读者总得知道spring里的各个标签是干啥的吧，因此文中前一部分罗列了spring常见的注解用法。并搞了点SpringAOP和spring事务源码的解析作为后面正式开始的导读的开胃菜
介绍完了，让我们开始吧！！！。
spring注解 @Configuration 用于标注配置类 @Bean 结合@Configuration（full mode）使用或结合@Component（light mode）使用。可以导入第三方组件,入方法有参数默认从IOC容器中获取，可以指定initMethod和destroyMethod 指定初始化和销毁方法,多实例对象不会调用销毁方法. 包扫描@ComponentScan (@ComponentScans可以配置多个扫描,@TypeFilter:指定过滤规则,自己实现TypeFilter类) 组件(@Service、@Controller、@Repository):包扫描+组件注解导入注解。 @Scope:设置组件作用域 1.prototype:多例的2.singleton:单例的（默认值） @Lazy 懒加载 @Conditional({Condition}):按照一定的条件进行判断,满足条件给容器中注册Bean,传入Condition数组,，使用时需自己创建类继承Condition然后重写match方法。 @Import[快速给容器中导入一个组件] Import(类名),容器中就会自动注册这个组件，id默认是组件的全名 ImportSelector：返回需要导入的组件的全类名的数组 ImportBeanDefinitionRegistrar：手动注册bean FactoryBean:工厂Bean,交给spring用来生产Bean到spring容器中.可以通过前缀&amp;来获取工厂Bean本身. @Value:给属性赋值,也可以使用SpEL和外部文件的值 @PropertySource:读取外部配置文件中的k/v保存到运行环境中,结合@value使用,或使用ConfigurableEnvironment获取 @Profile:结合@Bean使用,默认为default环境,可以通过命令行参数来切换环境 自定义组件使用Spring容器底层的组件:需要让自定义组件实现xxxAware，(例如:ApplicationContextAware),spring在创建对象的时候,会帮我们自动注入。spring通过BeanPostProcessor机制来实现XXXXAware的自动注入。 ApplicationContextProcessor.java private void invokeAwareInterfaces(Object bean) { if (bean instanceof Aware) { if (bean instanceof ResourceLoaderAware) { ((ResourceLoaderAware)bean).setResourceLoader(this.applicationContext); } if (bean instanceof ApplicationContextAware) { ((ApplicationContextAware)bean).setApplicationContext(this.applicationContext); } } } @Autowried 装配优先级如下: 使用按照类型去容器中找对应的组件 按照属性名称去作为组件id去找对应的组件 @Qualifier:指定默认的组件,结合@Autowried使用 &ndash;标注在构造器:spring创建对象调用构造器创建对象 &ndash;标注在方法上: @Primary:spring自动装配的时候,默认首先bean,配合@Bean使用 @Resource(JSR250):jsr规范:按照组件名称进行装配 @Inject(JSR330):jsr规范和@Autowired功能一致,不支持require=false; Bean生命周期: 初始化和销毁
通过@Bean 指定init-method和destroy-method 实现InitializingBean定义初始化逻辑,实现DisposableBean定义销毁方法 实现BeanPostProcessor接口的后置拦截器放入容器中，可以拦截bean初始化，并可以在被拦截的Bean的初始化前后进行一些处理工作。 spring底层常用的BeanPostProcessor：
* BeanValidationPostProcessor用来实现数据校验 * AutowireAnnotationBeanPostProcessor,@Autowire实现 * ApplicationContextProcessor实现XXXAware的自动注入。 执行时机
doCreateBean -populateBean（）：给bean的各种属性赋值 -initializeBean（）：初始化bean -处理Aware方法 -applyBeanPostProcessorsBeforeInitialization：后置处理器的实例化前拦截 -invokeInitMethods:执行@Bean指定的initMethod -applyBeanPostProcessorsAfterInitialization：后置处理器的实例化后拦截 SpringAOP实现原理 使用步骤
@EnableAspectJAutoProxy 开启基于注解的aop模式 @Aspect：定义切面类，切面类里定义通知 @PointCut 切入点，可以写切入点表达式，指定在哪个方法切入 通知方法 @Before(前置通知) @After(后置通知) @AfterReturning(返回通知) @AfterTrowing(异常通知)@Around(环绕通知) JoinPoint：连接点,是一个类，配合通知使用，用于获取切入的点的信息 SpringAop原理
@EnableAspectJAutoProxy @EnableAspectJAutoProxy 通过@Import(AspectJAutoProxyRegistrar.class)给spring容器中导入了一个AnnotationAwareAspectJAutoProxyCreator。 AnnotationAwareAspectJAutoProxyCreator实现了InstantiationAwareBeanPostProcessor,InstantiationAwareBeanPostProcessor是一个BeanPostProcessor。它可以拦截spring的Bean初始化(Initialization)前后和实例化(Initialization)前后。 AnnotationAwareAspectJAutoProxyCreator的postProcessBeforeInstantiation(bean实例化前)：会通过调用isInfrastructureClass(beanClass)来判断 被拦截的类是否是基础类型的Advice、PointCut、Advisor、AopInfrastructureBean，或者是否是切面（@Aspect），若是则放入adviseBean集合。这里主要是用来处理我们的切面类。 AnnotationAwareAspectJAutoProxyCreator的BeanPostProcessorsAfterInitialization（bean初始化后）： 首先找到被拦截的Bean的匹配的增强器（通知方法），这里有切入点表达式匹配的逻辑 将增强器保存到proxyFactory中， 根据被拦截的Bean是否实现了接口，spring自动决定使用JdkDynamicAopProxy还是ObjenesisCglibAopProxy 最后返回被拦截的Bean的代理对象，注册到spring容器中 代理Bean的目标方法执行过程：CglibAopProxy.intercept(); 保存所有的增强器，并处理转换为一个拦截器链 如果没有拦截器链，就直接执行目标方法 如果有拦截器链，就将目标方法，拦截器链等信息传入并创建CglibMethodInvocation对象，并调用proceed()方法获取返回值。proceed方法内部会依次执行拦截器链。 spring 声明式事务 基本步骤
配置数据源：DataSource 配置事务管理器来控制事务：PlatformTransactionManager @EnableTransactionManagement开启基于注解的事务管理功能 给方法上面标注@Transactional标识当前方法是一个事务方法 声明式事务实现原理
@EnableTransactionManagement利用TransactionManagementConfigurationSelector给spring容器中导入两个组件：AutoProxyRegistrar和ProxyTransactionManagementConfiguration AutoProxyRegistrar给spring容器中注册一个InfrastructureAdvisorAutoProxyCreator，InfrastructureAdvisorAutoProxyCreator实现了InstantiationAwareBeanPostProcessor,InstantiationAwareBeanPostProcessor是一个BeanPostProcessor。它可以拦截spring的Bean初始化(Initialization)前后和实例化(Initialization)前后。利用后置处理器机制在被拦截的bean创建以后包装该bean并返回一个代理对象代理对象执行方法利用拦截器链进行调用（同springAop的原理） ProxyTransactionManagementConfiguration：是一个spring的配置类,它为spring容器注册了一个BeanFactoryTransactionAttributeSourceAdvisor,是一个事务事务增强器。它有两个重要的字段：AnnotationTransactionAttributeSource和TransactionInterceptor。 AnnotationTransactionAttributeSource：用于解析事务注解的相关信息 TransactionInterceptor：事务拦截器，在事务方法执行时，都会调用TransactionInterceptor的invoke->invokeWithinTransaction方法，这里面通过配置的PlatformTransactionManager控制着事务的提交和回滚。 Spring 扩展(钩子) BeanFactoryPostProcessor：beanFactory后置处理器，的拦截时机：所有Bean的定义信息已经加载到容器，但还没有被实例化。可以对beanFactory进行一些操作。 BeanPostProcessor：bean后置处理器，拦截时机：bean创建对象初始化前后进行拦截工作。可以对每一个Bean进行一些操作。 BeanDefinitionRegistryPostProcessor：是BeanFactoryPostProcessor的子接口，拦截时机：所有Bean的定义信息已经加载到容器，但还没有被实例化，可以对每一个Bean的BeanDefinition进行一些操作。 ApplicationListener,自定义ApplicationListener实现类并加入到容器中,可以监听spring容器中发布的事件。spring在创建容器的时候（finishRefresh（）方法）会发布ContextRefreshedEvent事件，关闭的时候（doClose()）会发布ContextClosedEvent事件。也可以通过spring容器的publishEvent发布自己的事件。 事件发布流程：publishEvent方法 获取事件的多播器，getApplicationEventMulticaster()。 调用multicastEvent(applicationEvent, eventType)派发事件。获取到所有的ApplicationListener,即getApplicationListeners()，然后同步或者异步的方式执行监听器的onApplicationEvent。 事件的多播器的初始化中（initApplicationEventMulticaster（）），如果容器中没有配置applicationEventMulticaster，就使用SimpleApplicationEventMulticaster。然后获取所有的监听器，并把它们注册到SimpleApplicationEventMulticaster中。 @EventListener(class={})：在普通的业务逻辑的方法上监听事件特定的事件。原理：EventListenerMethodProcessor是一个SmartInitializingSingleton，当所有的单例bean都初始化完以后， 容器会回调该接口的方法afterSingletonsInstantiated(),该方法里会遍历容器中所有的bean，并判断每一个bean里是否带有@EventListener注解的Method，然后创建ApplicationListenerMethodAdapter存储并包装该Method，最后将ApplicationListenerMethodAdapter添加到spring容器中。 Spring源代码分析 spring核心逻辑AbstractApplicationContext的refresh()方法如下
public void refresh() { synchronized (this.startupShutdownMonitor) { // 刷新前的预准备工作 prepareRefresh(); // 提取bean的配置信息并封装成BeanDefinition实例，然后将其添加到注册中心。注册中心是一个ConcurrentHashMap&lt;String,BeanDefinition>类型，key为Bean的名字，value为BeanDefinition实例。 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //对beanFactory进行一些配置，注册一些BeanPostProcessor和一些特殊的Bean。 prepareBeanFactory(beanFactory);
//留给子类在BeanFactory准备工作完成后处理一些工作。
postProcessBeanFactory(beanFactory);
//调用 BeanFactory的后置处理器。
invokeBeanFactoryPostProcessors(beanFactory);
//注册Bean的后置处理器。
registerBeanPostProcessors(beanFactory);
//国际化相关功能
initMessageSource();
//初始化事件派发器；
initApplicationEventMulticaster();
// 提供给子容器类，供子容器去实例化其他的特殊的Bean
onRefresh();
// 处理容器中已有的ApplicationListener
registerListeners();
//初始化容器中剩余的单实例bean
finishBeanFactoryInitialization(beanFactory);
//最后一步
finishRefresh();
}
}
prepareRefresh() 记录启动时间，设置容器的active和close状态。 initPropertySources():提供给子容器类，子容器类可覆盖该方法进行一些自定义的属性设置。 getEnvironment().validateRequiredProperties()：检验属性的合法性 this.earlyApplicationEvents = new LinkedHashSet() ：保存容器中的一些早期的事件，待事件多播器创建后执行。 obtainFreshBeanFactory() 提取bean的配置信息并封装成BeanDefinition实例，然后将其添加到注册中心。注册中心是一个ConcurrentHashMap&lt;String,BeanDefinition>类型，key为Bean的名字，value为BeanDefinition实例。
refreshBeanFactory：如果当前容器已经有了BeanFactory就销毁原来的BeanFactory。然后创建一个DefaultListableBeanFactory(); 对BeanFactory并进行配置，主要配置是否允许BeanDefinition覆盖，是否允许Bean间的循环引用。 加载BeanDefinition，解析XML文件和配置文件，将其转换为BeanDefinition，然后保存到DefaultListableBeanFactory的beanDefinitionMap字段中。 getBeanFactory() 简单的返回beanFactory，即DefaultListableBeanFactory。 prepareBeanFactory（） 设置BeanFactory的类加载器、设置支持SPEL表达式的解析器。 添加ApplicationContextAwareProcessor用于处理XXXAware接口的回调。 设置忽略一些接口。并注册一些类，这些类可以在bean里直接进行自动装配。 添加ApplicationListenerDetector用于识别并保存ApplicationListener的子类。 postProcessBeanFactory（）： 提供给子容器类，子容器类可以覆盖该方法在BeanFactory准备工作完成后处理一些工作。
invokeBeanFactoryPostProcessors() 执行BeanFactoryPostProcessor类型的监听方法。
BeanFactoryPostProcessor是beanFactory后置处理器，在整个BeanFactory标准初始化完成后进行拦截调用，
BeanDefinitionRegistryPostProcessor继承了BeanFactoryPostProcessor，在beanFactory解析完所有的BeanDefinition后拦截调用。
BeanFactoryPostProcessor来源
通过ApplicationContent的addBeanFactoryPostProcessor()方法手动添加自己的拦截器 系统默认了一些BeanFactoryPostProcessor。例如：ConfigurationClassPostProcessor用来处理@Configuration标注的Spring配置类。 调用顺序
先调用BeanDefinitionRegistryPostProcessor类型的拦截器， 然后再依次调用实现了PriorityOrdered,Ordered接口的BeanFactoryPostProcessor 最后调用普通的BeanFactoryPostProcessor BeanFactoryPostProcessor是beanFactory后置处理器，在整个BeanFactory标准初始化完成后进行拦截调用，
BeanDefinitionRegistryPostProcessor继承了BeanFactoryPostProcessor，在beanFactory解析完所有的BeanDefinition后拦截调用。
BeanFactoryPostProcessor来源
通过ApplicationContent的addBeanFactoryPostProcessor()方法手动添加自己的拦截器 系统默认了一些BeanFactoryPostProcessor。例如：ConfigurationClassPostProcessor用来处理@Configuration标注的Spring配置类。 调用顺序
先调用BeanDefinitionRegistryPostProcessor类型的拦截器， 然后再依次调用实现了PriorityOrdered,Ordered接口的BeanFactoryPostProcessor 最后调用普通的BeanFactoryPostProcessor registerBeanPostProcessors() 注册Bean的后置处理器。
从beanFactory里获取所有BeanPostProcessor类型的Bean的名称。
调用beanFactory的getBean方法并传入每一个BeanPostProcesso类型的Bean名称，从容器中获取该Bean的实例。
第一步向beanFactory注册实现了PriorityOrdered的BeanPostProcessor类型的Bean实例。 第二步向beanFactory注册实现了Ordered的BeanPostProcessor类型的Bean实例。 第三步向beanFactory注册普通的BeanPostProcessor类型的Bean实例。 最后一步向beanFactory重新注册实现了MergedBeanDefinitionPostProcessor的BeanPostProcessor类型的Bean实例 向beanFactory注册BeanPostProcessor的过程就是简单的将实例保存到beanFactory的beanPostProcessors属性中。
initMessageSource() 国际化相关功能
看容器中是否有id为messageSource的，类型是MessageSource的Bean实例。如果有赋值给messageSource，如果没有自己创建一个DelegatingMessageSource。 把创建好的MessageSource注册在容器中，以后获取国际化配置文件的值的时候，可以自动注入MessageSource。 initApplicationEventMulticaster() 初始化事件派发器；
看容中是否有名称为applicationEventMulticaster的，类型是ApplicationEventMulticaster的Bean实例。如果没有就创建一个SimpleApplicationEventMulticaster。 把创建好的ApplicationEventMulticaster添加到BeanFactory中。 onRefresh()： 提供给子容器类，供子容器去实例化其他的特殊的Bean。
registerListeners()： 处理容器中已有的ApplicationListener。
1. 从容器中获得所有的ApplicationListener 2. 将每个监听器添加到事件派发器（ApplicationEventMulticaster）中； 3. 处理之前步骤产生的事件； finishBeanFactoryInitialization()： 初始化容器中剩余的单实例bean：拿到剩余的所有的BeanDefinition，依次调用getBean方法（详看beanFactory.getBean的执行流程）
finishRefresh()： 最后一步。
1. 初始化和生命周期有关的后置处理器；LifecycleProcessor，如果容器中没有指定处理就创建一个DefaultLifecycleProcessor加入到容器。 2. 获取容器中所有的LifecycleProcessor回调onRefresh()方法。 3. 发布容器刷新完成事件ContextRefreshedEvent。 ConfigurationClassPostProcessor处理@Configuration的过程： 先从主从中心取出所有的BeanDefinition。依次判断，若一个BeanDefinition是被@Configuration标注的，spring将其标记为FullMode，否则若一个BeanDefinition没有被@Configuration标注，但有被@Bean标注的方法，spring将其标记为LightMode。筛选出所有候选配置BeanDefinition（FullMode和LightMode） 创建一个ConfigurationClassParser，调用parse方法解析每一个配置类。 解析@PropertySources,将解析结果设置到Environment 利用ComponentScanAnnotationParser，将@ComponentScans标签解析成BeanDefinitionHolder。再迭代解析BeanDefinitionHolder 解析@Import，@ImportResource 将@Bean解析为MethodMetadata，将结果保存到ConfigurationClass中。最终ConfigurationClass会被保存到ConfigurationClassParser的configurationClasses中。 调用ConfigurationClassParser的loadBeanDefinitions方法，加载解析结果到注册中。 从利用ComponentScanAnnotationParser的configurationClasses获取所有的ConfigurationClass，依次调用loadBeanDefinitionsForConfigurationClass方法。 loadBeanDefinitionsForConfigurationClass会将每一个BeanMethod转为ConfigurationClassBeanDefinition，最后将其添加到spring的注册中心。 beanFactory.getBean方法执行的过程 首先将方法传入的beanName进行转换：先去除FactoryBean前缀（&amp;符）如果传递的beanName是别名，则通过别名找到bean的原始名称。 根据名称先从singletonObjects（一个Map类型的容）获取bean实例。如果能获取到就先判断该bean实例是否实现了FactoryBean，如果是FactoryBean类型的bean实例，就通过FactoryBean获取Bean。然后直接返回该bean实例。getBean方法结束。 如果从singletonObjects没有获取到bean实例就开始创建Bean的过程。 首先标记该Bean处于创建状态。 根据Bean的名称找到BeanDefinition。查看该Bean是否有前置依赖的Bean。若有则先创建该Bean前置依赖的Bean。 spring调用AbstractAutowireCapableBeanFactory的createBean方法并传入BeanDefinition开始创建对象。先调用resolveBeforeInstantiation给BeanPostProcessor一个机会去返回一个代理对象去替代目标Bean的实例。 如果BeanPostProcessor没有返回Bean的代理就通过doCreateBean方法创建对象。 首先确定Bean的构造函数，如果有有参构造器，先自动装配有参构造器，默认使用无参数构造器。 选择一个实例化策略去实例化bean。默认使用CglibSubclassingInstantiationStrategy。该策略模式中,首先判断bean是否有方法被覆盖,如果没有则直接通过反射的方式来创建,如果有的话则通过CGLIB来实例化bean对象. 把创建好的bean对象包裹在BeanWrapper里。 调用MergedBeanDefinitionPostProcessor的postProcessMergedBeanDefinition 判断容器是否允许循环依赖，如果允许循环依赖，就创建一个ObjectFactory类并实现ObjectFactory接口的唯一的一个方法getObject（）用于返回Bean。然后将该ObjectFactory添加到singletonFactories中。 调用populateBean为bean实例赋值。在赋值之前执行InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation和postProcessPropertyValues方法。 调用initializeBean初始化bean。如果Bean实现了XXXAware，就先处理对应的Aware方法。然后调用beanProcessor的postProcessBeforeInitialization方法。再以反射的方式调用指定的bean指定的init方法。最后调用beanProcessor的postProcessAfterInitialization方法。 调用registerDisposableBeanIfNecessary，将该bean保存在一个以beanName为key，以包装了bean引用的DisposableBeanAdapter，为value的map中，在spring容器关闭时，遍历这个map来获取需要调用bean来依次调用Bean的destroyMethod指定的方法。 将新创建出来的Bean保存到singletonObjects中 spring原理补充 spring解决循环依赖 以类A，B互相依赖注入为例
根据类A的名称先从singletonObjects获取Bean实例，发现获取不到，就通过doGetBean方法开始创建Bean的流程。 根据A的名称找到对应的BeanDefinition，通过doCreateBean（）方法创建对象，先确定类A的构造函数，然后选择一个实例化策略去实例化类A。 判断容器是否允许循环依赖，如果允许循环依赖，就创建一个ObjectFactory类并实现ObjectFactory接口的唯一的一个方法getObject（）用于返回类A。然后将该ObjectFactory添加到singletonFactories中。 调用populateBean（）为类A进行属性赋值，发现需要依赖类B，此时类B尚未创建，启动创建类B的流程。 根据类B的名称先从singletonObjects获取Bean实例，发现获取不到，就开始通过doGetBean方法开始创建Bean的流程 找到类B对应的BeanDefinition，确认B的构造函数，然后实例化B。 判断容器是否允许循环依赖，创建一个ObjectFactory并实现getObject（）方法，用于返回类B，并添加到singletonFactories中。 调用populateBean（）为类B进行属性赋值，发现需要依赖类A，调用getSingleton方法获取A：A现在已存在于singletonFactories中，getSingleton将A从singletonFactories方法中移除并放入earlySingletonObjects中。 调用getSingleton（）方法获取B：getSingleton将A从singletonFactories方法中移除并放入earlySingletonObjects中。 调用initializeBean初始化bean，最后将新创建出来的类B保存到singletonObjects中 调用getSingleton（）方法获取A，这时A已在earlySingletonObjects中了，就直接返回A 调用initializeBean初始化bean，最后将新创建出来的类B保存到singletonObjects中。 @Autowire 实现原理 上面介绍beanFactory.getBean方法执行的过程中提到：populateBean为bean实例赋值。在赋值之前执行InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation和postProcessPropertyValues方法。@Autowire由AutowiredAnnotationBeanPostProcessor完成，它实现了InstantiationAwareBeanPostProcessor。 AutowiredAnnotationBeanPostProcessor执行过程：
postProcessAfterInstantiation方法执行，直接return null。 postProcessPropertyValues方法执行，主要逻辑在此处理。待补充。。。。。</content></entry><entry><title>软件架构设计读书笔记</title><url>https://codingroam.github.io/post/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url><categories><category>软件架构</category><category>计算机原理</category></categories><tags><tag>软件架构</tag><tag>计算机原理</tag></tags><content type="html"> 根据软件架构设计：大型网站技术架构与业务架构融合之道一书所作的读书笔记
1. 架构分层 第一层：基础架构
指云平台、操作系统、网络、存储、数据库等
第二层：中间件与大数据平台
中间件：如分布式服务中间件，消息中间件、数据库中间件、缓存中间件、监控中间件、工作流或者规则引擎等 大数据架构：Hadoop生态体系，hive,Spark,Storm,Flink等 第三层：业务系统架构
通用软件系统，常用的办公软件、浏览器、播放器等 离线业务系统，如各种基于大数据的BI分析，数据挖掘，报表与可视化。 大型在线业务系统，如搜索，推荐，即时通信，电商，游戏，广告，企业ERP等 架构的道与术：解决一系列问题的方法论即为道，具体到某种语言技术，某个中间件的使用即为术。道是知行合一中的知，是理论，套路，方法论。术是行，是实践操作，用于解决一个个实际问题
2. 计算机功底 WAL即 Write Ahead Log，WAL的主要意思是说在将元数据的变更操作写入磁盘之前，先预先写入到一个log文件中，增加IO速度。磁盘顺序读写速度可媲美内存速度。 Checksum：总和检验码，校验和。在数据处理和数据通信领域中，用于校验目的的一组数据项的和。这些数据项可以是数字或在计算检验总和过程中看作数字的其它字符串，可以保证数据的完整性和正确性 TCP如何把不可靠变成可靠，mvcc解决并发多版本一致性等 计算机思维举例，可以借鉴
3. 操作系统 缓冲IO与直接IO
缓冲IO是C语言提供的函数库，以f打头，如fopen,fclose,fseek,fread,fwrite，直接IO是Linux系统的API,如open,read，write等
对于缓冲IO,读写都是三次拷贝
读：磁盘→内核缓冲区→用户缓冲区→应用程序内存
写：应用程序内存→用户缓冲区→内核缓冲区→磁盘
对于直接IO,读写都是两次数据拷贝
读：磁盘→内核缓冲区→应用程序内存
写：应用程序内存→内核缓冲区→磁盘
所以，直接IO没有用户缓冲区
应用层序内存：通常是代码用malloc/free、new/delete等分配出来的内存
用户缓冲区：C语言的FILE结构体中的buffer
内核缓冲区：Linux操作系统的Page Cahce。为了加快磁盘IO，Linux会把磁盘上的数据以page为单位加载到内存中，page是一个逻辑概念，一般一个page为4K
缓冲IO与直接IO有几点需要贴别说明：
fflush和fsync的区别。fflush是缓冲IO的一个api，作用是把数据从用户缓存刷到内核缓存，fsync是把数据从内核缓存刷到磁盘，所以无论是直接IO还是缓冲IO在写数据之后不调用fsync,此时断电的话会造成数据丢失。 对于直接IO也有read/write和pread/pwrite两组api，后者在多线程读写同一个文件时效率更高 内存映射与零拷贝
相较于直接IO,内存映射文件更进一步，其实现是用应用程序的逻辑内存与Linux操作系统的内核缓存区映射，相当于与内核共用内存，数据拷贝只需一次即
磁盘$\Leftrightarrow$内核缓冲区</content></entry><entry><title>设计模式七原则</title><url>https://codingroam.github.io/post/java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url><categories><category>设计模式</category></categories><tags><tag>设计模式</tag></tags><content type="html"> 设计模式学习
1 七大原则 单一职责原则：一个类只负责一个功能领域中的相应职责，或者可以定义为：就一个类而言，应该只有一个引起它变化的原因
开闭原则：一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。 为了满足开闭原则，需要对系统进行抽象化设计，抽象化是开闭原则的关键，可以为系统定义一个相对稳定的抽象层，而将不同的实现行为移至具体的实现层中完成。
里氏代换原则：所有引用基类（父类）的地方必须能透明地使用其子类的对象。里氏代换原则是实现开闭原则的重要方式之一，由于使用基类对象的地方都可以使用子类对象，因此在程序中尽量使用基类类型来对对象进行定义，而在运行时再确定其子类类型，用子类对象来替换父类对象
在使用里氏代换原则时需要注意如下几个问题：
子类的所有方法必须在父类中声明，或子类必须实现父类中声明的所有方法。根据里氏代换原 则，为了保证系统的扩展性，在程序中通常使用父类来进行定义，如果一个方法只存在子类中，在父类中不提供相应的声明，则无法在以父类定义的对象中使用该方法。 我们在运用里氏代换原则时，尽量把父类设计为抽象类或者接口，让子类继承父类或实现父接口，并实现在父类中声明的方法，运行时，子类实例替换父类实例，我们可以很方便地扩展系统的功能，同时无须修改原有子类的代码，增加新的功能可以通过增加一个新的子类来实现。里氏代换原则是开闭原则的具体实现手段之一。 依赖倒转原则：抽象不应该依赖于细节，细节应当 依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。依赖倒转原则要求我们在程序代码中传递参数时或在关联关系中，尽量引用层次高的抽象层类，即使
用接口和抽象类进行变量类型声明、参数类型声明、方法返回类型声明，以及数据类型的转换等，而不要用具体类来做这些事情。
在实现依赖倒转原则时，我们需要针对抽象层编程，而将具体类的对象通过依赖注入(DependencyInjection, DI)的方式注入到其他对象中，依赖注入是指当一个对象要与其他对象 发生依赖关系时，通过抽象来注入所依赖的对象。常用的注入方式有三种，分别是：构造注入，设值注入（Setter注入）和接口注入
接口隔离原则：使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。
合成复用原则：尽量使用对象组合，而不是继承来达到复用的目的。合成复用原则就是在一个新的对象里通过关联关系（包括组合关系和聚合关系）来使用一些已有的对象，使之成为新对象的一部分；新对象通过委派调用已有对象的方法达到复用功能的目的。
两个类之间是“Has-A”的关系应使用组合或聚合，如果是“Is-A”关系可使用继 承。”Is-A”是严格的分类学意义上的定义，意思是一个类是另一个类的”一种”；而”Has-A”则不同，它表示某一个角色具有某一项责任。
迪米特法则：一个软件实体应当尽可能少地与其他实体发生相互作用（最少知道原则）。
不要和“陌生人”说话、只与你的直接朋友通信等，在迪米特法 则中，对于一个对象，其朋友包括以下几类：
当前对象本身(this)；
以参数形式传入到当前对象方法中的对象；
当前对象的成员对象；
如果当前对象的成员对象是一个集合，那么集合中的元素也都是朋友；
当前对象所创建的对象。
任何一个对象，如果满足上面的条件之一，就是当前对象的“朋友”，否则就是“陌生人”。
迪米特法则要求我们在设计系统时，应该尽量减少对象之间的交互，如果两个对象之间不必彼此直接 通信，那么这两个对象就不应该发生任何直接的相互作用，如果其中的一个对象需要调用另一个对象 的某一个方法的话，可以通过第三者转发这个调用。简言之，就是通过引入一个合理的第三者来降低 现有对象之间的耦合度。</content></entry><entry><title>MySql原理学习记录</title><url>https://codingroam.github.io/post/mysql%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>Learning</tag></tags><content type="html"> 根据《MySql是怎样运行的一书》整理记录知识点
1 mysql编码 mysql支持41种字符集包括常用的utf-8、ASCII、GB2312等
utf是unicode编码的一种实现方式，utf8使用1～4个字节编码一个字符，utf16使用2个或4个字节编码一个字符，utf32使用4个字节编码一个字符 mysql中的utf8(utf8mb3)是标准utf8的阉割版，采用1~3个字节存储字符，涵盖常用字符的表示范围 mysql中utf8mb4 才是正宗的 utf8 字符集，使用1～4个字节表示字符，可以表示不常用字符，如表情符号等 SHOW CHARACTER SET;查看字符集 比较规则，比较规则和字符集是绑定的
每种字符集都有几种对应的比较规则，如忽略大小写等，比较规则用于比较字符或者排序（order by）
SHOW COLLATION [LIKE 匹配的模式]; SHOW COLLATION LIKE &lsquo;utf8_%&rsquo;;查看utf8编码的比较规则 字符集和比较规则的应用
MySQL 有4个级别的字符集和比较规则，数据字符集采用就近原则，如果没有就从上级继承，从上到下级别分别是：①服务器级别②数据库③表级别④列级别
以服务器级别为例：
SHOW VARIABLES in(&lsquo;character_set_server&rsquo;,&lsquo;collation_server&rsquo;);查看服务器级别的字符集和比较规则
数据库乱码原因及原理
结论：编码和解码使用的字符集不一致的后果
mysql字符编系统变量
过程：客户端发往服务器的请求本质上就是一个字符串，服务器向客户端返回的结果本质上也是一个字符。过程分为三步：①客户端发送往服务器的字符串经过编码成二进制数据才能发送，比如客户端的默认编码是gbk，发送的数据就是以gbk字符集编码的二进制数据，数据到了mysql端，mysql就会使用character_set_client字符集来对数据进行解码，如果字符集不是gbk这一步就会乱码，导致sql语句无法正确解析②第一步成功编解码之后，服务器会把字符按照character_set_connection字符集进行编码，然后拿着编码后的数据去数据库表中对应列进行匹配（数据库中字段值是按照character_set_connection字符集编码后存储的）③最后将查出来的数据用character_set_results编码之后发给客户端，客户端再用客户端默认字符集解码得到最终展示数据
总结：服务器认为客户端发送过来的请求是用 character_set_client 编码的。假设你的客户端采用的字符集和 character_set_client 不一样的话，这就会出现意想不到的情况。比如我的客户端使用的是 utf8 字符集，如果把系统变量 character_set_client 的值设置为 ascii 的话，服务器可能无法理解我们发送的请求，更别谈处理这个请求了。服务器将把得到的结果集使用 character_set_results 编码后发送给客户端。假设你的客户端采用的字符集和 character_set_results 不一样的话，这就可能会出现客户端无法解码结果集的情况，结果就是在你的屏幕上出现乱码。客户端默认字符集==character_set_client==character_set_results才不会乱码
设置成统一字符集
SET NAMES 字符集名;
等价于：
SET character_set_client = 字符集名;
SET character_set_connection = 字符集名;
SET character_set_results = 字符集名;
等价于：
在my.cnf配置文件中
[client]
default-character-set=字符集名
2 存储引擎InnoDB 我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为 行格式 或者 记录格式 设计 InnoDB 存储引擎的大叔们到现在为止设计了4种不同类型的 行格式 ，分别是 Compact 、 Redundant Dynamic 和 Compressed 行格式
行格式用法：
CREATE TABLE 表名 (列的信息) ROW_FORMAT=Compact
ALTER TABLE 表名 ROW_FORMAT=Compact
compact行格式
变长字段长度列表
变长字段，使用变长数据类型（varchar(n)，blob,text等）或者变长字符集（如utf8是1~3个字符）存储的字段。在 Compact 行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表，各变长字段数据占用的字节数按照列的顺序逆序存放，
NULL值列表
我们知道表中的某些列可能存储 NULL 值，如果把这些 NULL 值都放到 记录的真实数据 中存储会很占地方，所以 Compact 行格式把这些值为 NULL 的列统一管理起来，存储到 NULL 值列表中，它的处理过程是这样的：
首先统计表中允许存储 NULL 的列有哪些。我们前边说过，主键列、被 NOT NULL 修饰的列都是不可以存储 NULL 值的，所以在统计的时候不会把这些列算进去。比方说表 record_format_demo 的3个列 c1 、 c3 、 c4 都是允许存储 NULL 值的，而 c2 列是被NOT NULL 修饰，不允许存储 NULL 值。
如果表中没有允许存储 NULL 的列，则 NULL值列表 也不存在了，否则将每个允许存储 NULL 的列对应一个二进制位，二进制位按照列的顺序逆序排列，二进制位表示的意义如下：二进制位的值为 1 时，代表该列的值为 NULL 。二进制位的值为 0 时，代表该列的值不为 NULL
MySQL 规定 NULL值列表 必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补 0 。 表 record_format_demo 只有3个值允许为 NULL 的列，对应3个二进制位，不足一个字节，所以在字节的高位补 0
记录头信息
除了 变长字段长度列表 、 NULL值列表 之外，还有一个用于描述记录的 记录头信息 ，它是由固定的 5 个字节组成。 5 个字节也就是 40 个二进制位，不同的位代表不同的意思
存储行
/*MySQL 对一条记录占用的最大存储空间是有限制的，除了 BLOB 或者 TEXT 类型的列之 外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节 存储一个 VARCHAR(M) 类型的列，其实需要占用3部分存储空间： ①真实数据 ②真实数据占用字节的长度 ③NULL 值标识，如果该列有 NOT NULL 属性则可以没有这部分存储空间 如果该 VARCHAR 类型的列没有 NOT NULL 属性，那最多只能存储 65532 个字节的数据，因为真实数据的长度可能 占用2个字节， NULL 值标识需要占用1个字节： */ CREATE TABLE varchar_size_demo( c1 VARCHAR(65532) )CHARSET=ASCII ROW_FORMAT=COMPACT; /*编码如果是gbk,最多只能32766 （也就是：65532/2），因为gbk一个字符占两个字节 utf8 字符集表示一个字符最多需要 3 个字节，那在该字符集下， M 的最大取值就是 21844 ，就是说最多能存 储 21844 （也就是：65532/3）个字符*/ /*这些都是表中只有一个字段的情况下，如果是多个字段，则所有字段（（不包括隐藏 列和记录头信息））加起来不能超过65535*/ CREATE TABLE varchar_size_demo( c VARCHAR(32766) ) CHARSET=gbk ROW_FORMAT=COMPACT; 3 访问方法 const: 通过主键或者唯一二级索引列与常数的等值比较来定位一条记录,如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较
ref：通过某个普通的二级索引列与常数进行等值比较（普通二级索引与唯一二级索引对应，普通二级索引等值比较可能查出多个列）
二级索引列值为 NULL 的情况
不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含 NULL 值的数量并不限制，所以我们采用key IS NULL 这种形式的搜索条件最多只能使用 ref 的访问方法，而不是 const 的访问方法。
对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用 ref的访问方法，比方说下边这几个查询：
SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo;;
SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo; AND key_part2 = &rsquo;legendary&rsquo;;
SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo; AND key_part2 = &rsquo;legendary&rsquo;
AND key_part3 = &lsquo;penta kill&rsquo;。但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为 ref 了，比方说这样： SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo; AND key_part2 > &rsquo;legendary&rsquo;;
ref_or_null：有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为 NULL 的记录也找出来，就像下边这个查询：
SELECT * FROM single_demo WHERE key1 = &lsquo;abc&rsquo; OR key1 IS NULL;
range：利用索引进行范围匹配
index：遍历二级索引记录的执行方式
SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &lsquo;abc&rsquo;;
key_part1, key_part2, key_part3三个要查询的字段是联合索引，查询条件key_part2 不是最左索引，此时需要直接通过遍历 idx_key_part 索引的叶子节点的记录来比较 key_part2 = &lsquo;abc&rsquo; 这个条件是否成立，把匹配成功的二级索引记录的 key_part1 , key_part2 , key_part3 列的值直接加到结果集中就行了，不需要回表
当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是 index ，比如这样：
mysql> EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = &lsquo;a&rsquo;;
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;-
&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| id | select_type | table | partitions | type | possible_keys | key | key
_len | ref | rows | filtered | Extra |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;-
&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| 1 | SIMPLE | s1 | NULL | index | NULL | idx_key_part | 909
| NULL | 9688 | 10.00 | Using where; Using index |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;-
&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
1 row in set, 1 warning (0.00 sec)
上述查询中的搜索列表中只有 key_part2 一个列，而且搜索条件中也只有 key_part3 一个列，这两个列又恰
好包含在 idx_key_part 这个索引中，可是搜索条件 key_part3 不能直接使用该索引进行 ref 或者 range 方
式的访问，只能扫描整个 idx_key_part 索引的记录，所以查询计划的 type 列的值就是 index 。
小贴士：
再一次强调，对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，
而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，
也就是扫描聚簇索引的代价更低一些。
all：全表扫描
eq_ref：在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是
eq_ref
index_merge：一般情况下对于某个表的查询只能使用到一个索引，在某些场景下可以使用 Intersection 、 Union 、 Sort-Union 这三种索引合并的方式来执行查询，如
mysql> EXPLAIN SELECT * FROM s1 WHERE key1 = &lsquo;a&rsquo; OR key3 = &lsquo;a&rsquo;;
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;
&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
&mdash;&ndash;+
| id | select_type | table | partitions | type | possible_keys | key
| key_len | ref | rows | filtered | Extra |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;
&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
&mdash;&ndash;+
| 1 | SIMPLE | s1 | NULL | index_merge | idx_key1,idx_key3 | idx_key
1,idx_key3 | 303,303 | NULL | 14 | 100.00 | Using union(idx_key1,idx_key3); Using
where |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;
&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
&mdash;&ndash;+
1 row in set, 1 warning (0.01 sec)
unique_subquery：类似于两表连接中被驱动表的 eq_ref 访问方法， unique_subquery 是针对在一些包含 IN 子查询的查询语句中，如果查询优化器决定将 IN 子查询转换为 EXISTS 子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的 type 列的值就是 unique_subquery ，比如下边的这个查询语句：
mysql> EXPLAIN SELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 = s2.
key1) OR key3 = &lsquo;a&rsquo;;
+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;
+&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;-+
| id | select_type | table | partitions | type | possible_keys
| key | key_len | ref | rows | filtered | Extra |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;
+&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;-+
| 1 | PRIMARY | s1 | NULL | ALL | idx_key3
| NULL | NULL | NULL | 9688 | 100.00 | Using where |
| 2 | DEPENDENT SUBQUERY | s2 | NULL | unique_subquery | PRIMARY,idx_key1
| PRIMARY | 4 | func | 1 | 10.00 | Using where |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;
+&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;-+
system：当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是 system
4 redo log 概念：在系统奔溃重启时需要按照上述内容所记录的步骤重新更新数据页，所以上述内容也被称之为 重做日志 ，英文名为 redo log 。
描述：想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。所以我们其实没有必要在每次事务提交时就把该事务在内存中修改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好，这样我们在事务提交时，把上述内容刷新到磁盘中，即使之后系统崩溃了，重启之后只要按照上述内容所记录的步骤重新更新一下数据页，那么该事务对数据库中所做的修改又可以被恢复出来。
结构：
type ：该条 redo 日志的类型。
在 MySQL 5.7.21 这个版本中，设计 InnoDB 的大叔一共为 redo 日志设计了53种不同的类型，稍后会详细介
绍不同类型的 redo 日志。
space ID ：表空间ID。
page number ：页号。
data ：该条 redo 日志的具体内容。
刷盘时机：redo日志不会直接写入磁盘，而是写入内存的log buffer中，将log buffer刷入磁盘的时机有以下几种情况：①log buffer空间不足②事务提交③后台线程大概每秒会做一次刷盘④正常关闭服务器时⑤做checkpoint时
Mini-Transaction：底层页面中的一次原子访问的过程称之为一个 Mini-Transaction ，简称 mtr ，比如上边
所说的修改一次 Max Row ID 的值算是一个 Mini-Transaction ，向某个索引对应的 B+ 树中插入一条记录的过程也算是一个 Mini-Transaction 。通过上边的叙述我们也知道，一个所谓的 mtr 可以包含一组 redo 日志，在进行奔溃恢复时这一组 redo 日志作为一个不可分割的整体。一个事务可以包含若干条语句，每一条语句其实是由若干个 mtr 组成，每一个 mtr 又可以包含若干条 redo 日志
lsn和flushed_to_disk_lsn：日志序列号 ，简称 lsn有新的 redo 日志写入到 log buffer 时，首先 lsn 的值会增长。刷新到磁盘中的 redo 日志量的全局变量，称之为flushed_to_disk_lsn 。系统第一次启动时，该变量的值和初始的 lsn 值是相同的，都是 8704 。随着系统的运行， redo 日志被不断写入 log buffer ，但是并不会立即刷新到磁盘， lsn 的值就和 flushed_to_disk_lsn 的值拉开了差距，如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了
checkpoint：全局变量 checkpoint_lsn 来代表当前系统中可以被覆盖的 redo 日志总量是多少，比方说现在 页a 被刷新到了磁盘， mtr_1 生成的 redo 日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn 的操作，我们把这个过程称之为做一次 checkpoint 。计算一下当前系统中可以被覆盖的 redo 日志对应的 lsn 值最大是多少，redo 日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的 oldest_modification 值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的 oldest_modification 赋值给 checkpoint_lsn
崩溃恢复：系统崩溃后重启时根据 redo 日志中的记录就可以将页面恢复到系统奔溃前的状态。
崩溃恢复起点：checkpoint_lsn 之前的 redo 日志都可以被覆盖，也就是说这些 redo 日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于 checkpoint_lsn 之后的 redo 日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从 checkpoint_lsn 开始读取 redo 日志来恢复页面。 崩溃恢复的终点：写 redo 日志的时候都是顺序写的，写满了一个block之后会再往下一个block，普通block的 log block header 部分有一个称之为 LOG_BLOCK_HDR_DATA_LEN 的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为 512 。如果该属性的值不为 512，那该block就是最后一个block，也就是redo日志的终点。 怎么恢复：按照 redo 日志的顺序依次扫描checkpoint_lsn 之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来， 5 undo log 定义：在事务中为了回滚而记录的这些东东称之为撤销日志，英文名为 undo log 6 隔离级别 事务并发遇到的问题：
脏写（ Dirty Write ）：如果一个事务修改了另一个未提交事务修改过的数据，那就意味着发生了 脏写 脏读（ Dirty Read ）：如果一个事务读到了另一个未提交事务修改过的数据，那就意味着发生了 脏读 不可重复读（Non-Repeatable Read）：如果一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值，那就意味着发生了 不可重复读 幻读（Phantom）：如果一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来，那就意味着发生了 幻读 四种隔离级别：
READ UNCOMMITTED ：未提交读。
READ COMMITTED ：已提交读。
REPEATABLE READ ：可重复读。
SERIALIZABLE ：可串行化。
READ UNCOMMITTED 隔离级别下，可能发生 脏读 、 不可重复读 和 幻读 问题。
READ COMMITTED 隔离级别下，可能发生 不可重复读 和 幻读 问题，但是不可以发生 脏读 问题。
REPEATABLE READ 隔离级别下，可能发生 幻读 问题，但是不可以发生 脏读 和 不可重复读 的问题。
SERIALIZABLE 隔离级别下，各种问题都不可以发生。
MySQL在REPEATABLE READ隔离级别下，是可以禁止幻读问题的发生的
MVCC原理
版本链描述：每次对记录进行改动，都会记录一条 undo日志 ，每条 undo日志 也都有一个 roll_pointer 属性（ INSERT 操作对应的 undo日志 没有该属性，因为该记录并没有更早的版本），可以将这些 undo日志 都连来，串成一个链表，对该记录每次更新后，都会将旧值放到一条 undo日志 中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，我们把这个链表称之为 版本链 ，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的 事务id
ReadView：对于使用 READ UNCOMMITTED 隔离级别的事务来说，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了；对于使用 SERIALIZABLE 隔离级别的事务来说，设计 InnoDB 的大叔规定使用加锁的方式来访问记录（加锁是啥我们后续文章中说哈）；对于使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是：需要判断一下版本链中的哪个版本是当前事务可见的。为此，设计 InnoDB 的大叔提出了一个 ReadView 的概念，这个 ReadView 中主要包含4个比较重要的内容：
m_ids ：表示在生成 ReadView 时当前系统中活跃的读写事务的 事务id 列表。
min_trx_id ：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的 事务id ，也就是 m_ids 中的最
小值。
max_trx_id ：表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。
小贴士：
注意max_trx_id并不是m_ids中的最大值，事务id是递增分配的。比方说现在有id为1，2，3这三
个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，m_ids就包括1和2，mi
n_trx_id的值就是1，max_trx_id的值就是4。
creator_trx_id ：表示生成该 ReadView 的事务的 事务id 。
有了这个 ReadView ，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：
如果被访问版本的 trx_id 属性值与 ReadView 中的 creator_trx_id 值相同，意味着当前事务在访问它自己
修改过的记录，所以该版本可以被当前事务访问。如果被访问版本的 trx_id 属性值小于 ReadView 中的 min_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。
如果被访问版本的 trx_id 属性值大于 ReadView 中的 max_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。
如果被访问版本的 trx_id 属性值在 ReadView 的 min_trx_id 和 max_trx_id 之间，那就需要判断一下
trx_id 属性值是不是在 m_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该
版本不可以被访问；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。
**在 MySQL 中， READ COMMITTED 和 REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同，READ COMMITTED —— 每次读取数据前都生成一个****ReadView，REPEATABLE READ —— 在第一次读取数据时生成一个****ReadView
小结：从上边的描述中我们可以看出来，所谓的 MVCC （Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行普通的 SEELCT 操作时访问记录的版本链的过程，这样子可以使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。 READ COMMITTD 、REPEATABLE READ 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了。
7 锁 结构属性：
trx信息 ：代表这个锁结构是哪个事务生成的。
is_waiting ：代表当前事务是否在等待。
type：Next-Key Locks/插入意向锁/&hellip;
加解锁过程：
事务 T1 要改动某条记录时，就生成了一个 锁结构 与该记录关联，因为之前没有别的事务为这条记录加锁，所以 is_waiting 属性就是 false ，我们把这个场景就称之为获取锁成功，或者加锁成功，然后就可以继续执行操作了。
在事务 T1 提交之前，另一个事务 T2 也想对该记录做改动，那么先去看看有没有 锁结构 与这条记录关联，发现有一个 锁结构 与之关联后，然后也生成了一个 锁结构 与这条记录关联，不过 锁结构 的is_waiting 属性值为 true ，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，或者没有成功的获取到锁
在事务 T1 提交之后，就会把该事务生成的 锁结构 释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务 T2 还在等待获取锁，所以把事务 T2 对应的锁结构的 is_waiting 属性设置为 false ，然后把该事务对应的线程唤醒，让它继续执行，此时事务 T2 就算获取到锁了
读操作利用多版本并发控制（ MVCC ），写操作进行加锁，除非有特殊场景需要读写都加锁，比如一些业务场景不允许读取记录的旧版本，而是每次都必须去读取记录的最新版本，比方在银行存款的事务中，你需要先把账户的余额读出来，然后将其加上本次存款的数额，最后再写到数据库中。在将账户余额读取出来后，就不想让别的事务再访问该余额，直到本次存款事务执行完成，其他事务才可以访问账户的余额。这样在读取记录的时候也就需要对其进行 加锁 操作，这样也就意味着 读操作和 写 操作也像 写-写 操作那样排队执行
一致性读和锁定读
事务利用 MVCC 进行的读取操作称之为 一致性读 ，或者 一致性无锁读 ，有的地方也称之为 快照读 。所有普通的 SELECT 语句（ plain SELECT ）在 READ COMMITTED 、 REPEATABLE READ 隔离级别下都算是 一致性读，一致性读 并不会对表中的任何记录做 加锁 操作，其他事务可以自由的对表中的记录做改动。
锁定读要求在读-读 情况不受影响，又要使 写-写 、 读-写 或 写-读 情况中的操作相互阻塞，mysql中用两种锁独占锁共享锁和独占锁来实现：
共享锁 ，英文名： Shared Locks ，简称 S锁 。在事务要读取一条记录时，需要先获取该记录的 S锁 独占锁 ，也常称 排他锁 ，英文名： Exclusive Locks ，简称 X锁 。在事务要改动一条记录时，需要先获取该记录的 X锁 假如事务 T1 首先获取了一条记录的 S锁 之后，事务 T2 接着也要访问这条记录：如果事务 T2 想要再获取一个记录的 S锁 ，那么事务 T2 也会获得该锁，也就意味着事务 T1 和 T2 在该记录上同时持有 S锁 。如果事务 T2 想要再获取一个记录的 X锁 ，那么此操作会被阻塞，直到事务 T1 提交之后将 S锁 释放掉。如果事务 T1 首先获取了一条记录的 X锁 之后，那么不管事务 T2 接着想获取该记录的 S锁 还是 X锁 都会被阻塞，直到事务 T1 提交。 总结：S锁 和 S锁 是兼容的， S锁 和 X锁 是不兼容的， X锁 和 X锁 也是不兼容的 加锁语句：S锁 SELECT &hellip; LOCK IN SHARE MODE; X锁 SELECT &hellip; FOR UPDATE; 写操作 DELETE ：对一条记录做 DELETE 操作的过程其实是先在 B+ 树中定位到这条记录的位置，然后获取一下这条记录的 X 锁 ，然后再执行 delete mark 操作。我们也可以把这个定位待删除记录在 B+ 树中位置的过程看成是一个获取 X锁 的 锁定读 。
UPDATE ：在对一条记录做 UPDATE 操作时分为三种情况：如果未修改该记录的键值并且被更新的列占用的存储空间在修改前后未发生变化，则先在 B+ 树中定位到这条记录的位置，然后再获取一下记录的 X锁 ，最后在原记录的位置进行修改操作。其实我们也可以把这个定位待修改记录在 B+ 树中位置的过程看成是一个获取 X锁 的 锁定读 。如果未修改该记录的键值并且至少有一个被更新的列占用的存储空间在修改前后发生变化，则先在B+ 树中定位到这条记录的位置，然后获取一下记录的 X锁 ，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插入一条新记录。这个定位待修改记录在 B+ 树中位置的过程看成是一个获取 X 锁 的 锁定读 ，新插入的记录由 INSERT 操作提供的 隐式锁 进行保护。如果修改了该记录的键值，则相当于在原录上做 DELETE 操作之后再来一次 INSERT 操作，加锁操作就需要按照 DELETE 和 INSERT 的规则进行了。
INSERT ：一般情况下，新插入一条记录的操作并不加锁，设计 InnoDB 的大叔通过一种称之为 隐式锁 的东东来保护这条新插入的记录在本事务提交前不被别的事务访问，更多细节我们后边看哈～
多粒度锁
我们前边提到的 锁 都是针对记录的，也可以被称之为 行级锁 或者 行锁 ，对一条记录加锁影响的也只是这条记录而已，我们就说这个锁的粒度比较细；其实一个事务也可以在 表 级别进行加锁，自然就被称之为 表级锁 或 者 表锁 ，对一个表加锁影响整个表中的记录，我们就说这个锁的粒度比较粗。给表加的锁也可以分为 共享锁 S锁 ）和 独占锁 （ X锁 ）
给表加 S锁 ：如果一个事务给表加了 S锁 ，那么别的事务可以继续获得该表的 S锁，别的事务可以继续获得该表中的某些记录的 S锁，别的事务不可以继续获得该表的 X锁，别的事务不可以继续获得该表中的某些记录的 X锁
给表加 X锁 ：如果一个事务给表加了 X锁 （意味着该事务要独占这个表），那么：别的事务不可以继续获得该表的 S锁，别的事务不可以继续获得该表中的某些记录的 S锁别的事务不可以继续获得该表的 X锁，别的事务不可以继续获得该表中的某些记录的 X锁
我们在对教学楼整体上锁（ 表锁 ）时，怎么知道教学楼中有没有教室已经被上锁（ 行锁 ）了呢？依次检查每一间教室门口有没有上锁？那这效率也太慢了吧！遍历是不可能遍历的，这辈子也不可能遍历的，于是乎设计
InnoDB 的大叔们提出了一种称之为 意向锁
意向共享锁，英文名： Intention Shared Lock ，简称 IS锁 。当事务准备在某条记录上加 S锁 时，需要先
在表级别加一个 IS锁 。
意向独占锁，英文名： Intention Exclusive Lock ，简称 IX锁 。当事务准备在某条记录上加 X锁 时，需要先在表级别加一个 IX锁 。
当加表锁时，如果要加S锁，要查看表级有无IX锁，如果要加X锁，要看有无表级IS锁和IX锁
IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否
被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是兼容的，IX锁和IX锁是兼容的
InnoDB存储引擎中的锁
表级锁：在InnoDB 存储引擎提供的表级 S锁 或者 X锁 是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。不过我们还是可以手动获取一下的，比方说在系统变量autocommit=0，innodb_table_locks =1 时，手动获取 InnoDB 存储引擎提供的表 t 的 S锁 或者 X锁 可以这么写：LOCK TABLES t READ ： InnoDB 存储引擎会对表 t 加表级别的 S锁 。LOCK TABLES t WRITE ： InnoDB 存储引擎会对表 t 加表级别的 X锁 。
行级锁：行锁 ，也称为 记录锁 ，顾名思义就是在记录上加的锁。不过设计 InnoDB 的大叔很有才，一个 行锁 玩出了各种花样，也就是把 行锁 分成了各种类型。换句话说即使对同一条记录加 行锁 ，如果类型不同，起到的功效也是不同的
Record Locks：行记录锁，分为S锁和X锁
Gap Locks：间隙锁，MySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方案解决，也可以采用 加锁 方案解决。但是在使用 加锁 方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上Record Locks，于是就有了间隙锁，顾名思义间隙锁是记录的间隙加锁，使其无法在某个范围内插入数据，从而解决幻读。
原文：A gap lock is a lock on a gap between index records, or a lock on the gap before the first or after the last index record.
译文：间隙锁是索引记录之间间隙上的锁，或者是第一个索引记录之前或最后一个索引记录之后间隙上的锁
Next-Key Locks：其实就是Record Locks+Gap Locks，即给当前行加锁，又给当前行的前后间隙加锁。
比如在查询select * from test_gaplock where id>1 and id&lt;7 for update中一共有id=3和id=5两条记录，其中id为主键或者唯一列的情况下，会锁定范围id(1,3)，（3，5），（5，7）之间不允许插入数据，并且id=3和id=5两条记录也会被锁定。如果id不为主键或者唯一列，则有可能锁定整张表，在本次事务结束前别的事物无法插入数据。
另外在id列为主键或者唯一列的情况下，select * from test_gaplock where id=1 for update 不会产生间隙锁，因为已经明确查询的范围只会有唯一一条数据，不需要间隙锁。
Insert Intention Locks：插入意向锁，事务在对语句加锁之前要先判断语句上有没有锁，于是就产生了插入意向锁。在1锁结构中有tyep字段来表明当前事务的锁是什么锁，插入意向锁就是其中一种所结构，它表示当前记录范围被其他事务锁定，当前事务对被锁定的记录范围有插入意向，正在等待插入（被阻塞）。插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁
隐式锁：一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），当别的事务在对这条新加的记录加 S锁 或者 X锁时，程序会先判断新纪录上的trx_id是否与要加锁的事务一致，如果不一致，则会给插入记录的事务生成一个锁结构，然后再给此事务生成一个锁结构后进入等待状态，这种方式成为隐式加锁。</content></entry><entry><title>VMware虚拟机CentOS 7.5设置静态ip</title><url>https://codingroam.github.io/post/vmware%E8%99%9A%E6%8B%9F%E6%9C%BAcentos-7.5%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81ip/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Hands-on</tag><tag>VMWare</tag></tags><content type="html"> CentOS 7.5设置静态ip,配置网关，DNS等
打开vmware虚拟机界面的编辑->虚拟网络编辑器如下图： 打开网配置文件
#vim /etc/sysconfig/network-scripts/ifcfg-ens33
DEFROUTE="yes" IPV4_FAILURE_FATAL="no" IPV6INIT="yes" IPV6_AUTOCONF="yes" IPV6_DEFROUTE="yes" IPV6_FAILURE_FATAL="no" IPV6_ADDR_GEN_MODE="stable-privacy" NAME="ens33" UUID="48aeeb81-96b5-4d08-880a-53e3f527469c" DEVICE="ens33" ONBOOT="yes" BOOTPROTO=static #静态ip获取方式 IPADDR="192.168.52.104" #静态ip地址 NETMASK="255.255.255.0" #子网掩码 GATEWAY="192.168.52.2" #网关 DNS1="114.114.114.114" #国内移动联通电信网络DNS</content></entry><entry><title>Linux常用设置、命令杂记</title><url>https://codingroam.github.io/post/linux%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%E5%91%BD%E4%BB%A4%E6%9D%82%E8%AE%B0/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Learning</tag></tags><content type="html"> 记录Linux常用的便捷设置和常用命令
快速卸载已安装的rpm软件 rpm -qa | grep -i java | xargs -n1 rpm -e &ndash;nodeps
Ø rpm -qa：查询所安装的所有rpm软件包
Ø grep -i：忽略大小写
Ø xargs -n1：表示每次只传递一个参数
Ø rpm -e –nodeps：强制卸载软件，忽略依赖
安装java *1**）**卸载现有**JDK*
注意：安装JDK前，一定确保提前删除了虚拟机自带的JDK。详细步骤见问文档3.1节中卸载JDK步骤。
*2**）**用**XShell传输**工具将JDK导入到opt目录下面的software文件夹下面*
*3**）**在Linux系统下的opt目录中查看软件包是否导入成功*
[atguigu@hadoop102 ~]$ ls /opt/software/
看到如下结果：
jdk-8u212-linux-x64.tar.gz
*4**）**解压JDK到/opt/module目录下*
[atguigu@hadoop102 software]$ tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
*5**）**配置JDK环境变量*
​ （1）新建/etc/profile.d/my_env.sh文件
[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh
添加如下内容
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
​ （2）保存后退出
:wq
​ （3）source一下/etc/profile文件，让新的环境变量PATH生效
[atguigu@hadoop102 ~]$ source /etc/profile
*6**）**测试JDK**是否**安装成功*
[atguigu@hadoop102 ~]$ java -version
如果能看到以下结果，则代表Java安装成功。
java version &ldquo;1.8.0_212&rdquo;
编写集群分发脚本xsync *1）scp（**secure copy**）**安全**拷贝*
（1）scp定义
scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）
（2）基本语法
scp -r $pdir/$fname $user@$host:$pdir/$fname
命令 递归 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称
（3）案例实操
Ø 前提：在hadoop102、hadoop103、hadoop104都已经创建好的/opt/module、 /opt/software两个目录，并且已经把这两个目录修改为atguigu:atguigu
[atguigu@hadoop102 ~]$ sudo chown atguigu:atguigu -R /opt/module
（a）在hadoop102上，将hadoop102中/opt/module/jdk1.8.0_212目录拷贝到hadoop103上。
[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212 atguigu@hadoop103:/opt/module
（b）在hadoop103上，将hadoop102中/opt/module/hadoop-3.1.3目录拷贝到hadoop103上。
[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/
（c）在hadoop103上操作，将hadoop102中/opt/module目录下所有目录拷贝到hadoop104上。
[atguigu@hadoop103 opt]$ scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module
*2）rsync远程**同步**工具*
rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。
rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。
​ （1）基本语法
rsync -av $pdir/$fname $user@$host:$pdir/$fname
命令 选项参数 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称
​ 选项参数说明
选项 功能 -a 归档拷贝 -v 显示复制过程 （2）案例实操
​ （a）删除hadoop103中/opt/module/hadoop-3.1.3/wcinput
[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/
​ （b）同步hadoop102中的/opt/module/hadoop-3.1.3到hadoop103
[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/
*3）**xsync集群分发**脚本*
（1）需求：循环复制文件到所有节点的相同目录下
​ （2）需求分析：
（a）rsync命令原始拷贝：
rsync -av /opt/module atguigu@hadoop103:/opt/
（b）期望脚本：
xsync要同步的文件名称
（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）
[atguigu@hadoop102 ~]$ echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin:/opt/module/jdk1.8.0_212/bin
（3）脚本实现
（a）在/home/atguigu/bin目录下创建xsync文件
[atguigu@hadoop102 opt]$ cd /home/atguigu
[atguigu@hadoop102 ~]$ mkdir bin
[atguigu@hadoop102 ~]$ cd bin
[atguigu@hadoop102 bin]$ vim xsync
在该文件中编写如下代码
# #!/bin/bash #1. 判断参数个数 if [ $# -lt 1 ] then echo Not Enough Arguement! exit; fi #2. 遍历集群所有机器 for host in centos1 centos2 centos4 do echo ==================== $host ==================== #3. 遍历所有目录，挨个发送 for file in $@ do #4. 判断文件是否存在 if [ -e $file ] then #5. 获取父目录 pdir=$(cd -P $(dirname $file); pwd) #6. 获取当前文件的名称 fname=$(basename $file) ssh $host "mkdir -p $pdir" rsync -av $pdir/$fname $host:$pdir else echo $file does not exists! fi done done （b）修改脚本 xsync 具有执行权限
[atguigu@hadoop102 bin]$ chmod +x xsync
（c）测试脚本
[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin
（d）将脚本复制到/bin中，以便全局调用
[atguigu@hadoop102 bin]$ sudo cp xsync /bin/
（e）同步环境变量配置（root所有者）
[atguigu@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh
注意：如果用了sudo，那么xsync一定要给它的路径补全。
让环境变量生效
[atguigu@hadoop103 bin]$ source /etc/profile
[atguigu@hadoop104 opt]$ source /etc/profile
SSH无密登录配置 *1**）**配置ssh*
（1）基本语法
ssh另一台电脑的IP地址
（2）ssh连接时出现Host key verification failed的解决方法
[atguigu@hadoop102 ~]$ ssh hadoop103
Ø 如果出现如下内容
Are you sure you want to continue connecting (yes/no)?
Ø 输入yes，并回车
（3）退回到hadoop102
[atguigu@hadoop103 ~]$ exit
*2**）**无密钥配置*
（1）免密登录原理
（2）生成公钥和私钥
[atguigu@hadoop102 .ssh]$ pwd
/home/atguigu/.ssh
[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa
然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
（3）将公钥拷贝到要免密登录的目标机器上
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104
注意：
还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。
还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。
还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；
*3**）**.ssh文件夹下**（~/.ssh）**的文件功能解释*
known_hosts 记录ssh访问过计算机的公钥（public key） id_rsa 生成的私钥 id_rsa.pub 生成的公钥 authorized_keys 存放授权过的无密登录服务器公钥</content></entry><entry><title>Linux文件详细属性学习</title><url>https://codingroam.github.io/post/linux%E6%96%87%E4%BB%B6%E8%AF%A6%E7%BB%86%E5%B1%9E%E6%80%A7%E5%AD%A6%E4%B9%A0/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Learning</tag></tags><content type="html"> 详细介绍如何查看并分辨Linux文件的详细属性，包括文件的类型，权限信息，所有者，用户等
Linux的基本思想有两点：第一，一切都是文件；第二，每个文件都有确定的用途。其中第一条详细来讲就是系统中的所有都归结为一个文件，包括命令、硬件和软件设备、操作系统、进程等等对于操作系统内核而言，都被视为拥有各自特性或类型的文件
ls -l 命令是以长格式的形式查看当前目录下所有可见文件的详细属性
文件的详细属性如下：
如果是一个符号链接，那么会有一个 “->" 箭头符号，后面根一个它指向的文件名;
灰白色表示普通文件；
亮绿色表示可执行文件；
亮红色表示压缩文件；
灰蓝色表示目录；
亮蓝色表示链接文件；
亮黄色表示设备文件；</content></entry><entry><title>Vim的四种模式及操作命令大全</title><url>https://codingroam.github.io/post/vim%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8F%E5%8F%8A%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Vim</tag></tags><content type="html"> 介绍Vim的四种模式及命令操作大全
一、四种模式 1、正常模式（normal） 正常模式是使用vim打开文件时的默认模式。 无论在哪种模式下，按下Esc键就会进入正常模式。 在这个模式下： 可以移动光标 选中行，复制（ctrl+C） 可以增、删 x删除光标后的一个字符，nx（n是数字）删除光标后的n个字符，X删除光标前的一个字符， dd剪切光标所在的那一行，ndd剪切光标所在行后的n行 p光标所在行开始，向后粘贴已经复制的内容，P光标所在行开始，向前粘贴已经复制的内容 yy复制光标所在的行，nyy复制光标所在行后的n行 u还原上一次的操作 2、命令模式（command） 在正常模式下输入:或/进入命令行模式 在该模式下可以进行保存，搜索，替换，退出，显示行号等。 /word 光标之后查找字符串word，按n向后搜索，按N向前搜索 ?word光标之前查找字符串word，按n向前搜索，按N向前搜索 :n1,n2/word1/word2/g 将n1到n2行之间的word1替换为word2，不加g则只替换每行的第一个word1，加g则搜到的word1全部替换为word2； :1,$s/word1/word2/g将文章中的word1替换为word2，不加g则只替换每行的第一个word1 :w保存文本 ，:w!强制保存 :q退出vim ；:q!强制退出 :wq 保存并退出 :set nu 显示行号，:set nonu不显示行号 3、插入模式（insert） 在正常模式下按下i键，进入插入模式。在插入模式下按Esc键切换到普通模式。 插入模式里可以进行文字的输入 i 在光标所在字符前开始输入文字并进入插入模式。 I 在行首开始输入文字并进入插入模式。此行首指第一个非空白字符处。如果行首有空格，则在空格之后输入文字并进入插入模式 a 在光标所在字符后开始输入文字并进入插入模式 A 在行尾开始输入文字并进入插入模式。这个好用，您不必管光标在此行的什麽地方，只要按 A 就会在行尾等着您输入文字。 o 在光标所在行的下面单独开一新行，来输入文字并进入插入模式 O 在光标所在行的上面单独开一新行来输入文字并进入插入模式 s 删除光标所在的字符并进入插入模式 S 删除光标所在行并进入插入模式 4、可视模式（visual） 在正常模式下按v（小写）进入字符文本，按V（大写）进入行文本，然后使用上下左右键操作选中区域，对选中的部分使用 d进行删除 y进行复制 p进行粘贴 r进行文本替换 gu转换为小写，gU转换为大写，g~大小写互换 二、操作命令大全 [常用] 剪切和复制、粘贴
[n]x: 剪切光标右边n个字符，相当于d[n]l。 [n]X: 剪切光标左边n个字符，相当于d[n]h。 y: 复制在可视模式下选中的文本。 yy or Y: 复制整行文本。 y[n]w: 复制一(n)个词。 y[n]l: 复制光标右边1(n)个字符。 y[n]h: 复制光标左边1(n)个字符。 y$: 从光标当前位置复制到行尾。 y0: 从光标当前位置复制到行首。 :m,ny 复制m行到n行的内容。 y1G或ygg: 复制光标以上的所有行。 yG: 复制光标以下的所有行。 yaw和yas：复制一个词和复制一个句子，即使光标不在词首和句首也没关系。 d: 删除（剪切）在可视模式下选中的文本。 d$ or D: 删除（剪切）当前位置到行尾的内容。 d[n]w: 删除（剪切）1(n)个单词 d[n]l: 删除（剪切）光标右边1(n)个字符。 d[n]h: 删除（剪切）光标左边1(n)个字符。 d0: 删除（剪切）当前位置到行首的内容 [n] dd: 删除（剪切）1(n)行。 :m,nd 剪切m行到n行的内容。 d1G或dgg: 剪切光标以上的所有行。 dG: 剪切光标以下的所有行。 daw和das：剪切一个词和剪切一个句子，即使光标不在词首和句首也没关系。 d/f：这是一个比较高级的组合命令，它将删除当前位置 到下一个f之间的内容。 p: 在光标之后粘贴。 P: 在光标之前粘贴。 [常用]基本插入
i: 在光标前插入；一个小技巧：按8，再按i，进入插入模式，输入=， 按esc进入命令模式，就会出现8个=。 这在插入分割线时非常有用，如30i+就插入了36个+组成的分割线。 I: 在当前行第一个非空字符前插入； gI: 在当前行第一列插入； a: 在光标后插入； A: 在当前行最后插入； o: 在下面新建一行插入； O: 在上面新建一行插入； :r filename在当前位置插入另一个文件的内容。 :[n]r filename在第n行插入另一个文件的内容。 :r !date 在光标处插入当前日期与时间。同理，:r !command可以将其它shell命令的输出插入当前文档。 [常用] 基本移动
以下移动都是在normal模式下。
h或退格: 左移一个字符； l或空格: 右移一个字符； j: 下移一行； k: 上移一行； gj: 移动到一段内的下一行； gk: 移动到一段内的上一行； +或Enter: 把光标移至下一行第一个非空白字符。 -: 把光标移至上一行第一个非空白字符。 w: 前移一个单词，光标停在下一个单词开头； W: 移动下一个单词开头，但忽略一些标点； e: 前移一个单词，光标停在下一个单词末尾； E: 移动到下一个单词末尾，如果词尾有标点，则移动到标点； b: 后移一个单词，光标停在上一个单词开头； B: 移动到上一个单词开头，忽略一些标点； ge: 后移一个单词，光标停在上一个单词末尾； gE: 同 ge ，不过‘单词’包含单词相邻的标点。 (: 前移1句。 ): 后移1句。 {: 前移1段。 }: 后移1段。 fc: 把光标移到同一行的下一个c字符处 Fc: 把光标移到同一行的上一个c字符处 tc: 把光标移到同一行的下一个c字符前 Tc: 把光标移到同一行的上一个c字符后 ;: 配合f &amp; t使用，重复一次 ,: 配合f &amp; t使用，反向重复一次 上面的操作都可以配合n使用，比如在正常模式(下面会讲到)下输入3h， 则光标向左移动3个字符。
0: 移动到行首。 g0: 移到光标所在屏幕行行首。 ^: 移动到本行第一个非空白字符。 g^: 同 ^ ，但是移动到当前屏幕行第一个非空字符处。 $: 移动到行尾。 g$: 移动光标所在屏幕行行尾。 n|: 把光标移到递n列上。 nG: 到文件第n行。 :n 移动到第n行。 :$ 移动到最后一行。 H: 把光标移到屏幕最顶端一行。 M: 把光标移到屏幕中间一行。 L: 把光标移到屏幕最底端一行。 gg: 到文件头部。 G: 到文件尾部。 2. 启动Vim
vim -c cmd file: 在打开文件前，先执行指定的命令； vim -r file: 恢复上次异常退出的文件； vim -R file: 以只读的方式打开文件，但可以强制保存； vim -M file: 以只读的方式打开文件，不可以强制保存； vim -y num file: 将编辑窗口的大小设为num行； vim + file: 从文件的末尾开始； vim +num file: 从第num行开始； vim +/string file: 打开file，并将光标停留在第一个找到的string上。 vim &ndash;remote file: 用已有的vim进程打开指定的文件。 如果你不想启用多个vim会话，这个很有用。但要注意， 如果你用vim，会寻找名叫VIM的服务器；如果你已经有一个gvim在运行了， 你可以用gvim &ndash;remote file在已有的gvim中打开文件。 3. 文档操作
:e file &ndash;关闭当前编辑的文件，并开启新的文件。 如果对当前文件的修改未保存，vi会警告。 :e! file &ndash;放弃对当前文件的修改，编辑新的文件。 :e+file &ndash; 开始新的文件，并从文件尾开始编辑。 :e+n file &ndash; 开始新的文件，并从第n行开始编辑。 :enew &ndash;编译一个未命名的新文档。(CTRL-W n) :e &ndash; 重新加载当前文档。 :e! &ndash; 重新加载当前文档，并丢弃已做的改动。 :e#或ctrl+^ &ndash; 回到刚才编辑的文件，很实用。 :f或ctrl+g &ndash; 显示文档名，是否修改，和光标位置。 :f filename &ndash; 改变编辑的文件名，这时再保存相当于另存为。 gf &ndash; 打开以光标所在字符串为文件名的文件。 :w &ndash; 保存修改。 :n1,n2w filename &ndash; 选择性保存从某n1行到另n2行的内容。 :wq &ndash; 保存并退出。 ZZ &ndash; 保存并退出。 :x &ndash; 保存并退出。 :q[uit] ——退出当前窗口。(CTRL-W q或CTRL-W CTRL-Q) :saveas newfilename &ndash; 另存为 :browse e &ndash; 会打开一个文件浏览器让你选择要编辑的文件。 如果是终端中，则会打开netrw的文件浏览窗口； 如果是gvim，则会打开一个图形界面的浏览窗口。 实际上:browse后可以跟任何编辑文档的命令，如sp等。 用browse打开的起始目录可以由browsedir来设置： :set browsedir=last &ndash; 用上次访问过的目录（默认）； :set browsedir=buffer &ndash; 用当前文件所在目录； :set browsedir=current &ndash; 用当前工作目录； :Sex &ndash; 水平分割一个窗口，浏览文件系统； :Vex &ndash; 垂直分割一个窗口，浏览文件系统； 4. 光标的移动
4.1 基本移动
以下移动都是在normal模式下。
h或退格: 左移一个字符； l或空格: 右移一个字符； j: 下移一行； k: 上移一行； gj: 移动到一段内的下一行； gk: 移动到一段内的上一行； +或Enter: 把光标移至下一行第一个非空白字符。 -: 把光标移至上一行第一个非空白字符。 w: 前移一个单词，光标停在下一个单词开头； W: 移动下一个单词开头，但忽略一些标点； e: 前移一个单词，光标停在下一个单词末尾； E: 移动到下一个单词末尾，如果词尾有标点，则移动到标点； b: 后移一个单词，光标停在上一个单词开头； B: 移动到上一个单词开头，忽略一些标点； ge: 后移一个单词，光标停在上一个单词末尾； gE: 同 ge ，不过‘单词’包含单词相邻的标点。 (: 前移1句。 ): 后移1句。 {: 前移1段。 }: 后移1段。 fc: 把光标移到同一行的下一个c字符处 Fc: 把光标移到同一行的上一个c字符处 tc: 把光标移到同一行的下一个c字符前 Tc: 把光标移到同一行的上一个c字符后 ;: 配合f &amp; t使用，重复一次 ,: 配合f &amp; t使用，反向重复一次 上面的操作都可以配合n使用，比如在正常模式(下面会讲到)下输入3h， 则光标向左移动3个字符。
0: 移动到行首。 g0: 移到光标所在屏幕行行首。 ^: 移动到本行第一个非空白字符。 g^: 同 ^ ，但是移动到当前屏幕行第一个非空字符处。 $: 移动到行尾。 g$: 移动光标所在屏幕行行尾。 n|: 把光标移到递n列上。 nG: 到文件第n行。 :n 移动到第n行。 :$ 移动到最后一行。 H: 把光标移到屏幕最顶端一行。 M: 把光标移到屏幕中间一行。 L: 把光标移到屏幕最底端一行。 gg: 到文件头部。 G: 到文件尾部。 4.2 翻屏
ctrl+f: 下翻一屏。 ctrl+b: 上翻一屏。 ctrl+d: 下翻半屏。 ctrl+u: 上翻半屏。 ctrl+e: 向下滚动一行。 ctrl+y: 向上滚动一行。 n%: 到文件n%的位置。 zz: 将当前行移动到屏幕中央。 zt: 将当前行移动到屏幕顶端。 zb: 将当前行移动到屏幕底端。 4.3 标记
使用标记可以快速移动。到达标记后，可以用Ctrl+o返回原来的位置。 Ctrl+o和Ctrl+i 很像浏览器上的 后退 和 前进 。
m{a-z}: 标记光标所在位置，局部标记，只用于当前文件。 m{A-Z}: 标记光标所在位置，全局标记。标记之后，退出Vim， 重新启动，标记仍然有效。 `{a-z}: 移动到标记位置。 &lsquo;{a-z}: 移动到标记行的行首。 `{0-9}：回到上[2-10]次关闭vim时最后离开的位置。 : 移动到上次编辑的位置。''也可以，不过精确到列，而&rsquo;&lsquo;精确到行 。如果想跳转到更老的位置，可以按C-o，跳转到更新的位置用C-i。 `": 移动到上次离开的地方。 `.: 移动到最后改动的地方。 :marks 显示所有标记。 :delmarks a b &ndash; 删除标记a和b。 :delmarks a-c &ndash; 删除标记a、b和c。 :delmarks a c-f &ndash; 删除标记a、c、d、e、f。 :delmarks! &ndash; 删除当前缓冲区的所有标记。 :help mark-motions 查看更多关于mark的知识。 5. 插入文本
[ 5.2 改写插入
c[n]w: 改写光标后1(n)个词。 c[n]l: 改写光标后n个字母。 c[n]h: 改写光标前n个字母。 [n]cc: 修改当前[n]行。 [n]s: 以输入的文本替代光标之后1(n)个字符，相当于c[n]l。 [n]S: 删除指定数目的行，并以所输入文本代替之。 注意，类似cnw,dnw,ynw的形式同样可以写为ncw,ndw,nyw。
6. 剪切复制和寄存器
6.2 文本对象
aw：一个词 as：一句。 ap：一段。 ab：一块（包含在圆括号中的）。 y, d, c, v都可以跟文本对象。
6.3 寄存器
a-z：都可以用作寄存器名。&ldquo;ayy把当前行的内容放入a寄存器。 A-Z：用大写字母索引寄存器，可以在寄存器中追加内容。 如"Ayy把当前行的内容追加到a寄存器中。 :reg 显示所有寄存器的内容。 &ldquo;"：不加寄存器索引时，默认使用的寄存器。 &ldquo;*：当前选择缓冲区，"*yy把当前行的内容放入当前选择缓冲区。 &ldquo;+：系统剪贴板。"+yy把当前行的内容放入系统剪贴板。 7. 查找与替换
7.1 查找
/something: 在后面的文本中查找something。 ?something: 在前面的文本中查找something。 /pattern/+number: 将光标停在包含pattern的行后面第number行上。 /pattern/-number: 将光标停在包含pattern的行前面第number行上。 n: 向后查找下一个。 N: 向前查找下一个。 可以用grep或vimgrep查找一个模式都在哪些地方出现过，
其中:grep是调用外部的grep程序，而:vimgrep是vim自己的查找算法。
用法为： :vim[grep]/pattern/[g] [j] files
g的含义是如果一个模式在一行中多次出现，则这一行也在结果中多次出现。
j的含义是grep结束后，结果停在第j项，默认是停在第一项。
vimgrep前面可以加数字限定搜索结果的上限，如
:1vim/pattern/ % 只查找那个模式在本文件中的第一个出现。
其实vimgrep在读纯文本电子书时特别有用，可以生成导航的目录。
比如电子书中每一节的标题形式为：n. xxxx。你就可以这样：
:vim/^d{1,}./ %
然后用:cw或:copen查看结果，可以用C-w H把quickfix窗口移到左侧，
就更像个目录了。
7.2 替换
:s/old/new - 用new替换当前行第一个old。 :s/old/new/g - 用new替换当前行所有的old。 :n1,n2s/old/new/g - 用new替换文件n1行到n2行所有的old。 :%s/old/new/g - 用new替换文件中所有的old。 :%s/^/xxx/g - 在每一行的行首插入xxx，^表示行首。 :%s/$/xxx/g - 在每一行的行尾插入xxx，$表示行尾。 所有替换命令末尾加上c，每个替换都将需要用户确认。 如：%s/old/new/gc，加上i则忽略大小写(ignore)。 还有一种比替换更灵活的方式，它是匹配到某个模式后执行某种命令，
语法为 :[range]g/pattern/command
例如 :%g/^ xyz/normal dd。
表示对于以一个空格和xyz开头的行执行normal模式下的dd命令。
关于range的规定为：
如果不指定range，则表示当前行。 m,n: 从m行到n行。 0: 最开始一行（可能是这样）。 $: 最后一行 .: 当前行 %: 所有行 7.3 正则表达式
高级的查找替换就要用到正则表达式。
d: 表示十进制数（我猜的） s: 表示空格 S: 非空字符 a: 英文字母 |: 表示 或 .: 表示. {m,n}: 表示m到n个字符。这要和 s与a等连用，如 a{m,n} 表示m 到n个英文字母。 {m,}: 表示m到无限多个字符。 **: 当前目录下的所有子目录。 :help pattern得到更多帮助。
8. 排版
8.1 基本排版
&laquo; 向左缩进一个shiftwidth >> 向右缩进一个shiftwidth :ce(nter) 本行文字居中 :le(ft) 本行文字靠左 :ri(ght) 本行文字靠右 gq 对选中的文字重排，即对过长的文字进行断行 gqq 重排当前行 gqnq 重排n行 gqap 重排当前段 gqnap 重排n段 gqnj 重排当前行和下面n行 gqQ 重排当前段对文章末尾 J 拼接当前行和下一行 gJ 同 J ，不过合并后不留空格。 8.2 拼写检查
:set spell－开启拼写检查功能 :set nospell－关闭拼写检查功能 ]s－移到下一个拼写错误的单词 [s－作用与上一命令类似，但它是从相反方向进行搜索 z=－显示一个有关拼写错误单词的列表，可从中选择 zg－告诉拼写检查器该单词是拼写正确的 zw－与上一命令相反，告诉拼写检查器该单词是拼写错误的 8.3 统计字数
g ^g可以统计文档字符数，行数。 将光标放在最后一个字符上，用字符数减去行数可以粗略统计中文文档的字数。 以上对 Mac 或 Unix 的文件格式适用。 如果是 Windows 文件格式（即换行符有两个字节），字数的统计方法为： 字符数 - 行数 * 2。
9. 编辑多个文件
9.1 一次编辑多个文件
我们可以一次打开多个文件，如
&lt;span style="font-size:14px;">vi a.txt b.txt c.txt &lt;/span> 使用:next(:n)编辑下一个文件。 :2n 编辑下2个文件。 使用:previous或:N编辑上一个文件。 使用:wnext，保存当前文件，并编辑下一个文件。 使用:wprevious，保存当前文件，并编辑上一个文件。 使用:args 显示文件列表。 :n filenames或:args filenames 指定新的文件列表。 vi -o filenames 在水平分割的多个窗口中编辑多个文件。 vi -O filenames 在垂直分割的多个窗口中编辑多个文件。 9.2 多标签编辑
vim -p files: 打开多个文件，每个文件占用一个标签页。 :tabe, tabnew &ndash; 如果加文件名，就在新的标签中打开这个文件， 否则打开一个空缓冲区。 ^w gf &ndash; 在新的标签页里打开光标下路径指定的文件。 :tabn &ndash; 切换到下一个标签。Control + PageDown，也可以。 :tabp &ndash; 切换到上一个标签。Control + PageUp，也可以。 [n] gt &ndash; 切换到下一个标签。如果前面加了 n ， 就切换到第n个标签。第一个标签的序号就是1。 :tab split &ndash; 将当前缓冲区的内容在新页签中打开。 :tabc[lose] &ndash; 关闭当前的标签页。 :tabo[nly] &ndash; 关闭其它的标签页。 :tabs &ndash; 列出所有的标签页和它们包含的窗口。 :tabm[ove] [N] &ndash; 移动标签页，移动到第N个标签页之后。 如 tabm 0 当前标签页，就会变成第一个标签页。 9.3 缓冲区
:buffers或:ls或:files 显示缓冲区列表。 ctrl+^：在最近两个缓冲区间切换。 :bn &ndash; 下一个缓冲区。 :bp &ndash; 上一个缓冲区。 :bl &ndash; 最后一个缓冲区。 :b[n]或:[n]b &ndash; 切换到第n个缓冲区。 :nbw(ipeout) &ndash; 彻底删除第n个缓冲区。 :nbd(elete) &ndash; 删除第n个缓冲区，并未真正删除，还在unlisted列表中。 :ba[ll] &ndash; 把所有的缓冲区在当前页中打开，每个缓冲区占一个窗口。 10. 分屏编辑
vim -o file1 file2:水平分割窗口，同时打开file1和file2 vim -O file1 file2:垂直分割窗口，同时打开file1和file2 10.1 水平分割
:split(:sp) &ndash; 把当前窗水平分割成两个窗口。(CTRL-W s 或 CTRL-W CTRL-S) 注意如果在终端下，CTRL-S可能会冻结终端，请按CTRL-Q继续。 :split filename &ndash; 水平分割窗口，并在新窗口中显示另一个文件。 :nsplit(:nsp) &ndash; 水平分割出一个n行高的窗口。 :[N]new &ndash; 水平分割出一个N行高的窗口，并编辑一个新文件。 (CTRL-W n或 CTRL-W CTRL-N) ctrl+w f &ndash;水平分割出一个窗口，并在新窗口打开名称为光标所在词的文件 。 C-w C-^ &ndash; 水平分割一个窗口，打开刚才编辑的文件。 10.2 垂直分割
:vsplit(:vsp) &ndash; 把当前窗口分割成水平分布的两个窗口。 (CTRL-W v或CTRL CTRL-V) :[N]vne[w] &ndash; 垂直分割出一个新窗口。 :vertical 水平分割的命令： 相应的垂直分割。 10.3 关闭子窗口
:qall &ndash; 关闭所有窗口，退出vim。 :wall &ndash; 保存所有修改过的窗口。 :only &ndash; 只保留当前窗口，关闭其它窗口。(CTRL-W o) :close &ndash; 关闭当前窗口，CTRL-W c能实现同样的功能。 (象 :q :x同样工作 ) 10.4 调整窗口大小
ctrl+w + &ndash;当前窗口增高一行。也可以用n增高n行。 ctrl+w - &ndash;当前窗口减小一行。也可以用n减小n行。 ctrl+w _ &ndash;当前窗口扩展到尽可能的大。也可以用n设定行数。 :resize n &ndash; 当前窗口n行高。 ctrl+w = &ndash; 所有窗口同样高度。 n ctrl+w _ &ndash; 当前窗口的高度设定为n行。 ctrl+w &lt; &ndash;当前窗口减少一列。也可以用n减少n列。 ctrl+w > &ndash;当前窗口增宽一列。也可以用n增宽n列。 ctrl+w | &ndash;当前窗口尽可能的宽。也可以用n设定列数。 10.5 切换和移动窗口
如果支持鼠标，切换和调整子窗口的大小就简单了。
ctrl+w ctrl+w: 切换到下一个窗口。或者是ctrl+w w。 ctrl+w p: 切换到前一个窗口。 ctrl+w h(l,j,k):切换到左（右，下，上）的窗口。 ctrl+w t(b):切换到最上（下）面的窗口。 ctrl+w H(L,K,J): 将当前窗口移动到最左（右、上、下）面。 ctrl+w r：旋转窗口的位置。 ctrl+w T: 将当前的窗口移动到新的标签页上。 11. 快速编辑
11.1 改变大小写
~: 反转光标所在字符的大小写。 可视模式下的U或u：把选中的文本变为大写或小写。 gu(U)接范围（如$，或G），可以把从光标当前位置到指定位置之间字母全部 转换成小写或大写。如ggguG，就是把开头到最后一行之间的字母全部变为小 写。再如gu5j，把当前行和下面四行全部变成小写。 11.2 替换（normal模式）
r: 替换光标处的字符，同样支持汉字。 R: 进入替换模式，按esc回到正常模式。 11.3 撤消与重做（normal模式）
[n] u: 取消一(n)个改动。 :undo 5 &ndash; 撤销5个改变。 :undolist &ndash; 你的撤销历史。 ctrl + r: 重做最后的改动。 U: 取消当前行中所有的改动。 :earlier 4m &ndash; 回到4分钟前 :later 55s &ndash; 前进55秒 11.4 宏
. &ndash;重复上一个编辑动作 qa：开始录制宏a（键盘操作记录） q：停止录制 @a：播放宏a 12. 编辑特殊文件
12.1 文件加解密
vim -x file: 开始编辑一个加密的文件。 :X &ndash; 为当前文件设置密码。 :set key= &ndash; 去除文件的密码。 这里是
滇狐总结的比较高级的vi技巧。
12.2 文件的编码
:e ++enc=utf8 filename, 让vim用utf-8的编码打开这个文件。 :w ++enc=gbk，不管当前文件什么编码，把它转存成gbk编码。 :set fenc或:set fileencoding，查看当前文件的编码。 在vimrc中添加set fileencoding=ucs-bom,utf-8,cp936，vim会根据要打开的文件选择合适的编码。 注意：编码之间不要留空格。 cp936对应于gbk编码。 ucs-bom对应于windows下的文件格式。 让vim 正确处理文件格式和文件编码，有赖于 ~/.vimrc的正确配置
12.3 文件格式
大致有三种文件格式：unix, dos, mac. 三种格式的区别主要在于回车键的编码：dos 下是回车加换行，unix 下只有 换行符，mac 下只有回车符。
:e ++ff=dos filename, 让vim用dos格式打开这个文件。 :w ++ff=mac filename, 以mac格式存储这个文件。 :set ff，显示当前文件的格式。 在vimrc中添加set fileformats=unix,dos,mac，让vim自动识别文件格式。 13. 编程辅助
13.1 一些按键
gd: 跳转到局部变量的定义处； gD: 跳转到全局变量的定义处，从当前文件开头开始搜索； g;: 上一个修改过的地方； g,: 下一个修改过的地方； [[: 跳转到上一个函数块开始，需要有单独一行的{。 ]]: 跳转到下一个函数块开始，需要有单独一行的{。 []: 跳转到上一个函数块结束，需要有单独一行的}。 ][: 跳转到下一个函数块结束，需要有单独一行的}。 [{: 跳转到当前块开始处； ]}: 跳转到当前块结束处； [/: 跳转到当前注释块开始处； ]/: 跳转到当前注释块结束处； %: 不仅能移动到匹配的(),{}或[]上，而且能在#if，#else， #endif之间跳跃。 下面的括号匹配对编程很实用的。
ci&rsquo;, di&rsquo;, yi&rsquo;：修改、剪切或复制&rsquo;之间的内容。 ca&rsquo;, da&rsquo;, ya&rsquo;：修改、剪切或复制&rsquo;之间的内容，包含&rsquo;。 ci&rdquo;, di&rdquo;, yi&rdquo;：修改、剪切或复制"之间的内容。 ca&rdquo;, da", ya"：修改、剪切或复制"之间的内容，包含"。 ci(, di(, yi(：修改、剪切或复制()之间的内容。 ca(, da(, ya(：修改、剪切或复制()之间的内容，包含()。 ci[, di[, yi[：修改、剪切或复制[]之间的内容。 ca[, da[, ya[：修改、剪切或复制[]之间的内容，包含[]。 ci{, di{, yi{：修改、剪切或复制{}之间的内容。 ca{, da{, ya{：修改、剪切或复制{}之间的内容，包含{}。 ci&lt;, di&lt;, yi&lt;：修改、剪切或复制&lt;>之间的内容。 ca&lt;, da&lt;, ya&lt;：修改、剪切或复制&lt;>之间的内容，包含&lt;>。 13.2 ctags
ctags -R: 生成tag文件，-R表示也为子目录中的文件生成tags :set tags=path/tags &ndash; 告诉ctags使用哪个tag文件 :tag xyz &ndash; 跳到xyz的定义处，或者将光标放在xyz上按C-]，返回用C-t :stag xyz &ndash; 用分割的窗口显示xyz的定义，或者C-w ]， 如果用C-w n ]，就会打开一个n行高的窗口 :ptag xyz &ndash; 在预览窗口中打开xyz的定义，热键是C-w }。 :pclose &ndash; 关闭预览窗口。热键是C-w z。 :pedit abc.h &ndash; 在预览窗口中编辑abc.h :psearch abc &ndash; 搜索当前文件和当前文件include的文件，显示包含abc的行。 有时一个tag可能有多个匹配，如函数重载，一个函数名就会有多个匹配。 这种情况会先跳转到第一个匹配处。
:[n]tnext &ndash; 下一[n]个匹配。 :[n]tprev &ndash; 上一[n]个匹配。 :tfirst &ndash; 第一个匹配 :tlast &ndash; 最后一个匹配 :tselect tagname &ndash; 打开选择列表 tab键补齐
:tag xyz &ndash; 补齐以xyz开头的tag名，继续按tab键，会显示其他的。 :tag /xyz &ndash; 会用名字中含有xyz的tag名补全。 13.3 cscope
cscope -Rbq: 生成cscope.out文件 :cs add /path/to/cscope.out /your/work/dir :cs find c func &ndash; 查找func在哪些地方被调用 :cw &ndash; 打开quickfix窗口查看结果 13.4 gtags
Gtags综合了ctags和cscope的功能。 使用Gtags之前，你需要安装GNU Gtags。 然后在工程目录运行 gtags 。
:Gtags funcname 定位到 funcname 的定义处。 :Gtags -r funcname 查询 funcname被引用的地方。 :Gtags -s symbol 定位 symbol 出现的地方。 :Gtags -g string Goto string 出现的地方。 :Gtags -gi string 忽略大小写。 :Gtags -f filename 显示 filename 中的函数列表。 你可以用 :Gtags -f % 显示当前文件。 :Gtags -P pattern 显示路径中包含特定模式的文件。 如 :Gtags -P .h$ 显示所有头文件， :Gtags -P /vm/ 显示vm目录下的文件。 13.5 编译
vim提供了:make来编译程序，默认调用的是make， 如果你当前目录下有makefile，简单地:make即可。
如果你没有make程序，你可以通过配置makeprg选项来更改make调用的程序。 如果你只有一个abc.java文件，你可以这样设置：
&lt;span style="font-size:14px;">set makeprg=javac abc.java &lt;/span> 然后:make即可。如果程序有错，可以通过quickfix窗口查看错误。 不过如果要正确定位错误，需要设置好errorformat，让vim识别错误信息。 如：
&lt;span style="font-size:14px;">:setl efm=%A%f:%l: %m,%-Z%p^,%-C%.%# &lt;/span> %f表示文件名，%l表示行号， %m表示错误信息，其它的还不能理解。 请参考 :help errorformat。
13.6 快速修改窗口
其实是quickfix插件提供的功能， 对编译调试程序非常有用 :)
:copen &ndash; 打开快速修改窗口。 :cclose &ndash; 关闭快速修改窗口。 快速修改窗口在make程序时非常有用，当make之后：
:cl &ndash; 在快速修改窗口中列出错误。 :cn &ndash; 定位到下一个错误。 :cp &ndash; 定位到上一个错误。 :cr &ndash; 定位到第一个错误。 13.7 自动补全
C-x C-s &ndash; 拼写建议。 C-x C-v &ndash; 补全vim选项和命令。 C-x C-l &ndash; 整行补全。 C-x C-f &ndash; 自动补全文件路径。弹出菜单后，按C-f循环选择，当然也可以按 C-n和C-p。 C-x C-p 和C-x C-n &ndash; 用文档中出现过的单词补全当前的词。 直接按C-p和C-n也可以。 C-x C-o &ndash; 编程时可以补全关键字和函数名啊。 C-x C-i &ndash; 根据头文件内关键字补全。 C-x C-d &ndash; 补全宏定义。 C-x C-n &ndash; 按缓冲区中出现过的关键字补全。 直接按C-n或C-p即可。 当弹出补全菜单后：
C-p 向前切换成员； C-n 向后切换成员； C-e 退出下拉菜单，并退回到原来录入的文字； C-y 退出下拉菜单，并接受当前选项。 13.8 多行缩进缩出
正常模式下，按两下>;光标所在行会缩进。 如果先按了n，再按两下>;，光标以下的n行会缩进。 对应的，按两下&lt;;，光标所在行会缩出。 如果在编辑代码文件，可以用=进行调整。 在可视模式下，选择要调整的代码块，按=，代码会按书写规则缩排好。 或者n =，调整n行代码的缩排。 13.9 折叠
zf &ndash; 创建折叠的命令，可以在一个可视区域上使用该命令； zd &ndash; 删除当前行的折叠； zD &ndash; 删除当前行的折叠； zfap &ndash; 折叠光标所在的段； zo &ndash; 打开折叠的文本； zc &ndash; 收起折叠； za &ndash; 打开/关闭当前折叠； zr &ndash; 打开嵌套的折行； zm &ndash; 收起嵌套的折行； zR (zO) &ndash; 打开所有折行； zM (zC) &ndash; 收起所有折行； zj &ndash; 跳到下一个折叠处； zk &ndash; 跳到上一个折叠处； zi &ndash; enable/disable fold; 14. 命令行
normal模式下按:进入命令行模式
14.1 命令行模式下的快捷键：
上下方向键：上一条或者下一条命令。如果已经输入了部分命令，则找上一 条或者下一条匹配的命令。 左右方向键：左/右移一个字符。 C-w： 向前删除一个单词。 C-h： 向前删除一个字符，等同于Backspace。 C-u： 从当前位置移动到命令行开头。 C-b： 移动到命令行开头。 C-e： 移动到命令行末尾。 Shift-Left： 左移一个单词。 Shift-Right： 右移一个单词。 @： 重复上一次的冒号命令。 q： 正常模式下，q然后按&rsquo;:&rsquo;，打开命令行历史缓冲区， 可以像编辑文件一样编辑命令。 q/和q? 可以打开查找历史记录。 14.2 执行外部命令
:! cmd 执行外部命令。 :!! 执行上一次的外部命令。 :sh 调用shell，用exit返回vim。 :r !cmd 将命令的返回结果插入文件当前位置。 :m,nw !cmd 将文件的m行到n行之间的内容做为命令输入执行命令。 15. 其它
15.1 工作目录
:pwd 显示vim的工作目录。 :cd path 改变vim的工作目录。 :set autochdir 可以让vim 根据编辑的文件自动切换工作目录。 15.2 一些快捷键（收集中）
K: 打开光标所在词的manpage。 *: 向下搜索光标所在词。 g*: 同上，但部分符合即可。 #: 向上搜索光标所在词。 g#: 同上，但部分符合即可。 g C-g: 统计全文或统计部分的字数。 15.3 在线帮助
:h(elp)或F1 打开总的帮助。 :help user-manual 打开用户手册。 命令帮助的格式为：第一行指明怎么使用那个命令； 然后是缩进的一段解释这个命令的作用，然后是进一步的信息。 :helptags somepath 为somepath中的文档生成索引。 :helpgrep 可以搜索整个帮助文档，匹配的列表显示在quickfix窗口中。 Ctrl+] 跳转到tag主题，Ctrl+t 跳回。 :ver 显示版本信息。 15.4 一些小功能
简单计算器: 在插入模式下，输入C-r =，然后输入表达式，就能在 光标处得到计算结果。 vim命令小技巧 保存文件并退出 说起来有些惭愧，我也是最近才学到这个命令 x
和下面的命令是等价的： wq
都是保存当前文件并退出。
（译者注：这两个命令实际上并不完全等价，当文件被修改时两个命令时相同的。但如果未被修改，使用 : x 不会更改文件的修改时间，而使用 :wq 会改变文件的修改时间。）
基本计算器 在插入模式下，你可以使用 Ctrl+r 键然后输入 =，再输入一个简单的算式。按 Enter 键，计算结果就会插入到文件中。例如，尝试输入：
Ctrl+r '=2+2' ENTER 然后计算结果“4 ”会被插入到文件中。
查找重复的连续的单词 当你很快地打字时，很有可能会连续输入同一个单词两次，就像 this this。这种错误可能骗过任何一个人，即使是你自己重新阅读一遍也不可避免。幸运的是，有一个简单的正则表达式可以用来预防这个错误。使用搜索命令（默认是 /）然后输入：
这会显示所有重复的单词。要达到最好的效果，不要忘记把下面的命令：
set hlsearch 放到你的 .vimrc 文件中高亮所有的匹配。
缩写
一个很可能是最令人印象深刻的窍门是你可以在 Vim 中定义缩写，它可以实时地把你输入的东西替换为另外的东西。语法格式如下：
:ab [缩写] [要替换的文字] 一个通用的例子是：
:ab asap as soon as possible 会把你输入的 “asap” 替换为 “as soon as possible”。
在你忘记用 root 方式打开文件时的文件保存 这可能是一个在论坛中一直受欢迎的命令。每当你打开一个你没有写入权限的文件（比如系统配置文件）并做了一些修改，Vim 无法通过普通的 “:w” 命令来保存。
你不需要重新以 root 方式打开文件再进行修改，只需要运行：
:w !sudo tee % 这会直接以 root 方式保存。
实时加密文本 如果你不想让别人看懂你的屏幕上的内容，你可以使用一个内置的选项，通过下面的命令使用 ROT13
来对文本进行编码：
ggVGg? gg 把光标移动到 Vim 缓冲区的第一行，V 进入可视模式，G 把光标移动到缓冲区的最后一行。因此，ggVG 使可视模式覆盖这个当前缓冲区。最后 g? 使用 ROT13 对整个区域进行编码。
注意它可以被映射到一个最常使用的键。它对字母符号也可以很好地工作。要对它进行撤销，最好的方法就是使用撤销命令：u。
自动补全 这是另外一个令我感到惭愧的功能，但我发现周围很多人并不知道。Vim 默认有自动补全的功能。的确这个功能是很基本的，并且可以通过插件来增强，但它也很有帮助。方法很简单。Vim 尝试通过已经输入的单词来预测单词的结尾。比如当你在同一个文件中第二次输入 “compiler” 时，仅仅输入 “com” 然后保持在插入模式，按 Ctrl+n 键就可以看到 Vim 为你补全了单词。很简单，但也很有用。
比较两个文件的不同 你们中的大多数很可能都知道 vimdiff 命令，它可以使用分离模式打开 Vim 并比较两个文件的不同。语法如下：
$ vimdiff [文件1] [文件2] 但同样的结果也可以通过下面的 Vim 命令来获得：
:diffthis 首先在 Vim 中打开原始文件。然后使用分离模式带来第二个文件：
:vsp [文件2] 最后在第一个缓冲区里输入：
:diffthis 通过 Ctrl+w 来切换缓冲区并再次输入：
:diffthis 这样两个文件中不同的部分就会被高亮。
（译者注：可以直接在一个缓冲区里使用命令 :windo diffthis，而不用输入 :diffthis 两次）
要停止比较，使用：
:diffoff 按时间回退文件 Vim 会记录文件的更改，你很容易可以回退到之前某个时间。该命令是相当直观的。比如：
:earlier 1m 会把文件回退到 1 分钟以前的状态。
注意，你可以使用下面的命令进行相反的转换：
:later 删除标记内部的文字 当我开始使用 Vim 时，一件我总是想很方便做的事情是如何轻松的删除方括号或圆括号里的内容。转到开始的标记，然后使用下面的语法：
di[标记] 比如，把光标放在开始的圆括号上，使用下面的命令来删除圆括号内的文字：
di( 如果是方括号或者是引号，则使用：
di{ 和：
di" 删除指定标记前的内容 和删除标记内部有些相似，但目的不同。命令如下：
dt[标记] 会删除所有光标和标记之间的内容（保持标记不动），如果在同一行有这个标记的话。例如
dt. 会删除至句子的末尾，但保持 ‘.’ 不动。
把 Vim 变为十六进制编辑器 这不是我最喜欢的窍门，但有时会很有趣。你可以把 Vim 和 xxd 功能连起来来把文件转换为十六进制模式。命令如下：
:%!xxd 类似的，你可以通过下面的命令恢复原来的状态：
:%!xxd -r 把光标下的文字置于屏幕中央 我们所要做的事情如标题所示。如果你想强制滚动屏幕来把光标下的文字置于屏幕的中央，在可视模式中使用命令（译者注：在普通模式中也可以）：
zz
跳到上一个／下一个位置 当你编辑一个很大的文件时，经常要做的事是在某处进行修改，然后跳到另外一处。如果你想跳回之前修改的地方，使用命令：
Ctrl+o 来回到之前修改的地方
类似的：
Ctrl+i 会回退上面的跳动。
把当前文件转化为网页 这会生成一个 HTML 文件来显示文本，并在分开的窗口显示源代码：
:%TOhtml</content></entry><entry><title>Linux(CentOS 7)系统四种安装软件的方式及mysql5.7的详细安装配置</title><url>https://codingroam.github.io/post/linuxcentos-7%E7%B3%BB%E7%BB%9F%E5%9B%9B%E7%A7%8D%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%8Amysql5.7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url><categories><category>Linux</category><category>MySQL</category></categories><tags><tag>Linux</tag><tag>MySQL</tag><tag>Hands-on</tag></tags><content type="html"> 本文以安装mysql为例介绍了Linux(CentOS 7)环境下安装软件的四种方式，比较四种方式的难易程度，以及详细介绍了mysql5.7的安装和配置方法
Linux安装软件的四种方式介绍 四种方式：rpm命令、yum命令、带bin目录的tar安装包和source源码编译安装
四种方式比较 rpm与yum为命令安装，不同的是npm在安装时需要手动下载依赖包，而yum会自动下载依赖
带bin的tar包和source源码安装区别是前者是编译好的，bin文件夹中就是编译之后的二进制可执行文件，后者需要自己手动编译
四种方式操作说明 一、npm和yum安装 rpm与yum命令对比（yum简单，rpm复杂）
RPM 全名 RedHat Package Managerment，是由Red Hat公司提出，被众多Linux发行版本所采用，是一种数据库记录的方式来将所需要的软件安装到到Linux系统的一套软件管理机制
rpm在安装时有严格的顺序限制，包与包有依赖关系，且安装过程中可能依赖别的包需要手动安装，而yum安装某个功能（例如mysql的server端）会自动下载安装依赖
安装软件：rpm -ivh [软件包名称] 卸载软件：rpm -e [软件包名称] 更新软件：rpm -Uvh [软件包名称]
yum check-update：列出所有可更新的软件清单命令;
yum update：更新所有软件或指定软件命令;
yum install ：仅安装指定的软件命令；
yum list：列出所有可安装的软件清单命令；
yum remove ：删除软件包命令；
yum search ：查找软件包命令：
以安装mysql为例，对比yum和rpm的安装过程
①查看是否安装了mysql/mariadb的服务
[root@localhost ~]# rpm -qa |grep -i mysql MySQL-client-5.6.23-1.sles11.x86_64 MySQL-server-5.6.23-1.sles11.x86_64 MySQL-shared-5.6.23-1.sles11.x86_64 MySQL-devel-5.6.23-1.sles11.x86_64 ... [root@localhost ~]# root@# rpm -qa |grep -i mariadb ... ②如果安装需要卸载所有服务
[root@localhost ~]# rpm -e --nodeps MySQL-client-5.6.23-1.sles11.x86_64 [root@localhost ~]# rpm -e --nodeps MySQL-server-5.6.23-1.sles11.x86_64 ... ③使用命令或者去mysql官网下载rpm包
yum安装mysql需要先下载一个基础包安装 [root@localhost ~]# wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm rpm安装需要下载bundle包（rpm文件打包合集）
rpm Bundle表示rpm的打包合集，包含基础包，lib包，client,server 等包如下图
④安装：
yum方式
先安装基础包： [root@localhost ~]# yum install y mysql57-community-release-el7-10.noarch.rpm [root@localhost ~]# yum install mysql-community-server --nogpgcheck --安装完毕 rpm
rpm需要依次安装bundle包中的rpm包，包括基础包，库，client和server等 安装mysql-server服务，只需要安装如下4个软件包即可，使⽤rpm -ivh进⾏安装（按顺序安装，后⾯的 服务依赖前⾯的服务 [root@localhost ~]# rpm -ivh mysql-community-common-5.7.23-1.el7.x86_64.rpm [root@localhost ~]# rpm -ivh mysql-community-libs-5.7.23-1.el7.x86_64.rpm [root@localhost ~]# rpm -ivh mysql-community-client-5.7.23-1.el7.x86_64.rpm [root@localhost ~]# rpm -ivh mysql-community-server-5.7.23-1.el7.x86_64.rpm 安装时如果出现缺少依赖，如少libaio、net-tools还需要yum install [名称]来安装依赖 ⑤启动mysql，设置密码
1、[root@localhost ~]# systemctl start mysqld.service -->启动mysql 2、[root@localhost ~]# grep "password" /var/log/mysqld.log -->查看密码 CSLQ:F=Um5i1 A temporary password is generated for root@localhost: CSLQ:F=Um5i1 3、[root@localhost ~]# mysql -uroot -p -->登录root用户 4、[root@localhost ~]# CSLQ:F=Um5i1 -->输入密码 5、登录进mysql之后设置密码规则(不设置有可能无法修改成简单密码) set global validate_password_policy=0; set global validate_password_length=1; 6、ALTER USER 'root'@'localhost' IDENTIFIED BY 'root'; -->修改密码为root ⑥查找并修改mysql配置文件
1、[root@localhost ~]# which mysql -->查找mysql命令在什么位置 usr/bin/mysql 2.[root@localhost ~]# /usr/bin/mysql --verbose --help | grep -A 1 'Default options' Default options are read from the following files in the given order: /etc/mysql/my.cnf /etc/my.cnf ~/.my.cnf 返回信息表示首先读取的是/etc/mysql/my.cnf文件，如果前一个文件不存在则继续读/etc/my.cnf文件，如若还不存在便会去读~/.my.cnf文件,这三处即是mysql配置文件存放处，找到修改即可 默认配置文件如下： # For advice on how to change settings please see # http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html [mysqld] # # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # # Remove leading # to turn on a very important data integrity option: logging # changes to the binary log between backups. # log_bin # # Remove leading # to set options mainly useful for reporting servers. # The server defaults are faster for transactions and fast SELECTs. # Adjust sizes as needed, experiment to find the optimal values. # join_buffer_size = 128M # sort_buffer_size = 2M # read_rnd_buffer_size = 2M datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid 二、tar包安装和source源码安装 已编译tar包(带bin目录的tar)和source源码安装区别是前者是编译好的，bin文件夹中就是编译之后的二进制可执行文件，后者需要自己手动编译
①带bin的tar包安装
1、[root@localhost ~]# tar -xvf mysql-5.7.26-linux-glibc2.12-x86_64.tar 2、[root@localhost ~]# cp -r mysql-5.7.26-linux-glibc2.12-x86_64 /usr/local/mysql -->拷贝文件夹到/usr/local目录下并重命名为mysql 3、[root@localhost ~]# mkdir /usr/local/mysql/data /usr/local/mysql/logs -->创建data和logs文件夹 4、[root@localhost ~]# groupadd mysql -->添加mysql用户组 5、[root@localhost ~]# useradd -r -g mysql mysql -->向mysql用户组添加mysql用户，-r &lt;参数表示mysql用户是系统用户，不可用于登录系统，-g 参数表示把mysql用户添加到mysql用户组中 6、chown -R mysql:mysql /usr/local/mysql/ -->将mysql目录权限分配给mysql用户组下的mysql用户 已编译tar包安装需要新建mysql用户和用户组，并将mysql目录权限分配给用户，若不进行此操作，在mysql服务启动时会报Starting MySQL. ERROR! The server quit without updating PID file错误 7、[root@localhost ~]# vim /etc/my.cnf -->在/etc下新增配置文件 [mysqld] port = 3306 user = mysql basedir = /usr/local/mysql datadir = /usr/local/mysql/data socket = /usr/local/mysql/data/mysql.sock bind-address = 0.0.0.0 pid-file = /usr/local/mysql/data/mysqld.pid character-set-server = utf8 collation-server = utf8_general_ci max_connections = 200 log-error = /usr/local/mysql/logs/mysqld.log 8、[root@localhost ~]# cd /usr/local/mysql/bin/ -->进入mysql的bin目录 9、[root@localhost ~]#./mysqld --defaults-file=/etc/my.cnf --user=mysql --basedir=/usr/local/mysql/ --datadir=/data/mysql/ --initialize -->初始化 10、[root@localhost ~]# cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql -->将mysql.server放置到/etc/init.d/mysql中 11、[root@localhost ~]# service mysql start -->启动服务 12、[root@localhost ~]# cat /usr/local/mysql/logs/mysqld.log -->日志最后一行有随机生成的初始密码,可登录mysql 13、[root@localhost ~]# ln -s /usr/local/mysql/bin/mysql /usr/bin -->创建软连接到/usr/bin可以全局使用mysql命令(方便登录mysql) 14、[root@localhost ~]# mysql -uroot -p -->登录mysql 15、[root@localhost ~]# 输入12步得到的密码，登录mysql 16、若登录出现Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)，需要将mysql.sock软连接到tmp目录即可 [root@localhost ~]# ln -s /usr/local/mysql/data/mysql.sock /tmp ②source源码安装
(1)创建安装目录 [root@localhost ~]# mkdir /usr/local/mysql/{data,logs,tmp,run} -p (2）首先安装源码编译所需要的包 [root@localhost ~]# yum -y install make gcc-c++ cmake bison-devel ncurses-devel (3)解压 [root@localhost ~]# tar -zxvf mysql-5.7.27.tar.gz [root@localhost ~]# tar -zxvf mysql-boost-5.7.27.tar.gz #（不需要安装，在安装mysql时自动安装，） 两个文件解压以后都会在同一个目录上 mysql-5.7.27 (4)编译安装（编译参数按实际情况制定） cd mysql-5.7.27 cmake . \ -DCMAKE_INSTALL_PREFIX=/usr/local/mysql/ \ -DMYSQL_DATADIR=/usr/local/mysql/data \ -DDOWNLOAD_BOOST=1 \ -DWITH_BOOST=/opt/software/mysql-5.7.27/boost \ -DSYSCONFDIR=/etc \ -DWITH_INNOBASE_STORAGE_ENGINE=1 \ -DWITH_PARTITION_STORAGE_ENGINE=1 \ -DWITH_FEDERATED_STORAGE_ENGINE=1 \ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \ -DWITH_MYISAM_STORAGE_ENGINE=1 \ -DENABLED_LOCAL_INFILE=1 \ -DMYSQL_UNIX_ADDR=/usr/local/mysql/run/mysql.sock \ -DENABLE_DTRACE=0 \ -DDEFAULT_CHARSET=utf8 \ -DDEFAULT_COLLATION=utf8_general_ci \ -DWITH_EMBEDDED_SERVER=1 make &amp; make install (5)/etc/my.cnf配置mysql (6) ./mysqld --defaults-file=/etc/my.cnf --initialize #初始化 (7) cp support-files/mysql.server /etc/init.d/mysql #启动项设置 (8) service mysqld start (9)登录不在赘述 mysql常见报错及处理方式： Can&rsquo;t connect to local MySQL server through socket &lsquo;/tmp/mysql.sock&rsquo; (2)
https://blog.csdn.net/weixin_43464964/article/details/121807241
防火墙问题导致mysql连不上：https://www.68idc.com/news/content/582.html
连接mysql报错误码1130：https://blog.csdn.net/web_15534274656/article/details/126493936
CentOS7查看开放端口命令、查看端口占用情况和开启端口命令、杀掉进程
https://blog.csdn.net/weixin_48053866/article/details/127715462
centos 7 tcp6端口地址不通，导致无法访问：https://www.pianshen.com/article/54202723812/
​</content></entry><entry><title>你真的懂String吗？</title><url>https://codingroam.github.io/post/%E4%BD%A0%E7%9C%9F%E7%9A%84%E6%87%82string%E5%90%97/</url><categories><category>Java</category></categories><tags><tag>Java</tag><tag>Learning</tag></tags><content type="html"> 你真的懂String吗？
你真的懂String吗？ 先出面试题
Q1：下列程序的输出结果：
String s1 = “abc”;
String s2 = “abc”;
System.out.println(s1 == s2);
Q2：下列程序的输出结果：
String s1 = new String(“abc”);
String s2 = new String(“abc”);
System.out.println(s1 == s2);
Q3：下列程序的输出结果：
String s1 = “abc”;
String s2 = “a”;
String s3 = “bc”;
String s4 = s2 + s3;
System.out.println(s1 == s4);
Q3：下列程序的输出结果：
String s1 = “abc”;
final String s2 = “a”;
final String s3 = “bc”;
String s4 = s2 + s3;
System.out.println(s1 == s4);
Q4：下列程序的输出结果：
String s = new String(“abc”);
String s1 = “abc”;
String s2 = new String(“abc”);
System.out.println(s == s1.intern());
System.out.println(s == s2.intern());
System.out.println(s1 == s2.intern());
考察点
String intern在JDK1.6和JDk1.7中的区别，以及常量池在2个版本中的位置；
String intern返回的是什么？
String 为什么用final 修饰？
讲解
new String都是在堆上创建字符串对象。当调用 intern() 方法时，编译器会将字符串添加到常量池中（stringTable维护），并返回指向该常量的引用。 str1.intern()返回的是常量池中的引用。
@Test ​ public void test1() { ​ String abc = new String("abc"); ​ System.out.println(abc.intern() == abc); //false ​ } 2通过字面量赋值创建字符串（如：String str=”twm”）时，会先在常量池中查找是否存在相同的字符串，若存在，则将栈中的引用直接指向该字符串；若不存在，则在常量池中生成一个字符串，再将栈中的引用指向该字符串。
@Test ​ public void test2() { ​ String abc = "abc"; ​ System.out.println(abc.intern() == abc); //true ​ } 3常量字符串的“+”操作，编译阶段直接会合成为一个字符串。如string str=”JA”+”VA”，在编译阶段会直接合并成语句String str=”JAVA”，于是会去常量池中查找是否存在”JAVA”,从而进行创建或引用。
@Test public void test3() { ​ String java = "JA"+"VA"; ​ System.out.println(java.intern() == "JAVA"); //true ​ } 4对于final字段，编译期直接进行了常量替换（而对于非final字段则是在运行期进行赋值处理的）。
final String str1=”ja”; final String str2=”va”; String str3=str1+str2; 在编译时，直接替换成了String str3=”ja”+”va”，根据第三条规则，再次替换成String str3=”JAVA”
5常量字符串和变量拼接时（如：String str3=baseStr + “01”;）会调用stringBuilder.append()在堆上创建新的对象。
6JDK 1.7后，intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。简单的说，就是往常量池放的东西变了：原来在常量池中找不到时，复制一个副本放到常量池，1.7后则是将在堆上的地址引用复制到常量池。
举例说明：
@Test ​ public void test4() { ​ String str2 = new String("str") + new String("01"); //堆 ​ str2.intern(); //在常量池中生成堆的引用 ​ String str1 = "str01"; //常量池中的引用，其实就是堆的引用 ​ System.out.println(str2 == str1);//true ​ } 在JDK 1.7后，常量池从Perm迁移到堆。当执行str2.intern();时，因为常量池中没有“str01”这个字符串，所以会在常量池中生成一个对堆中的“str01”的引用(注意这里是引用 ，就是这个区别于JDK 1.6的地方。在JDK1.6下是生成原字符串的拷贝)，而在进行String str1 = “str01”;字面量赋值的时候，常量池中已经存在一个引用，所以直接返回了该引用，因此str1和str2都指向堆中的同一个字符串，返回true。
@Test ​ public void test5() { ​ String str1 = "str01"; //常量池对象 ​ String str2 = new String("str") + new String("01"); //堆 ​ str2.intern(); //在常量池生成引用，但是引用是指向堆 ​ System.out.println(str2 == str1); //false ​ } 将中间两行调换位置以后，因为在进行字面量赋值（String str1 = “str01″）的时候，常量池中不存在，所以str1指向的常量池中的位置，而str2指向的是堆中的对象，再进行intern方法时，对str1和str2已经没有影响了，所以返回false。
解答
有了对以上的知识的了解，我们现在再来看常见的面试或笔试题就很简单了：
Q：下列程序的输出结果：
String s1 = “abc”;
String s2 = “abc”;
System.out.println(s1 == s2);
A：true，均指向常量池中对象。
Q：下列程序的输出结果：
String s1 = new String(“abc”);
String s2 = new String(“abc”);
System.out.println(s1 == s2);
A：false，两个引用指向堆中的不同对象。
Q：下列程序的输出结果：
String s1 = “abc”;
String s2 = “a”;
String s3 = “bc”;
String s4 = s2 + s3;
System.out.println(s1 == s4);
A：false，因为s2+s3实际上是使用StringBuilder.append来完成，会生成不同的对象。
Q：下列程序的输出结果：
String s1 = “abc”;
final String s2 = “a”;
final String s3 = “bc”;
String s4 = s2 + s3;
System.out.println(s1 == s4);
A：true，因为final变量在编译后会直接替换成对应的值，所以实际上等于s4=”a”+”bc”，而这种情况下，编译器会直接合并为s4=”abc”，所以最终s1==s4。
Q：下列程序的输出结果：
String s = new String(“abc”);
String s1 = “abc”;
String s2 = new String(“abc”);
System.out.println(s == s1.intern());
System.out.println(s == s2.intern());
System.out.println(s1 == s2.intern());
A：false，false，true。</content></entry><entry><title>Markdown语法手册</title><url>https://codingroam.github.io/post/markdown-syntax/</url><categories><category>Markdown</category></categories><tags><tag>Markdown</tag><tag>Reference</tag></tags><content type="html"> 本文提供了一个可以在 Hugo 内容文件中使用的基本Markdown语法示例，还展示了基本 HTML 元素在 Hugo 主题中是否使用 CSS 装饰。
标题 下面的 HTML 代码&lt;h1>—&lt;h6> 元素表示六个级别的节标题。 &lt;h1>是最高的节级别，&lt;h6>是最低的节级别。
H1 H2 H3 H4 H5 H6 段落 生活是什么？生活是柴米油盐的平淡；是行色匆匆早出晚归的奔波；生活是错的时间遇到对的人的遗憾；是爱的付出与回报；生活是看不同的风景，遇到不同的人；是行至水穷尽，坐看云起时的峰回路转；生活是灵魂经历伤痛后的微笑怒放；是挫折坎坷被晾晒后的坚强；生活是酸甜苦辣被岁月沉淀后的馨香；是经历风霜雪雨洗礼后的懂得；生活是走遍千山万水后，回眸一笑的洒脱。
有些事，猝不及防，不管你在不在乎；有些人，并非所想，不管你明不明白；有些路，必须得走，不管你愿不愿意。不怕事，不惹事，不避事，做好自己，用真心面对一切；少埋怨，少指责，少发火，学会沉静，用微笑考量一切；多体察，多包容，多思索，尽心尽力，虽缺憾但无悔。像蒲公英一样美丽，虽轻盈，但并不卑微，它有自己的生命，也有自己的世界！
引用 blockquote 元素表示从另一个来源引用的内容，可选的引用必须在 footer 或 cite元素内，也可选的内嵌更改，如注释和缩写。
引用没有归属 读懂自我，带着简单的心情，看复杂的人生，走坎坷的路！
注意： 可以在块引用中使用 Markdown 语法。
带归属的引用 不要通过分享记忆来交流，通过交流来分享记忆。
— 罗布·派克1
表格 表不是Markdown核心规范的一部分，但是Hugo支持开箱即用。
Name Age Bob 27 Alice 23 表格内使用Markdown语法 Italics Bold Code italics bold code 图像 ![图像描述](图像地址) 示例 常规用法 SVG图像 Google Chrome
Firefox Browser
小图标 点击图像可以打开图像浏览器，快试试吧。
代码块 带有引号的代码块 &lt;!doctype html> &lt;html lang="en"> &lt;head> &lt;meta charset="utf-8"> &lt;title>Example HTML5 Document&lt;/title> &lt;/head> &lt;body> &lt;p>Test&lt;/p> &lt;/body> &lt;/html> 用四个空格缩进的代码块 &lt;!doctype html>
&lt;html lang="en">
&lt;head>
&lt;meta charset="utf-8">
&lt;title>Example HTML5 Document&lt;/title>
&lt;/head>
&lt;body>
&lt;p>Test&lt;/p>
&lt;/body>
&lt;/html>
代码块引用Hugo的内部高亮短代码 &lt;!doctype html> &lt;html lang="en"> &lt;head> &lt;meta charset="utf-8"> &lt;title>Example HTML5 Document&lt;/title> &lt;/head> &lt;body> &lt;p>Test&lt;/p> &lt;/body> &lt;/html> 列表类型 有序列表 First item Second item Third item 无序列表 List item Another item And another item 嵌套列表 Fruit Apple Orange Banana Dairy Milk Cheese 其他元素 — abbr, sub, sup, kbd, mark GIF 是位图图像格式。
H2O
Xn + Yn = Zn
按 CTRL+ALT+Delete 组合键结束会话。
大多数蝾螈在夜间活动，捕食昆虫、蠕虫和其他小动物。
以上引文摘自Rob Pike在2015年11月18日 Gopherfest 上的演讲
。&#160;&#8617;&#xfe0e;</content></entry><entry><title>图像占位符显示</title><url>https://codingroam.github.io/post/placeholder-text/</url><categories><category>Markdown</category></categories><tags><tag>Markdown</tag><tag>Reference</tag></tags><content type="html"> 范德格拉夫原理（Van de Graaf Canon）重构了曾经用于书籍设计中将页面划分为舒适比例的方法。这一原理也被称为“秘密原理”，用于许多中世纪的手稿和古板书中。在范德格拉夫原理中，文本区域和页面的长款具有相同的比例，并且文本区域的高度等于页面宽度，通过划分页面得到九分之一的订口边距和九分之二的切口边距，以及与页面长宽相同的比例的文本区域。
Vagus 示例 The Van de Graaf Canon
总结 当然设计中的黄金比例是为人所熟知的，黄金分割的公式为a:b=b:(a+b)。这是指较小的两个矩形与较大的两个矩形以相同的组合方式相关联。黄金分割比例为1:1.618。</content></entry><entry><title>数据公式设置显示</title><url>https://codingroam.github.io/post/math-typesetting/</url><categories/><tags/><content type="html"> Hugo 项目中的数学表示法可以通过使用第三方 JavaScript 库来实现。
在这个例子中，我们将使用 MathJax
创建一个文件 /content/en[zh-CN]/math.md
可以全局启用MathJax，请在项目配置中将参数math设置为true
或是在每页基础上启用MathJax，在内容文件中包括参数math: true
注意： 使用支持的TeX功能
的联机参考资料
例子 重复的分数 $$ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} \equiv 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } } $$
总和记号 $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$
几何级数之和 我把接下来的两个例子分成了几行，这样它在手机上表现得更好。这就是为什么它们包含 \displaystyle。
$$ \displaystyle\sum_{i=1}^{k+1}i $$
$$ \displaystyle= \left(\sum_{i=1}^{k}i\right) +(k+1) $$
$$ \displaystyle= \frac{k(k+1)}{2}+k+1 $$
$$ \displaystyle= \frac{k(k+1)+2(k+1)}{2} $$
$$ \displaystyle= \frac{(k+1)(k+2)}{2} $$
$$ \displaystyle= \frac{(k+1)((k+1)+1)}{2} $$
乘记号 $$ \displaystyle 1 + \frac{q^2}{(1-q)}+\frac{q^6}{(1-q)(1-q^2)}+\cdots = \displaystyle \prod_{j=0}^{\infty}\frac{1}{(1-q^{5j+2})(1-q^{5j+3})}, \displaystyle\text{ for }\lvert q\rvert &lt; 1. $$
随文数式 这是一些线性数学: $$ k_{n+1} = n^2 + k_n^2 - k_{n-1} $$ ， 然后是更多的文本。
希腊字母 $$ \Gamma\ \Delta\ \Theta\ \Lambda\ \Xi\ \Pi\ \Sigma\ \Upsilon\ \Phi\ \Psi\ \Omega \alpha\ \beta\ \gamma\ \delta\ \epsilon\ \zeta\ \eta\ \theta\ \iota\ \kappa\ \lambda\ \mu\ \nu\ \xi \ \omicron\ \pi\ \rho\ \sigma\ \tau\ \upsilon\ \phi\ \chi\ \psi\ \omega\ \varepsilon\ \vartheta\ \varpi\ \varrho\ \varsigma\ \varphi $$
箭头 $$ \gets\ \to\ \leftarrow\ \rightarrow\ \uparrow\ \Uparrow\ \downarrow\ \Downarrow\ \updownarrow\ \Updownarrow $$
$$ \Leftarrow\ \Rightarrow\ \leftrightarrow\ \Leftrightarrow\ \mapsto\ \hookleftarrow \leftharpoonup\ \leftharpoondown\ \rightleftharpoons\ \longleftarrow\ \Longleftarrow\ \longrightarrow $$
$$ \Longrightarrow\ \longleftrightarrow\ \Longleftrightarrow\ \longmapsto\ \hookrightarrow\ \rightharpoonup $$
$$ \rightharpoondown\ \leadsto\ \nearrow\ \searrow\ \swarrow\ \nwarrow $$
符号 $$ \surd\ \barwedge\ \veebar\ \odot\ \oplus\ \otimes\ \oslash\ \circledcirc\ \boxdot\ \bigtriangleup $$
$$ \bigtriangledown\ \dagger\ \diamond\ \star\ \triangleleft\ \triangleright\ \angle\ \infty\ \prime\ \triangle $$
微积分学 $$ \int u \frac{dv}{dx},dx=uv-\int \frac{du}{dx}v,dx $$
$$ f(x) = \int_{-\infty}^\infty \hat f(\xi),e^{2 \pi i \xi x} $$
$$ \oint \vec{F} \cdot d\vec{s}=0 $$
洛伦茨方程 $$ \begin{aligned} \dot{x} &amp; = \sigma(y-x) \ \dot{y} &amp; = \rho x - y - xz \ \dot{z} &amp; = -\beta z + xy \end{aligned} $$
交叉乘积 这在KaTeX中是可行的，但在这种环境中馏分的分离不是很好。
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
这里有一个解决方案:使用“mfrac”类(在MathJax情况下没有区别)的额外类使分数更小:
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
强调 $$ \hat{x}\ \vec{x}\ \ddot{x} $$
有弹性的括号 $$ \left(\frac{x^2}{y^3}\right) $$
评估范围 $$ \left.\frac{x^3}{3}\right|_0^1 $$
诊断标准 $$ f(n) = \begin{cases} \frac{n}{2}, &amp; \text{if } n\text{ is even} \ 3n+1, &amp; \text{if } n\text{ is odd} \end{cases} $$
麦克斯韦方程组 $$ \begin{aligned} \nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \ \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \ \nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; = \vec{\mathbf{0}} \ \nabla \cdot \vec{\mathbf{B}} &amp; = 0 \end{aligned} $$
这些方程式很狭窄。我们可以使用(例如)添加垂直间距 [1em] 在每个换行符(\)之后。正如你在这里看到的：
$$ \begin{aligned} \nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \[1em] \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \[0.5em] \nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; = \vec{\mathbf{0}} \[1em] \nabla \cdot \vec{\mathbf{B}} &amp; = 0 \end{aligned} $$
统计学 固定词组：
$$ \frac{n!}{k!(n-k)!} = {^n}C_k {n \choose k} $$
分数在分数 $$ \frac{\frac{1}{x}+\frac{1}{y}}{y-z} $$
ｎ次方根 $$ \sqrt[n]{1+x+x^2+x^3+\ldots} $$
矩阵 $$ \begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\ a_{21} &amp; a_{22} &amp; a_{23}\ a_{31} &amp; a_{32} &amp; a_{33} \end{pmatrix} \begin{bmatrix} 0 &amp; \cdots &amp; 0 \ \vdots &amp; \ddots &amp; \vdots \ 0 &amp; \cdots &amp; 0 \end{bmatrix} $$
标点符号 $$ f(x) = \sqrt{1+x} \quad (x \ge -1) f(x) \sim x^2 \quad (x\to\infty) $$
现在用标点符号:
$$ f(x) = \sqrt{1+x}, \quad x \ge -1 f(x) \sim x^2, \quad x\to\infty $$</content></entry><entry><title>关于我</title><url>https://codingroam.github.io/about.html</url><categories/><tags/><content type="html"> Hugo是用Go编写的一个开放源代码静态站点生成器，可在Apache许可证2.0
下使用。 Hugo支持TOML, YAML和JSON数据文件类型，Markdown和HTML内容文件，并使用短代码添加丰富的内容。其他值得注意的功能包括分类法、多语言模式、图像处理、自定义输出格式、HTML/CSS/JS缩小和对Sass SCSS工作流的支持。
Hugo使用了多种开源项目，包括:
https://github.com/yuin/goldmark
https://github.com/alecthomas/chroma
https://github.com/muesli/smartcrop
https://github.com/spf13/cobra
https://github.com/spf13/viper
Hugo是博客、企业网站、创意作品集、在线杂志、单页应用程序甚至是数千页的网站的理想选择。
Hugo适合那些想要手工编写自己的网站代码，而不用担心设置复杂的运行时、依赖关系和数据库的人。
使用Hugo建立的网站非常快速、安全，可以部署在任何地方，包括AWS、GitHub Pages、Heroku、Netlify和任何其他托管提供商。
更多信息请访问GitHub
.</content></entry></search>