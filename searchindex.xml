<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>MySQL七种日志介绍</title><url>/post/mysql%E4%B8%83%E7%A7%8D%E6%97%A5%E5%BF%97%E4%BB%8B%E7%BB%8D/</url><categories><category>Mysql</category></categories><tags><tag>Mysql</tag></tags><content type="html"> MySQL七种日志介绍
进入正题前先简单看看MySQL的逻辑架构，相信我用的着。
MySQL逻辑架构
MySQL的逻辑架构大致可以分为三层：
第一层：处理客户端连接、授权认证，安全校验等。 第二层：服务器server层，负责对SQL解释、分析、优化、执行操作引擎等。 第三层：存储引擎，负责MySQL中数据的存储和提取。 我们要知道MySQL的服务器层是不管理事务的，事务是由存储引擎实现的，而MySQL中支持事务的存储引擎又属InnoDB使用的最为广泛，所以后续文中提到的存储引擎都以InnoDB为主。
MySQL数据更新流程
记住！ 记住！ 记住！ 上边这张图，她是MySQL更新数据的基础流程，其中包括redo log、bin log、undo log三种日志间的大致关系，好了闲话少说直奔主题。
1. redo log（重做日志） redo log属于MySQL存储引擎InnoDB的事务日志。
MySQL的数据是存放在磁盘中的，每次读写数据都需做磁盘IO操作，如果并发场景下性能就会很差。为此MySQL提供了一个优化手段，引入缓存Buffer Pool。这个缓存中包含了磁盘中部分数据页（page）的映射，以此来缓解数据库的磁盘压力。
当从数据库读数据时，首先从缓存中读取，如果缓存中没有，则从磁盘读取后放入缓存；当向数据库写入数据时，先向缓存写入，此时缓存中的数据页数据变更，这个数据页称为脏页，Buffer Pool中修改完数据后会按照设定的更新策略，定期刷到磁盘中，这个过程称为刷脏页。
1.1 MySQL宕机 如果刷脏页还未完成，可MySQL由于某些原因宕机重启，此时Buffer Pool中修改的数据还没有及时的刷到磁盘中，就会导致数据丢失，无法保证事务的持久性。
为了解决这个问题引入了redo log，redo Log如其名侧重于重做！它记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页，且只能恢复到最后一次提交的位置。
redo log用到了WAL（Write-Ahead Logging）技术，这个技术的核心就在于修改记录前，一定要先写日志，并保证日志先落盘，才能算事务提交完成。
有了redo log再修改数据时，InnoDB引擎会把更新记录先写在redo log中，在修改Buffer Pool中的数据，当提交事务时，调用fsync把redo log刷入磁盘。至于缓存中更新的数据文件何时刷入磁盘，则由后台线程异步处理。
注意：此时redo log的事务状态是prepare，还未真正提交成功，要等bin log日志写入磁盘完成才会变更为commit，事务才算真正提交完成。
这样一来即使刷脏页之前MySQL意外宕机也没关系，只要在重启时解析redo log中的更改记录进行重放，重新刷盘即可。
1.2 大小固定 redo log采用固定大小，循环写入的格式，当redo log写满之后，重新从头开始如此循环写，形成一个环状。
那为什么要如此设计呢？
因为redo log记录的是数据页上的修改，如果Buffer Pool中数据页已经刷磁盘后，那这些记录就失效了，新日志会将这些失效的记录进行覆盖擦除。
上图中的write pos表示redo log当前记录的日志序列号LSN(log sequence number)，写入还未刷盘，循环往后递增；check point表示redo log中的修改记录已刷入磁盘后的LSN，循环往后递增，这个LSN之前的数据已经全落盘。
write pos到check point之间的部分是redo log空余的部分（绿色），用来记录新的日志；check point到write pos之间是redo log已经记录的数据页修改数据，此时数据页还未刷回磁盘的部分。当write pos追上check point时，会先推动check point向前移动，空出位置（刷盘）再记录新的日志。
注意：redo log日志满了，在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求的，此刻MySQL的性能会下降。所以在并发量大的情况下，合理调整redo log的文件大小非常重要。
1.3 crash-safe 因为redo log的存在使得Innodb引擎具有了crash-safe的能力，即MySQL宕机重启，系统会自动去检查redo log，将修改还未写入磁盘的数据从redo log恢复到MySQL中。
MySQL启动时，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。会先检查数据页中的LSN，如果这个 LSN 小于 redo log 中的LSN，即write pos位置，说明在redo log上记录着数据页上尚未完成的操作，接着就会从最近的一个check point出发，开始同步数据。
简单理解，比如：redo log的LSN是500，数据页的LSN是300，表明重启前有部分数据未完全刷入到磁盘中，那么系统则将redo log中LSN序号300到500的记录进行重放刷盘。
2. undo log（回滚日志） undo log也是属于MySQL存储引擎InnoDB的事务日志。
undo log属于逻辑日志，如其名主要起到回滚的作用，它是保证事务原子性的关键。记录的是数据修改前的状态，在数据修改的流程中，同时会记录一条与当前操作相反的逻辑日志到undo log中。
我们举个栗子：假如更新ID=1记录的name字段，name原始数据为小富，现改name为程序员内点事
事务执行update X set name = 程序员内点事 where id =1语句时，先会在undo log中记录一条相反逻辑的update X set name = 小富 where id =1记录，这样当某些原因导致服务异常事务失败，就可以借助undo log将数据回滚到事务执行前的状态，保证事务的完整性。
那可能有人会问：同一个事物内的一条记录被多次修改，那是不是每次都要把数据修改前的状态都写入undo log呢？
答案是不会的！
undo log只负责记录事务开始前要修改数据的原始版本，当我们再次对这行数据进行修改，所产生的修改记录会写入到redo log，undo log负责完成回滚，redo log负责完成前滚。
2.1 回滚 未提交的事务，即事务未执行commit。但该事务内修改的脏页中，可能有一部分脏块已经刷盘。如果此时数据库实例宕机重启，就需要用回滚来将先前那部分已经刷盘的脏块从磁盘上撤销。
2.2 前滚 未完全提交的事务，即事务已经执行commit，但该事务内修改的脏页中只有一部分数据被刷盘，另外一部分还在buffer pool缓存上，如果此时数据库实例宕机重启，就需要用前滚来完成未完全提交的事务。将先前那部分由于宕机在内存上的未来得及刷盘数据，从redo log中恢复出来并刷入磁盘。
数据库实例恢复时，先做前滚，后做回滚。
如果你仔细看过了上边的 MySQL数据更新流程图 就会发现，undo log、redo log、bin log三种日志都是在刷脏页之前就已经刷到磁盘了的，相互协作最大限度保证了用户提交的数据不丢失。
3. bin log（归档日志） bin log是一种数据库Server层（和什么引擎无关），以二进制形式存储在磁盘中的逻辑日志。bin log记录了数据库所有DDL和DML操作（不包含 SELECT 和 SHOW等命令，因为这类操作对数据本身并没有修改）。
默认情况下，二进制日志功能是关闭的。可以通过以下命令查看二进制日志是否开启：
mysql> SHOW VARIABLES LIKE 'log_bin'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | OFF | +---------------+-------+ bin log也被叫做归档日志，因为它不会像redo log那样循环写擦除之前的记录，而是会一直记录日志。一个bin log日志文件默认最大容量1G（也可以通过max_binlog_size参数修改），单个日志超过最大值，则会新创建一个文件继续写。
mysql> show binary logs; +-----------------+-----------+ | Log_name | File_size | +-----------------+-----------+ | mysq-bin.000001 | 8687 | | mysq-bin.000002 | 1445 | | mysq-bin.000003 | 3966 | | mysq-bin.000004 | 177 | | mysq-bin.000005 | 6405 | | mysq-bin.000006 | 177 | | mysq-bin.000007 | 154 | | mysq-bin.000008 | 154 | bin log日志的内容格式其实就是执行SQL命令的反向逻辑，这点和undo log有点类似。一般来说开启bin log都会给日志文件设置过期时间（expire_logs_days参数，默认永久保存），要不然日志的体量会非常庞大。
mysql> show variables like 'expire_logs_days'; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | expire_logs_days | 0 | +------------------+-------+ 1 row in set mysql> SET GLOBAL expire_logs_days=30; Query OK, 0 rows affected bin log主要应用于MySQL主从模式（master-slave）中，主从节点间的数据同步；以及基于时间点的数据还原。
3.1 主从同步 通过下图MySQL的主从复制过程，来了解下bin log在主从模式下的应用。
用户在主库master执行DDL和DML操作，修改记录顺序写入bin log; 从库slave的I/O线程连接上Master，并请求读取指定位置position的日志内容; Master收到从库slave请求后，将指定位置position之后的日志内容，和主库bin log文件的名称以及在日志中的位置推送给从库; slave的I/O线程接收到数据后，将接收到的日志内容依次写入到relay log文件最末端，并将读取到的主库bin log文件名和位置position记录到master-info文件中，以便在下一次读取用; slave的SQL线程检测到relay log中内容更新后，读取日志并解析成可执行的SQL语句，这样就实现了主从库的数据一致; 3.2 基于时间点还原 我们看到bin log也可以做数据的恢复，而redo log也可以，那它们有什么区别？
层次不同：redo log 是InnoDB存储引擎实现的，bin log 是MySQL的服务器层实现的，但MySQL数据库中的任何存储引擎对于数据库的更改都会产生bin log。 作用不同：redo log 用于碰撞恢复（crash recovery），保证MySQL宕机也不会影响持久性；bin log 用于时间点恢复（point-in-time recovery），保证服务器可以基于时间点恢复数据和主从复制。 内容不同：redo log 是物理日志，内容基于磁盘的页Page；bin log的内容是二进制，可以根据binlog_format参数自行设置。 写入方式不同：redo log 采用循环写的方式记录；binlog 通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上。 刷盘时机不同：bin log在事务提交时写入；redo log 在事务开始时即开始写入。 bin log 与 redo log 功能并不冲突而是起到相辅相成的作用，需要二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。
4. relay log（中继日志） relay log日志文件具有与bin log日志文件相同的格式，从上边MySQL主从复制的流程可以看出，relay log起到一个中转的作用，slave先从主库master读取二进制日志数据，写入从库本地，后续再异步由SQL线程读取解析relay log为对应的SQL命令执行。
5. slow query log 慢查询日志（slow query log）: 用来记录在 MySQL 中执行时间超过指定时间的查询语句，在 SQL 优化过程中会经常使用到。通过慢查询日志，我们可以查找出哪些查询语句的执行效率低，耗时严重。
出于性能方面的考虑，一般只有在排查慢SQL、调试参数时才会开启，默认情况下，慢查询日志功能是关闭的。可以通过以下命令查看是否开启慢查询日志：
mysql> SHOW VARIABLES LIKE 'slow_query%'; +---------------------+--------------------------------------------------------+ | Variable_name | Value | +---------------------+--------------------------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /usr/local/mysql/data/iZ2zebfzaequ90bdlz820sZ-slow.log | +---------------------+--------------------------------------------------------+ 通过如下命令开启慢查询日志后，我发现iZ2zebfzaequ90bdlz820sZ-slow.log 日志文件里并没有内容啊，可能因为我执行的 SQL 都比较简单没有超过指定时间。
mysql> SET GLOBAL slow_query_log=ON; Query OK, 0 rows affected 上边提到超过 指定时间 的查询语句才算是慢查询，那么这个时间阈值又是多少嘞？我们通过long_query_time参数来查看一下，发现默认是 10 秒。
mysql> SHOW VARIABLES LIKE 'long_query_time'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 这里我们将long_query_time 参数改小为 0.001秒再次执行查询SQL，看看慢查询日志里是否有变化。
mysql> SET GLOBAL long_query_time=0.001; Query OK, 0 rows affected 果然再执行 SQL 的时，执行时间大于 0.001秒，发现慢查询日志开始记录了。
6. general query log 一般查询日志（general query log）：用来记录用户的所有操作，包括客户端何时连接了服务器、客户端发送的所有SQL以及其他事件，比如 MySQL 服务启动和关闭等等。MySQL服务器会按照它接收到语句的先后顺序写入日志文件。
由于一般查询日志记录的内容过于详细，开启后 Log 文件的体量会非常庞大，所以出于对性能的考虑，默认情况下，该日志功能是关闭的，通常会在排查故障需获得详细日志的时候才会临时开启。
我们可以通过以下命令查看一般查询日志是否开启，命令如下：
mysql> show variables like 'general_log'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | general_log | OFF | +---------------+-------+ 下边开启一般查询日志并查看日志存放的位置。
mysql> SET GLOBAL general_log=on; Query OK, 0 rows affected mysql> show variables like 'general_log_file'; +------------------+---------------------------------------------------+ | Variable_name | Value | +------------------+---------------------------------------------------+ | general_log_file | /usr/local/mysql/data/iZ2zebfzaequ90bdlz820sZ.log | +------------------+---------------------------------------------------+ 执行一条查询 SQL 看看日志内容的变化。
mysql> select * from t_config; +---------------------+------------+---------------------+---------------------+ | id | remark | create_time | last_modify_time | +---------------------+------------+---------------------+---------------------+ | 1325741604307734530 | 我是广播表 | 2020-11-09 18:06:44 | 2020-11-09 18:06:44 | +---------------------+------------+---------------------+---------------------+ 我们看到日志内容详细的记录了所有执行的命令、SQL、SQL的解析过程、数据库设置等等。
7. error log 错误日志（error log）: 应该是 MySQL 中最好理解的一种日志，主要记录 MySQL 服务器每次启动和停止的时间以及诊断和出错信息。
默认情况下，该日志功能是开启的，通过如下命令查找错误日志文件的存放路径。
mysql> SHOW VARIABLES LIKE 'log_error'; +---------------+----------------------------------------------------------------+ | Variable_name | Value | +---------------+----------------------------------------------------------------+ | log_error | /usr/local/mysql/data/LAPTOP-UHQ6V8KP.err | +---------------+----------------------------------------------------------------+ 注意：错误日志中记录的可并非全是错误信息，像 MySQL 如何启动 InnoDB 的表空间文件、如何初始化自己的存储引擎，初始化 buffer pool 等等，这些也记录在错误日志文件中。</content></entry><entry><title>PostgreSQL查询出所有表的记录数</title><url>/post/postgresql%E6%9F%A5%E8%AF%A2%E5%87%BA%E6%89%80%E6%9C%89%E8%A1%A8%E7%9A%84%E8%AE%B0%E5%BD%95%E6%95%B0/</url><categories><category>数据库</category><category>PostgreSQL</category><category>数据库</category><category>SQL</category></categories><tags><tag>数据库</tag><tag>PostgreSQL</tag><tag>数据库</tag><tag>SQL</tag></tags><content type="html"> PostgreSQL查询出所有表的记录数
项目所有用的数据库从SQLServer 换成PostgreSQL,项目中很多sql 是针对SQLServer 写的，所以不得不从新写SQL，项目中有一个功能是要统计出数据库的情况，包括所有表的记录数。对数据库不太熟悉，找了半天，大致还是要从系统表pg_class上入手。
有关pg_class字段介绍：https://wizardforcel.gitbooks.io/postgresql-doc/content/714.html
查询出pg_class表中的reltuples就是表的记录数： select relname as TABLE_NAME, reltuples as rowCounts from pg_class where relkind = &lsquo;r&rsquo; order by rowCounts desc
这样查出来的有一个问题，就是会把系统表的数据也查出来，这显然不是我想要的。怎么去掉系统表。
可以查询Schema下的每张表的记录数 select relname as TABLE_NAME, reltuples as rowCounts from pg_class where relkind = &lsquo;r&rsquo; and relnamespace = (select oid from pg_namespace where nspname=&lsquo;public&rsquo;) order by rowCounts desc;</content></entry><entry><title>线性表的顺序存储—顺序表</title><url>/post/%E7%BA%BF%E6%80%A7%E8%A1%A8%E7%9A%84%E9%A1%BA%E5%BA%8F%E5%AD%98%E5%82%A8%E9%A1%BA%E5%BA%8F%E8%A1%A8/</url><categories><category>数据结构与算法</category><category>数据结构</category><category>c</category><category>线性表</category><category>顺序表</category></categories><tags><tag>数据结构与算法</tag><tag>数据结构</tag><tag>c</tag><tag>线性表</tag><tag>顺序表</tag></tags><content type="html"> 线性表的顺序存储—顺序表
线性表的顺序存储结构：把线性表中的所有元素按照其逻辑顺序依次存储到从计算机存储器中指定存储位置开始的一块连续的存储空间中。 这样,线性表中第一个元素的存储位置就是指定的存储位置，第i+1个元素（1≤i≤n-1）的存储位置紧接在第i个元素的存储位置的后面。 说明：由于C中数组的下标从0开始，线性表的第i个元素ai存放顺序表的第i-1位置上。为了清楚，将ai在逻辑序列中的位置称为逻辑位序，在顺序表中的位置称为物理位序。 线性表&lt;&mdash;-> 逻辑结构 顺序表 &lt;&mdash;> 存储结构
优点： 无须为表示表中元素之间的逻辑关系而增加额外的存储空间。 可以快速地存取表中任意位置的元素。
缺点： 插入和删除操作需要移动大量元素。 当线性表长度变化较大时，难以确定存储空间的容量。 容易造成存储空间的“碎片”。
#define LIST_INIT_SIZE 100 // 线性表存储空间的初始分配量 #define LISTINCREMENT 10 // 线性表存储空间的分配增量 #define OK 0 #define ERROR 1 #define OVERFLOW 0 typedef int ElemType;
/** 线性表的动态分配顺序存储结构 */ typedef struct { ElemType *elem; // 存储空间基址 int length; // 当前长度 int listsize; //当前分配的存储容量（以sizeof(ElemType)为单位） }SqList;
/*************************************************
Function: InitList
Description: 初始化空间
param: L SqList* 顺序表
Return: int
Others: 时间复杂度为O(1) **/ int InitList(SqList L) { L->elem = (ElemType )malloc(LIST_INIT_SIZE * sizeof(ElemType)); if(!L->elem) exit(OVERFLOW); //存储分配失败 L->length = 0; // 空表长度为0 L->listsize = LIST_INIT_SIZE; // 初始存储容量 return OK; } /
Function: AddListSize
Description: 为顺序表增加空间
param: L SqList* 顺序表
Return: int
Others: 时间复杂度为O(1) ***/ int AddListSize(SqList L) { ElemType newbase = (ElemType )realloc(L->elem,(L->listsize + LISTINCREMENT) * sizeof(ElemType)); if(!newbase) { exit(OVERFLOW); } L->elem = newbase; L->listsize += LISTINCREMENT; return OK; } /
Function: CreateList
Description: 为顺序表赋值
param: L SqList* 顺序表 a[] ElemType 数组 n int 长度
Others: 时间复杂度为O(n) *************************************************/ void CreateList(SqList *L, ElemType a[], int n) { int i;
/** 如果顺序表没有容量，初始化容量 */ if(L->listsize == 0) { InitList(L); //调用初始化函数 }
/** 如果顺序表的容量小于长度，增加容量 */ if(L->listsize &lt; n) { AddListSize(L); // 调用增加容量函数 }
/** 顺序表赋值 */ for(i = 0; i &lt; n; i++) { L->elem[i] = a[i]; } L->length = n; // 线性表长度赋值 }
/*************************************************
Function: ListInSert
Description: 在顺序表中第i个位置前插入新元素e，
param: L SqList* 顺序表
i int 插入的位置 e ElemType 插入的元素 Return: int
Others: 时间复杂度为O(n) *************************************************/ int ListInSert(SqList *L, int i, ElemType e) { int k;
/** 如果顺序表的长度大于或等于现象表的容量，增加容量 */ if(L->length >= L->listsize) { AddListSize(L); // 调用增加容量函数 }
/** 位置应该从1开始到表长+1，位置i如果小于1或者大于顺序表的长度，函数结束 */ if(i &lt; 1 || i > L->length+1) { return ERROR; }
/** 插入位置以及之后的元素后移 */ for(k = L->length-1; k >= i-1; k&ndash;) // { L->elem[k+1] = L->elem[k]; } L->elem[i-1] = e; // 插入元素 L->length +=1; //表长加1 return OK; }
/*************************************************
Function: ListDelete
Description: 删除顺序表中第i个位置的元素并将删除的元素赋值给e
param: L SqList* 顺序表
i int 删除的位置 e ElemType 删除的元素 Return: int
Others: 时间复杂度为O(n) *************************************************/ int ListDelete(SqList *L, int i, ElemType *e) { int k;
/** 如果顺序表的长度为零，顺序表中没有元素，函数结束 */ if(L->length == 0) { return ERROR; }
/** 位置应该从1开始到表长+1，位置i如果小于1或者大于顺序表的长度，函数结束 */ if(i &lt; 1 || i > L->length) { return ERROR; }
*e = L->elem[i-1]; // 将删除的元素给e
/** 删除位置之后的元素前移 */ for(k = i; k &lt; L->length; k++) { L->elem[k-1] = L->elem[k]; } L->length&ndash;; // 线性表的长度-1 return OK; }
/*************************************************
Function: DestroyList Description: 删除顺序表,释放存储空间 param: L SqList* 顺序表 Return: int Others: 时间复杂度为O(1) *************************************************/ void DestroyList(SqList *L) { L->length = 0; // 长度0 L->listsize = 0; // 容量0 free(L); // 释放空间 } /*************************************************
Function: ClearList Description: 清空顺序表中的元素 param: L SqList* 顺序表 Return: int Others: 时间复杂度为O(1) *************************************************/ int ClearList(SqList *L) { L->length = 0; // 线性表的长度设置为0 return OK; } /*************************************************
Function: ListEmpty Description: 顺序表是否为空 param: L SqList* 顺序表 Return: int 0代表顺序表为空，1代表顺序表不为空 Others: 时间复杂度为O(1) *************************************************/ int ListEmpty(SqList *L) { if(L->length >0) { return ERROR; } else { return OK; } } /*************************************************
Function: ListLength Description: 顺序表的长度 param: L SqList* 顺序表 Return: int 顺序表的长度 Others: 时间复杂度为O(1) *************************************************/ int ListLength(SqList *L) { return L->length; } /*************************************************
Function: GetElem Description: 获取顺序表中第i个位置的元素，并赋值给e param: L SqList* 顺序表 i int 元素的位置 e ElemType 元素 Return: int Others: 时间复杂度为O(1) *************************************************/ int GetElem(SqList L,int i, ElemType e) { / 位置应该从1开始到表长+1，位置i如果小于1或者大于顺序表的长度，函数结束 */ if(i &lt;1 || i > L->length) { return ERROR; } *e = L->elem[i-1]; // 逻辑位置i 物理位置应为i-1 return OK; } /*************************************************
Function: LocateElem
Description: 获取顺序表中元素所在的位置
param: L SqList* 顺序表
e ElemType 元素 Return: int 元素所在的位置
Others: 时间复杂度为O(n) *************************************************/ int LocateElem(SqList *L, ElemType e) { int i = 0;
/** 循环顺序表的长度，找出元素。 */ while(i &lt; L->length &amp;&amp; L->elem[i] != e) { i++; }
/** 循环结束后，i大于或等于顺序表的长度，说明没有找到，函数结束 */ if(i >= L->length) { return ERROR; }
return i+1; // 逻辑位置i从1开始，物理位置从0开始，返回逻辑位置。 }
/*************************************************
Function: DispList
Description: 输出显示显示顺序表
param: L SqList* 顺序表
Others: 时间复杂度为O(n) *************************************************/ void DispList(SqList *L) { int i;
/** 如果顺序表的长度为零，顺序表中没有元素，函数结束 */ if(L->length == 0) { return ERROR; }
/** 循环顺序表，输出元素。 */ for(i = 0; i &lt; L->length; i++) { printf("%d,",L->elem[i]); } printf("\n"); }
/*************************************************
Function: unionList
Description: 合并两个顺序表，将说有在顺序表LB中但不在LA中的元素插入到LA中
param: LA SqList* 顺序表
param: LB SqList* 顺序表
Others: 时间复杂度为O(ListLength(LA)*ListLength(LB)) *************************************************/ void unionList(SqList *LA, SqList *LB) { int lalen = LA->length; // 获得顺序表LA的长度 int lblen = LB->length; // 获取顺序表LB的长度 int i; ElemType e;
/** 循环顺序表LB 找出在LB中但不在LA中的元素插入到LA中 */ for(i = 1; i &lt;= lblen; i++) { GetElem(LB,i,e); // 调用GetElem函数，取LB中第i个数据元素赋给e
/** LA中不存在和e相同者,插入到LA中 */ if(!LocateElem(LA,e)) { ListInSert(LA,++lalen,e); } } }
/*************************************************
Function: Inverse Description: 顺序表的逆置 param: L SqList* 顺序表 Others: 时间复杂度为O(n) *************************************************/ void Inverse(SqList *L) { int low=0,high=L->length-1; ElemType temp; int i; for(i=0; ilength/2; i++) { temp=L->elem[low]; L->elem[low++]=L->elem[high]; L->elem[high&ndash;]=temp; } } int main() { SqList L; int length,a,i,pos; ElemType temp; InitList(&amp;L); printf(&ldquo;请先初始化顺序表\n&rdquo;); printf(&ldquo;输入顺序表的长度:&rdquo;); scanf("%d",&amp;length); printf(&ldquo;输入顺序表的元素:&rdquo;); for(i=0; i&lt;length; i++) { scanf("%d",&amp;temp); ListInSert(&amp;L,i+1,temp); } printf(&ldquo;创建好的顺序表L=&rdquo;); DispList(&amp;L); while(1) { printf(&ldquo;功能：\n&rdquo;); printf("\t【1】显示顺序表元素\n\t【2】插入元素\n\t【3】删除元素\n\t【4】查找元素\n\t【5】显示顺序表长度\n\t【6】倒置顺序表\n\t【0】退出\n"); printf(&ldquo;请选择功能：&rdquo;); scanf("%d",&amp;a); if(a == 0) { return 0; } else if(a == 1) { printf(&ldquo;创建好的顺序表La=&rdquo;); DispList(&amp;L); } else if(a == 2) { printf(&ldquo;请输入插入位置：&rdquo;); scanf("%d",&amp;pos); printf(&ldquo;请输入插入元素：&rdquo;); scanf("%d",&amp;temp); int is = ListInSert(&amp;L,pos,temp); if( is == 0) { printf(&ldquo;插入成功！\n&rdquo;); } else { printf(&ldquo;插入失败！\n&rdquo;); } } else if(a == 3) { printf(&ldquo;请输入删除元素的位置：&rdquo;); scanf("%d",&amp;pos); int is = ListDelete(&amp;L, pos, &amp;temp); if(is == 0) { printf(&ldquo;删除的元素为%d！\n&rdquo;,temp); } else { printf(&ldquo;删除失败！\n&rdquo;); } } else if(a == 4) { printf(&ldquo;请输入要查找的元素：&rdquo;); scanf("%d",&amp;temp); printf(&ldquo;要查找的元素在：%d\n&rdquo;,LocateElem(&amp;L,temp)); } else if(a == 5) { printf(&ldquo;顺序表的长度为：%d\n&rdquo;,ListLength(&amp;L)); } else if(a == 6) { Inverse(&amp;L); printf(&ldquo;倒置后的顺序表L=&rdquo;); DispList(&amp;L); } } return 0; }</content></entry><entry><title>Java SPI</title><url>/post/java-spi/</url><categories><category>Java</category><category>Java Core</category></categories><tags><tag>Java</tag><tag>Java Core</tag></tags><content type="html"> SPI之ServiceLoader应用与源码分析
在SPI的应用中，Java支持对外开放了一些服务接口（或者抽象类，也可以使用普通类，但是不建议），但是不提供具体的实现，这些实现可以由第三方提供，支持用户以扩展的方式集成到系统中。那么这就需要一种服务发现机制。也就是根据一定的规则到指定的位置找到指定服务的配置文件，然后找到实现类。ServiceLoader便提供了这么一个手段，能够在系统中"指定"位置（META-INF/services）的"指定"文件（文件名是服务全限定名称）中寻找指定服务的三方实现。
 比如在servlet容器中，需要支持servlet规范中对于ServletContainerInitializer的使用定义，会在容器启动的时候寻找ServletContainerInitializer的实现类，调用其onStartup方法。那么可以这样寻找其实现类：
ServiceLoader&lt;ServletContainerInitializer> loadedInitializers = ServiceLoader.load(ServletContainerInitializer.class); for (ServletContainerInitializer sci:loadedInitializers){ //do something...... }  ServiceLoader的使用方式一般就是先通过load方法得到ServiceLoader实例，然后迭代获取每个实现。我们知道对于增强的for()循环，在编译之后会转变成对应迭代器遍历的字节码实现，所以ServiceLoader需要实现迭代器Iterable接口，事实上也是这样：
public final class ServiceLoader&lt;S> implements Iterable&lt;S>{ ...... }  接下来我们看看load方法是如何实现的：
public static &lt;S> ServiceLoader&lt;S> load(Class&lt;S> service) { //使用TCCL类加载器 ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); }  SPI使用的是TCCL类加载器，关于TCCL，在 深入OpenJDK源码全面理解Java类加载器
进行过介绍，这里不再多聊。之所以使用TCCL是因为ServiceLoader定义在核心包中，会被BootStrapClassLoader加载，但是服务实现者却定义在应用jar包中，BootStrapClassLoader无法加载，那根据Java类加载的双亲委派机制，拿这些实现类还没有办法，所以需要通过TCCL的手段实现父类加载器调用子类加载器加载类的需求。 获取了TCCL之后，我们进入ServiceLoader.load(service, cl)方法调用逻辑：
public static &lt;S> ServiceLoader&lt;S> load(Class&lt;S> service, ClassLoader loader){ return new ServiceLoader&lt;>(service, loader); }  直接创建的ServiceLoader实例，那么转到ServiceLoader的构造函数：
private ServiceLoader(Class&lt;S> svc, ClassLoader cl) { //判空 service = Objects.requireNonNull(svc, "Service interface cannot be null"); //如果cl为空，那么从getSystemClassLoader获取，这个方法默认返回AppClassLoader loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; //安全检查 acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload(); }  经过简单的处理之后，会调用reload方法完成服务实现者的发现与加载动作(关于ClassLoader.getSystemClassLoader，在 深入OpenJDK源码全面理解Java类加载器
中有详细分析)，reload方法实现如下：
public void reload() { //清理providers，以便于重新加载服务提供者 providers.clear(); //创建一个LazyIterator lookupIterator = new LazyIterator(service, loader); }  主要根据service（服务）和loader（类加载器）创建了一个LazyIterator，这个LazyIterator看名字就知道是一个懒加载的迭代器，是ServiceLoader的一个私有内部类。前文提到了ServiceLoader实现了Iterable接口，那么我们看看它的iterator();方法是如何实现的：
public Iterator&lt;S> iterator() { return new Iterator&lt;S>() { //providers在reload的时候会清空 Iterator&lt;Map.Entry&lt;String,S>> knownProviders = providers.entrySet().iterator(); public boolean hasNext() { if (knownProviders.hasNext()) return true; return lookupIterator.hasNext(); } public S next() { if (knownProviders.hasNext()) return knownProviders.next().getValue(); return lookupIterator.next(); } public void remove() { throw new UnsupportedOperationException(); } }; }  可以看到，reload之后调用hasNext和next方法实际调用的是lookupIterator，也就是reload方法创建的LazyIterator。所以我们转到LazyIterator对应方法的实现（其中的next方法会调用nextService方法，hasNext方法会调用hasNextService方法，所以这里关注nextService和hasNextService方法就可以了）：
private class LazyIterator implements Iterator&lt;S>{ Class&lt;S> service; ClassLoader loader; Enumeration&lt;URL> configs = null; Iterator&lt;String> pending = null; String nextName = null; private boolean hasNextService() { if (nextName != null) { return true; } if (configs == null) { try { //PREFIX是"META-INF/services/" String fullName = PREFIX + service.getName(); //调用的ClassLoader.getResources方法寻找资源 if (loader == null) configs = ClassLoader.getSystemResources(fullName); else configs = loader.getResources(fullName); } catch (IOException x) { fail(service, "Error locating configuration files", x); } } while ((pending == null) || !pending.hasNext()) { if (!configs.hasMoreElements()) { return false; } //解析找到的资源（URL） pending = parse(service, configs.nextElement()); } //保存到nextName，在nextService方法中加载与实例化 nextName = pending.next(); return true; } private S nextService() { if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?> c = null; try { //加载找到的服务实现 c = Class.forName(cn, false, loader); } catch (ClassNotFoundException x) { fail(service, "Provider " + cn + " not found"); } if (!service.isAssignableFrom(c)) { fail(service, "Provider " + cn + " not a subtype"); } try { //实例化 S p = service.cast(c.newInstance()); //存入providers中 providers.put(cn, p); return p; } catch (Throwable x) { fail(service, "Provider " + cn + " could not be instantiated", x); } throw new Error(); // This cannot happen } }  代码看起来不少，不过核心逻辑就几点：首先在hasNextService方法中根据PREFIX 和serviceName组装一个fullName，对于ServletContainerInitializer的例子，fullName就是：META-INF/services/javax.servlet.ServletContainerInitializer。然后通过ClassLoader提供的getResources方法寻找实现类，最后在nextService方法中完成实现类的加载与实例化操作，还会存入providers中。 注意ClassLoader的getResources方法返回的是一个URL的枚举类（Enumeration，现在基本都使用迭代器了），会在parse方法中解析该URL：
private Iterator&lt;String> parse(Class&lt;?> service, URL u) throws ServiceConfigurationError { InputStream in = null; BufferedReader r = null; ArrayList&lt;String> names = new ArrayList&lt;>(); try { in = u.openStream(); r = new BufferedReader(new InputStreamReader(in, "utf-8")); int lc = 1; while ((lc = parseLine(service, u, r, lc, names)) >= 0); } catch (IOException x) { fail(service, "Error reading configuration file", x); } finally { try { if (r != null) r.close(); if (in != null) in.close(); } catch (IOException y) { fail(service, "Error closing configuration file", y); } } return names.iterator(); }  这个方法逻辑很简单，就不进行说明了，需要注意的是，一个META-INF/services/{serviceName}文件中可以有多个实现类，parse方法会将解析到的全限定类名都存入names集合中，然后返回其迭代器，交给后面的逻辑迭代加载和实例化。 这里还需要提一点的就是ClassLoader的getResources方法，该方法里会调用findResources方法，这个方法在ClassLoader中返回的是一个空集合：
protected Enumeration&lt;URL> findResources(String name) throws IOException { return java.util.Collections.emptyEnumeration(); }  所以需要子类去实现，ExtClassLoader和AppClassLoader都继承了URLClassLoader，在URLClassLoader中有该方法的具体实现，最终是通过URLClassPath完成的资源查找（支持网络字节码和本地文件字节码），最终返回URL枚举类。
 ServiceLoader主要依赖一个懒加载迭代器LazyIterator，LazyIterator生成后会在对ServiceLoader进行迭代的时候才进行资源的搜索和解析工作，工作流程总体说来如下：
根据传入的服务名生成该服务按照规范应该所在的文件名，比如：META-INF/services/javax.servlet.ServletContainerInitializer 使用指定的TCCL类加载器(如果为空则是AppClassLoader)寻找对应的资源列表，以枚举(Enumeration)的方式返回 然后遍历找到的资源文件，解析内容得到所有配置的服务实现类 对这些实现类完成加载和实例化的工作，返回实现类实例对象，同时存入ServiceLoader的providers属性中（一个LinkedHashMap，key为name，value为对应的实例对象）</content></entry><entry><title>Java程序使用jPowershell与powershell交互</title><url>/post/java%E7%A8%8B%E5%BA%8F%E4%BD%BF%E7%94%A8jpowershell%E4%B8%8Epowershell%E4%BA%A4%E4%BA%92/</url><categories><category>Java</category><category>Windows</category><category>Powershell</category></categories><tags><tag>Java</tag><tag>Windows</tag><tag>Powershell</tag></tags><content type="html"> Java程序使用jPowershell与powershell交互
一、java自带的Runtime 在某些场景下，需要在Java程序中使用Powershell进行终端交互，这种情况下当然可以直接使用自带的Runtime来完成：Runtime.getRuntime().exec(“powershell.exe Get-Item”); 但是这种只适合需要单条指令的情况，而存在多条指令时，无法保证前后指令的关联性。这里介绍一个第三方的类库：jPowershell
二、jPowershell 1、 Maven依赖： &lt;dependency> &lt;groupId>com.profesorfalken&lt;/groupId> &lt;artifactId>jPowerShell&lt;/artifactId> &lt;version>3.1.1&lt;/version> &lt;/dependency> 2、 程序代码 获取Powershell的实例：PowerShell session = PowerShell.openSession(); PowerShell对象提供了如下的基础方法： configuration(Map&lt;String,String> arg0)：指定配置对象 void 修改默认配置时使用 openSession()：PowerShell 启动一个PowerShell会话，在一个会话内可以与powershell交互多次命令 openSession(String arg0)：指定PowerShell终端的路径 PowerShell 启动一个PowerShell会话 executeCommand(String arg0)：Powershell指令，返回值PowerShellResponse是执行结果和信息 executeSingleCommand()：执行单一PowerShell指令（无会话模式） executeCommandAndChain(String arg0,PowerShellResponseHandler.. arg1)：PowerShell指令，Response处理器 PowerShell 执行一条PowerShell指令并对其Response进行处理 handleResponse(PowerShellResponseHandler arg0,PowerShellResponse arg1)：Response处理器，Response void 使用指定Response处理器处理响应 isLastCommandInError()：boolean 最后一条指令是否出错 executeScript(String arg0)：脚本路径 PowerShellResponse 执行指定的脚本 executeScript(String arg0, String arg1)：脚本路径，执行参数 PowerShellResponse 传入参数执行指定脚本 executeScript(BufferedReader arg0)：脚本缓冲对象 PowerShellResponse 从缓冲对象中执行脚本 executeScript(BufferedReader arg0, String arg1)：脚本缓冲对象，执行参数PowerShellResponse 传入参数执行缓冲对象中的脚本 close() ： void 关闭会话 closeAndWait(Future&lt;String> arg0)： 异步对象列表 boolean 等待异步对象列表中全部Task完成后关闭会话 checkState() ：void 检查当前会话状态 获取到的session对象是一个新的Powershell会话实例，可以通过下述方式来执行指令：
PowerShell session = PowerShell.openSession(); String readContext = "Get-Content D:\\1.sql"; session.executeCommand("$user = \""+username+"\""); session.executeCommand("$password = ConvertTo-SecureString \"" +password + "\" -AsPlainText -Force"); session.executeCommand("$credential = New-Object System.Management.Automation.PSCredential($user,$password)"); response = session.executeCommand("Invoke-Command -ComputerName " + server + " -Credential $credential -ScriptBlock {" + readContext + "}"); System.out.println(response.getCommandOutput().length()); 上述代码中，首先定义了两个shell变量，u s e r 和 user和 user和password，而后通过调用System.Management.Automation.PSCredential构造了一个$credential对象，再将其作为-Credential的值传入下一条指令中，完成远程Invoke-Command进行文件内容长度的读取。
每一条session.executeCommand()都会返回一个PowerShellResponse对象，可以通过调用这个对象的getCommandOutput()方法来获取命令输出。注意看日志最后的数字：
当然，在某些情况下，调用的指令执行时间会很长，在默认情况下，Powershell实例只会等待10秒钟，如果10秒钟之后还没有返回值，就直接跳到下一条继续执行了。这种情况下，可以通过配置Powershell的实例来修改等待时间。来看源码：
public class PowerShell implements AutoCloseable { private static final Logger logger = Logger.getLogger(PowerShell.class.getName()); private Process p; private long pid = -1L; private PrintWriter commandWriter; private boolean closed = false; private ExecutorService threadpool; private static final String DEFAULT_WIN_EXECUTABLE = "powershell.exe"; private static final String DEFAULT_LINUX_EXECUTABLE = "powershell"; private int waitPause = 5; private long maxWait = 10000L; private File tempFolder = null; private boolean scriptMode = false; public static final String END_SCRIPT_STRING = "--END-JPOWERSHELL-SCRIPT--"; private PowerShell() { } public PowerShell configuration(Map&lt;String, String> config) { try { this.waitPause = Integer.valueOf(config != null &amp;&amp; config.get("waitPause") != null ? (String)config.get("waitPause") : PowerShellConfig.getConfig().getProperty("waitPause")); this.maxWait = Long.valueOf(config != null &amp;&amp; config.get("maxWait") != null ? (String)config.get("maxWait") : PowerShellConfig.getConfig().getProperty("maxWait")); this.tempFolder = config != null &amp;&amp; config.get("tempFolder") != null ? this.getTempFolder((String)config.get("tempFolder")) : this.getTempFolder(PowerShellConfig.getConfig().getProperty("tempFolder")); } catch (NumberFormatException var3) { logger.log(Level.SEVERE, "Could not read configuration. Using default values.", var3); } return this; } } Powershell提供了一个configuration()方法，这个方法接受一个Map类型的配置参数，并将内容解析后代替默认参数。我们主要关注的是maxWait的值。通过成员定义可以看出，默认的maxWait的值就是10000毫秒，也就是10秒。因此，可以通过如下代码来使其支持更长时间指令的调用：
PowerShell session = PowerShell.openSession(); Map&lt;String,String> configMap = new HashMap&lt;>(); configMap.put("maxWait","600000"); session.configuration(configMap); 这样就把指令的最长等待时间设置为了600秒。</content></entry><entry><title>Maven工程打jar包的N种方式</title><url>/post/maven%E5%B7%A5%E7%A8%8B%E6%89%93jar%E5%8C%85%E7%9A%84n%E7%A7%8D%E6%96%B9%E5%BC%8F/</url><categories><category>实用技巧</category><category>maven</category><category>jar</category><category>springboot</category></categories><tags><tag>实用技巧</tag><tag>maven</tag><tag>jar</tag><tag>springboot</tag></tags><content type="html"> Maven工程打jar包的N种方式
Maven工程打jar包 一、IDEA自带打包插件 二、maven插件打包 2.1 制作瘦包（直接打包，不打包依赖包） 2.2 制作瘦包和依赖包（相互分离） 2.3 制作胖包（项目依赖包和项目打为一个包） 2.4 制作胖包（transform部分自定义） 三、SpringBoot项目打包 四、Scala项目打包 五、groovy项目打包
内容：此种方式可以自己选择制作胖包或者瘦包，但推荐此种方式制作瘦包。 输出：输出目录在out目录下 流程步骤：
第一步： 依次选择 file->projecct structure->artifacts->点击+ (选择jar)->选择 from module with dependencies 第二步：弹出窗口中指定Main Class，是否选择依赖jar包，是否包含测试。（尽量不选依赖包，防止依赖包选择不全） ![img](https://img-blog.csdnimg.cn/20201127122706462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center)![img](https://img-blog.csdnimg.cn/20201127122723284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center)![img](https://img-blog.csdnimg.cn/20201127122732840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center)![img](https://img-blog.csdnimg.cn/20201127122750258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0h1Z2hfR3Vhbg==,size_16,color_FFFFFF,t_70#pic_center) 3. 第三步：点击Build–>Build Artifacts–>选择bulid 输出：输出目录在target目录下
2.1 制作瘦包（直接打包，不打包依赖包） 内容：仅打包出项目中的代码到JAR包中。 方式：在pom.xml中添加如下plugin; 随后执行maven install
&lt;!-- java编译插件 --> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-compiler-plugin&lt;/artifactId> &lt;version>指定版本&lt;/version> &lt;configuration> &lt;source>1.8&lt;/source> &lt;target>1.8&lt;/target> &lt;encoding>UTF-8&lt;/encoding> &lt;/configuration> &lt;/plugin> 2.2 制作瘦包和依赖包（相互分离） 内容：将依赖JAR包输出到lib目录方式（打包方式对于JAVA项目是通用的） 将项目中的JAR包的依赖包输出到指定的目录下,修改outputDirectory配置，如下面的${project.build.directory}/lib。 方式：
pom.xml的build>plugins中添加如下配置。 点击maven project（右边栏）->选择Lifecycle->点击package打包 注意：如果想将打包好的JAR包通过命令直接运行，如java -jar xx.jar。需要制定manifest配置的classpathPrefix与上面配置的相对应。如上面把依赖JAR包输出到了lib，则这里的classpathPrefix也应指定为lib/；同时，并指定出程序的入口类，在配置mainClass节点中配好入口类的全类名。 &lt;plugins> &lt;!-- java编译插件 --> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-compiler-plugin&lt;/artifactId> &lt;configuration> &lt;source>1.8&lt;/source> &lt;target>1.8&lt;/target> &lt;encoding>UTF-8&lt;/encoding> &lt;/configuration> &lt;/plugin> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-jar-plugin&lt;/artifactId> &lt;configuration> &lt;archive> &lt;manifest> &lt;addClasspath>true&lt;/addClasspath> &lt;classpathPrefix>lib/&lt;/classpathPrefix> &lt;mainClass>com.yourpakagename.mainClassName&lt;/mainClass> &lt;/manifest> &lt;/archive> &lt;/configuration> &lt;/plugin> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-dependency-plugin&lt;/artifactId> &lt;executions> &lt;execution> &lt;id>copy&lt;/id> &lt;phase>install&lt;/phase> &lt;goals> &lt;goal>copy-dependencies&lt;/goal> &lt;/goals> &lt;configuration> &lt;outputDirectory>${ project.build.directory}/lib&lt;/outputDirectory> &lt;/configuration> &lt;/execution> &lt;/executions> &lt;/plugin> &lt;/plugins> 注意：默认的classpath会在jar包内。为了方便,可以在Main方法配置后加上manifestEntries配置，指定classpath。
&lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-jar-plugin&lt;/artifactId> &lt;configuration> &lt;classesDirectory>target/classes/&lt;/classesDirectory> &lt;archive> &lt;manifest> &lt;!-- 主函数的入口 --> &lt;mainClass>com.yourpakagename.mainClassName&lt;/mainClass> &lt;!-- 打包时 MANIFEST.MF文件不记录的时间戳版本 --> &lt;useUniqueVersions>false&lt;/useUniqueVersions> &lt;addClasspath>true&lt;/addClasspath> &lt;classpathPrefix>lib/&lt;/classpathPrefix> &lt;/manifest> &lt;manifestEntries> &lt;Class-Path>.&lt;/Class-Path> &lt;/manifestEntries> &lt;/archive> &lt;/configuration> &lt;/plugin> 2.3 制作胖包（项目依赖包和项目打为一个包） 内容：将项目中的依赖包和项目代码都打为一个JAR包 方式：
pom.xml的build>plugins中添加如下配置； 点击maven project（右边栏）->选择Plugins->选择assembly->点击assembly:assembly 注意：1. 针对传统的JAVA项目打包； 2. 打包指令为插件的assembly命令，尽量不用package指令。 &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-assembly-plugin&lt;/artifactId> &lt;version>2.5.5&lt;/version> &lt;configuration> &lt;archive> &lt;manifest> &lt;mainClass>com.xxg.Main&lt;/mainClass> &lt;/manifest> &lt;/archive> &lt;descriptorRefs> &lt;descriptorRef>jar-with-dependencies&lt;/descriptorRef> &lt;/descriptorRefs> &lt;/configuration> &lt;/plugin> 2.4 制作胖包（transform部分自定义） &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-shade-plugin&lt;/artifactId> &lt;version>2.4.3&lt;/version> &lt;executions> &lt;execution> &lt;phase>package&lt;/phase> &lt;goals> &lt;goal>shade&lt;/goal> &lt;/goals> &lt;configuration> &lt;filters> &lt;filter> &lt;artifact>*:*&lt;/artifact> &lt;excludes> &lt;exclude>META-INF/*.SF&lt;/exclude> &lt;exclude>META-INF/*.DSA&lt;/exclude> &lt;exclude>META-INF/*.RSA&lt;/exclude> &lt;/excludes> &lt;/filter> &lt;/filters> &lt;transformers> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"> &lt;resource>META-INF/spring.handlers&lt;/resource> &lt;/transformer> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"> &lt;resource>META-INF/spring.schemas&lt;/resource> &lt;/transformer> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"> &lt;resource>META-INF/spring.tooling&lt;/resource> &lt;/transformer> &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"> &lt;mainClass>com.xxx.xxxInvoke&lt;/mainClass> &lt;/transformer> &lt;/transformers> &lt;minimizeJar>true&lt;/minimizeJar> &lt;shadedArtifactAttached>true&lt;/shadedArtifactAttached> &lt;/configuration> &lt;/execution> &lt;/executions> &lt;/plugin> 内容：将当前项目里所有依赖包和当前项目的源码都打成一个JAR包，同时还会将没有依赖包的JAR包也打出来，以.original保存 方式：
在pom.xml的build>plugins中加入如下配置 点击maven project（右边栏）->选择Lifecycle->点击package或install打包 &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> 方式：
在pom.xml的build>plugins中加入如下配置 点击maven project（右边栏）->选择Lifecycle->点击package或install打包 &lt;plugin> &lt;groupId>org.scala-tools&lt;/groupId> &lt;artifactId>maven-scala-plugin&lt;/artifactId> &lt;executions> &lt;execution> &lt;goals> &lt;goal>compile&lt;/goal> &lt;goal>testCompile&lt;/goal> &lt;/goals> &lt;/execution> &lt;/executions> &lt;configuration> &lt;scalaVersion>${ scala.version}&lt;/scalaVersion> &lt;args> &lt;arg>-target:jvm-1.5&lt;/arg> &lt;/args> &lt;/configuration> &lt;/plugin> 方式：
在pom.xml的build>plugins中加入如下配置 点击maven project（右边栏）->选择Lifecycle->点击package或install打包 &lt;plugin> &lt;groupId>org.codehaus.gmavenplus&lt;/groupId> &lt;artifactId>gmavenplus-plugin&lt;/artifactId> &lt;version>1.2&lt;/version> &lt;executions> &lt;execution> &lt;goals> &lt;goal>addSources&lt;/goal> &lt;goal>addStubSources&lt;/goal> &lt;goal>compile&lt;/goal> &lt;goal>execute&lt;/goal> &lt;/goals> &lt;/execution> &lt;/executions> &lt;/plugin></content></entry><entry><title>MySQL总结--MVCC（read view和undo log）</title><url>/post/mysql%E6%80%BB%E7%BB%93--mvccread-view%E5%92%8Cundo-log/</url><categories><category>Mysql</category><category>MVCC</category></categories><tags><tag>Mysql</tag><tag>MVCC</tag></tags><content type="html"> MySQL总结&ndash;MVCC（read view和undo log）
MVCC(Multi-Version Concurrency Control)，即多版本并发控制，数据库通过它能够做到遇到并发读写的时候，在不加锁的前提下实现安全的并发读操作，是一种乐观锁的实现方式，能大大提高数据库的并发性能。
当前读：读取的是记录的最新版本，需要保证其它事务不能修改读取记录，所以会对记录进行加锁。比如 select for update、select lock in share mode、update等，都属于当前读。 快照读：基于MVCC实现的读，不对读操作加任何锁，读取的时候根据版本链和read view进行可见性判断，所以读取的数据不一定是数据库中的最新值。注意在串行化隔离级别下，读操作也会加锁，所以属于当前读。 undo log日志版本链  undo log是一种逻辑日志，当一个事务对记录做了变更操作就会产生undo log，也就是说undo log记录了记录变更的逻辑过程。当一个事务要更新一行记录时，会把当前记录当做历史快照保存下来，多个历史快照会用两个隐藏字段trx_id和roll_pointer串起来（关于隐藏字段，这里不用考虑隐式主键id:DB_ROW_ID），形成一个历史版本链。可以用于MVCC和事务回滚。 比如多个事务对id为1的数据做了更新，会形成如下图这种历史版本链： 注：更多关于undo log的内容，在MySQL–buffer pool、redo log、undo log、binlog中进行了较为详细的介绍，这里不再赘述。
read view（读视图）与可见性判断  在MySQL中，一个事务开启(注意这里说的不是传统意义上的开启，在InnoDB中，begin/start一个事务，并不会立即分配事务id，而是真正执行了操作才会分配事务id)的时候会被分配一个递增的ID。在事务执行快照读的时候，会将此时数据库中所有的活跃事务（未提交事务）id列表和已创建的最大事务id(+1)生成一个视图快照，如果是在可重复读隔离级别下，这个快照在此事务活跃期间不会变化，如果是读已提交隔离级别下，每次快照读都会重新生成。我们从read view中获取两个属性：
min_trx_id：read view生成时，活跃事务id列表中的最小id max_trx_id：read view生成时，数据库即将分配的事务id，也就是当前已创建最大事务id+1  一个事务在对一行数据做读取操作的时候，会从undo log历史版本链中从最新版本开始往前比对，通过一系列的规则，根据快照版本中的trx_id字段和read view来确定该版本对于当前事务是否可见，如果当前比对版本不可见，那么就通过roll_pointer找到上一个版本进行比对，直到找到可见版本或找不到任何一个可见版本。这些规则定义如下：
如果 trx_id &lt; min_trx_id，则说明该版本对于当前事务(read view)来说，是已提交事务生成的，那么对于当前事务可见。 如果trx_id >= max_trx_id：则说明该版本对于当前事务(read view)来说，是"将来"的事务生成的，那么对于当前事务不可见。 如果min_trx_id &lt;= trx_id &lt; max_trx_id： - 如果trx_id在read view的**活跃事务id列表**中，则说明该版本对于当前事务(read view)来说，是已开始但未提交的事务生成的，那么对于当前事务**不可见**。 - 如果trx_id不在read view的**活跃事务id列表**中，则说明该版本对于当前事务(read view)来说，是已提交的事务生成的，那么对于当前事务**可见**。 注：当前事务id(current_trx_id)也会在活跃事务id列表中，如果undo log是由当前事务生成的，也就是trx_id == current_trx_id，那么此版本对于当前事务来说当然可见
 另外，在前面undo log的文章中提到过，InnoDB对于删除操作，其实并不是直接删除数据，而是“相当于”一个update操作，也会生成一个对应删除事务的update undo log，只是将delete mark设置为1，之后会由purge线程清理。当根据上述规则比对时发现delete mark为1，就代表该记录已被删除，没有数据。
事务id和可见性  前面提到了，在InnoDB中，begin/start一个事务并不会立即分配事务id，而是真正执行了操作才会分配事务id。这会有什么现象呢？现假设事务A和事务B根据下图时间线执行： 虽然事务A先begin，但是它在执行do select的时候仍然能看到事务B提交的数据，因为事务在begin的时候并没有真正开始一个事务，事务A的read view是在do select的时候生成的，此时事务B对数据修改的版本快照按照规则来说就属于：trx_id &lt; min_trx_id，属于已提交事务生成，也就对于事务A来说可见。
参考：https://www.jianshu.com/p/8845ddca3b23</content></entry><entry><title>Spring源码-Bean生命周期总结</title><url>/post/spring%E6%BA%90%E7%A0%81bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%80%BB%E7%BB%93/</url><categories><category>Spring源码</category><category>Spring</category></categories><tags><tag>Spring源码</tag><tag>Spring</tag><tag>bean生命周期</tag></tags><content type="html"> Spring系列-Bean的生命周期
BeanFactory是Spring中的顶层接口，只负责管理bean，而ApplicationContext也实现了BeanFactory，提供了管理bean的功能，不过除了管理bean之外，它还提供了一些其它功能，比如依赖注入、事件发布等。只是ApplicationContext的bean管理功能也是通过BeanFactory提供的，在实现上就是ApplicationContext持有一个BeanFactory的引用，比如GenericApplicationContext中持有一个DefaultListableBeanFactory类型的beanFactory引用。
 在Spring容器中，一个Bean会首先被组装成一个BeanDefinition，然后将其注册到容器中，之后才会被实例化和初始化。Spring将这个过程细化成了很多步骤，在这些步骤中穿插了很多的后置处理器（PostProcessor）对各个环节进行拦截和扩展，以此实现了Spring极高的扩展性。 对于BeanDefinitionRegistry来说，它提供了注册BeanDefinition的接口，实现该接口表示了其具备注册BeanDefinition的能力。而BeanDefinitionReader的存在则为根据资源批量注册BeanDefinition提供了可能。从代码层面上看，就是BeanDefinitionReader能够通过loadBeanDefinitions方法根据一系列的Resource，创建BeanDefinition，然后通过BeanDefinitionRegistry进行注册。从面向对象的角度来看，就是BeanDefinitionReader依赖一些资源（Resource）创建BeanDefinition，并且依赖BeanDefinitionRegistry实现BeanDefinition的批量注册。 不过在Spring源码对于BeanDefinitionReader的注释中有明确的说明，BeanDefinitionReader虽然作为一个顶层接口提供了从资源批量创建注册BeanDefinition的接口。但是在具体的实现中，一个BeanDefinitionReader并不一定非要实现BeanDefinitionReader接口，而是在命名上遵循这个标准命名约定就行了，所以在实现上也可能并不是严格按照BeanDefinitionReader中定义的接口来进行BeanDefinition的注册的，Spring没有严格限制BeanDefinition注册的来源和方式，不论是对自己还是对开发者。 比如以我们最熟悉的AnnotatedBeanDefinitionReader来说，它是一个BeanDefinitionReader，但是实际上它并没有实现BeanDefinitionReader接口。在它的逻辑中，向BeanFactory注册了一些初始的PostProcessor，依赖这些PostProcessor来代替以前XML相关的操作。 比如在非Web环境下，我们如果要基于注解来使用Spring，则会创建一个AnnotationConfigApplicationContext，并且传入一个配置类。在AnnotationConfigApplicationContext的逻辑中就会创建一个前面提到的AnnotatedBeanDefinitionReader，而在AnnotatedBeanDefinitionReader初始化的过程中，会向容器注册了一个很重要的BeanDefinitionRegistryPostProcessor：ConfigurationClassPostProcessor。这个PostProcessor负责解析我们传入的配置类，比如解析@ComponentScan注解，然后根据ClassPathBeanDefinitionScanner从@ComponentScan注解配置的扫描包路径中扫描出合法的资源，最后创建BeanDefinition，注册到容器中，这个相当于是一个"另类"的BeanDefinition创建和注册逻辑，而这个逻辑会在后面的源码分析中详细总结。
概述  一个被Spring管理的Bean，在加载之后首先会被包装成一个BeanDefinition，BeanDefinition代表了Spring中一个Bean的各种基础属性（元数据），比如beanName、作用域、是否懒加载、所属类class、自动装配类型等等，同时还包含了初始化、依赖注入等需要的数据，Spring根据BeanDefinition来实例化和初始化bean。 类被包装成BeanDefinition之后，会存放在BeanFactory的一个ConcurrentHashMap结构的beanDefinitionMap中，key为bean的名称，value为BeanDefinition。Spring容器在刷新的时候，会首先把所有的BeanDefinition都创建缓存完毕，然后调用一系列的后置处理器对其进行加工，此处和BeanDefinition相关的后置处理器主要有两种，分别是：
BeanFactoryPostProcessor BeanDefinitionRegistryPostProcessor：继承了BeanFactoryPostProcessor  首先，通过BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法可以向BeanFactory中继续注册BeanDefinition，之后通过BeanFactoryPostProcessor可以将BeanFactory中的BeanDefinition中取出来做进一步的加工，当这两种后置处理器处理完成之后，会冻结所有已经注册的BeanDefinition的元数据，这表示这些已经注册的BeanDefinition的元数据信息不能再被修改，接下来就要开始实例化。 现在有了BeanDefinition，就代表有了创建Bean的模板，就可以开始创建非懒加载的单例Bean了，在创建Bean的过程中也有几个关键相关的后置处理器调用，分别是：
BeanPostProcessor MergedBeanDefinitionPostProcessor：继承了BeanPostProcessor InstantiationAwareBeanPostProcessor：继承了BeanPostProcessor  接下来进入创建bean的流程：循环BeanFactory中的BeanDefinitionMap，实例化所有单例bean，在这个过程中会先把beanName放入到一个Set中标识当前bean正在创建（此处涉及到循环依赖和三级缓存的问题，这个后面再进行详细说明），然后循环调用InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation方法（这里还涉及到AOP的部分实现，后面再详细说明）拦截bean的实例化，如果有某一个InstantiationAwareBeanPostProcessor返回了一个实例对象，那么就说明bean的实例化被拦截了，那么接着循环调用BeanPostProcessor的postProcessAfterInitialization方法，这时候代表实例对象创建成功，并且完成了初始化。 否则说明bean的实例化没有被拦截，那么根据BeanDefinition获取Bean的构造方式进行对象的实例化，这个实例化的过程很复杂，涉及到构造器推断等，这里面也涉及到后置处理器的调用，比如构造器函数上的@Autowired支持和方法上的@Lookup注解支持都是在这里实现的，涉及到的是AutowiredAnnotationBeanPostProcessor的determineCandidateConstructors方法调用。这个方法实际上归属于SmartInstantiationAwareBeanPostProcessor，包括三级缓存中的getEarlyBeanReference方法也属于这个后置处理器，它是后置处理器InstantiationAwareBeanPostProcessor的一个扩展。实例化之后会循环调用所有MergedBeanDefinitionPostProcessor的postProcessMergedBeanDefinition方法，在这个后置处理器中可以根据实例化之后的对象对BeanDefinition进行进一步的扩展， 这个行为发生在对象实例化之后，初始化之前，所以可以为后续的初始化操作准备一些必要的数据，同时也支持修改BeanDefinition中的部分数据，比如@Autowired注解支持的AutowiredAnnotationBeanPostProcessor后置处理器在这里对@Autowired进行预解析，找到并缓存需要注入的点。
注：此时生产出来的bean还只是一个"空的"，还没有进行任何初始化操作，称之为早期对象，会将其放入三级缓存中，这个概念在解决循环依赖的实现中有着重要作用，注意三级缓存存放的不是bean，而是一个函数接口~
 接下来对bean进行属性填充（populateBean），不过在填充之前，还会循环调用所有InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation，允许在Bean实例化之后对Bean的属性填充进行拦截，该方法返回值为布尔类型，如果有任何一个InstantiationAwareBeanPostProcessor的该方法处理bean的时候返回false，那么会中断后续的bean属性填充操作。通过这个后置处理器可以实现对bean中的字段做自定义填充，不过这个在日常工作中基本不会使用，在spring源码中实现该方法的类都默认返回的true。如果没有被postProcessAfterInstantiation方法拦截，那么就会进行bean的属性填充操作。属性填充的过程是先将需要的数据封装成一个PropertyValues，然后调用所有InstantiationAwareBeanPostProcessor的postProcessProperties方法和postProcessPropertyValues方法对PropertyValues在应用到bean之前进行进一步的加工和合法性判断等，然后将PropertyValues应用到bean中。 在bean的属性填充之后，就来到了真正开始正式初始化bean的环节，而在初始化bean之前，会先调用容器默认的3个Aware方法，分别是BeanNameAware、BeanClassLoaderAware和BeanFactoryAware，如果bean实现了这三个接口，那么会带着各自的参数调用它们各自的方法。接着调用所有BeanPostProcessor的postProcessBeforeInitialization方法，该方法允许对已经经过属性填充的bean进行进一步包装或者属性更新和扩展，方法返回值就是后续需要使用的bean，如果有任何一个处理器返回null，那么后续的处理器都不会被调用，比如@PostConstruct注解标识的初始化方法就是依赖该后置处理器实现的。接下来调用bean的初始化方法，bean的初始化方法除了刚刚提到的使用@PostConstruct注解标识外，还可以通过实现InitializingBean接口和xml中指定init-method方法，所以在通过后置处理器处理了@PostConstruct注解之后，就需要调用通过另外两种方式指定的初始化方法，顺序是先执行InitializingBean接口的afterPropertiesSet方法，再调用指定的init-method方法，当然如果init-method方法就是afterPropertiesSet，则只会调用一次。 到这里一个bean就算实例化+初始化完成了，那么接下来调用所有BeanPostProcessor的postProcessAfterInitialization方法，这个方法又允许对已经初始化后的bean做进一步的包装处理，比如创建动态代理（注意这里是普通情况下的动态代理，如果循环依赖遇上了动态代理，那么动态代理在三级缓存中就创建了）。到这步就表示bean已经初始化完成，如果该bean实现了DisposableBean、AutoCloseable接口或者指定了destroyMethod等支持容器回调bean的销毁方法，那么就将该bean包装成一个DisposableBeanAdapter，放到一个名为disposableBeans的LinkedHashMap中，在容器关闭的时候会调用bean的销毁方法。 最后，如果bean实现了SmartInitializingSingleton接口，那么还会调用SmartInitializingSingleton的afterSingletonsInstantiated方法。
注： 对于FactoryBean来说，容器刷新的以后在单例缓存池中保存的实例对象为原对象，而当调用getBean获取该对象的时候，如果发现该对象是一个FactoryBean，那么会尝试去从factoryBeanObjectCache缓存池中获取真实的bean，如果没有获取到，那么调用getBean方法创建真实对象，然后将该对象放入到factoryBeanObjectCache缓存池，之后直接从缓存中获取。如果要获取原对象，那么在调用getBean的时候在传入的beanName前面加上"&amp;&ldquo;字符，spring会根据该字符判断是获取真实对象还是原对象。在spring中实现BeanFactory注册和管理功能的类是FactoryBeanRegistrySupport，而我们所使用的BeanFactory继承了这个抽象类，所以具备这能力。 在Spring的类体系中，将很多行为都抽象为了一个一个的接口或者抽象类，一个类如果想要具备某个行为能力，那么就继承对应的抽象类或者实现对应的接口。
 最后就是bean的销毁流程，这个发生在容器关闭的时候，在销毁的时候也涉及到后置处理器的调用：DestructionAwareBeanPostProcessor（继承自BeanPostProcessor）。那么销毁流程是怎么样的呢？我们以实现DisposableBean接口为例，在前面已经提到过，在bean初始化完成之后，如果bean实现了DisposableBean接口，那么会将bean包装成一个DisposableBeanAdapter缓存到disposableBeans中，容器在关闭的时候，会将disposableBeans中的bean都取出来，调用其destroy方法，而在DisposableBeanAdapter的destroy方法中，就会调用所有DestructionAwareBeanPostProcessor的postProcessBeforeDestruction方法，@PreDestroy注解就是依赖该后置处理器实现的，最后调用bean的destroy方法完成bean的销毁。
bean生命周期总结 扫描class创建BeanDefinition BeanDefinitionRegistryPostProcessor#postProcessBeanDefinitionRegistry BeanFactoryPostProcessor#postProcessBeanFactory InstantiationAwareBeanPostProcessor#postProcessBeforeInstantiation（实例化之前，可以拦截bean的实例化） 实例化bean MergedBeanDefinitionPostProcessor#postProcessMergedBeanDefinition（根据实例化bean进一步扩展BeanDefinition，比如解析缓存需要依赖注入的属性等） InstantiationAwareBeanPostProcessor#postProcessAfterInstantiation（实例化之后，可以拦截属性填充）：该方法在populateBean方法中调用 bean属性填充：populateBean方法，先通过InstantiationAwareBeanPostProcessor的postProcessPropertyValues方法将需要注入的属性值封装到PropertyValues的pvs中，然后通过applyPropertyValues方法设置到实例对象中 三个Aware接口的调用，分别是BeanNameAware、BeanClassLoaderAware和BeanFactoryAware（还有很多其他Aware是通过后置处理器ApplicationContextAwareProcessor的postProcessBeforeInitialization实现的） BeanPostProcessor#postProcessBeforeInitialization（初始化之前，@PostConstruct注解在这里实现） 执行InitializingBean接口的afterPropertiesSet方法 执行xml指定的init-method BeanPostProcessor#postProcessAfterInitialization（初始化之后，可以在这里创建bean的代理） 如果bean支持销毁回调，包装DisposableBeanAdapter并缓存 SmartInitializingSingleton#afterSingletonsInstantiated方法（该方法在单例bean完全初始化完成后调用，用于实现最后阶段的自定义初始化操作，可以看做是InitializingBean的替代方案） bean销毁 取出前面缓存的DisposableBeanAdapter调用其destroy方法，在这个适配器的destroy方法中会触发DestructionAwareBeanPostProcessor的postProcessBeforeDestruction方法执行 调用DisposableBean的destroy方法 调用destroyMethod 注：Spring依赖注入主要依靠了后置处理器AutowiredAnnotationBeanPostProcessor，它相当于一个复合后置处理器，实现了多个后置处理器的行为，穿插运行于bean生命周期的几个阶段。比如，determineCandidateConstructors方法（实例化之前）：解析@Lookup注解，检查构造方法中的 @Autowired或@Value注解；postProcessMergedBeanDefinition（实例化之后，populateBean之前）：执行findAutowiringMetadata逻辑，收集@Autowired或@Value注解方法字段等存入缓存；postProcessPropertyValues（实例化之后，populateBean）：处理依赖注入对象，为属性赋值做准备等
Spring容器核心方法之invokeBeanFactoryPostProcessors方法  这个方法主要做的工作就是先调用BeanDefinitionRegistryPostProcessor，再调用BeanFactoryPostProcessor，只是调用的时候会根据是否实现PriorityOrdered接口、是否实现Ordered接口等确定调用顺序。 另外还需要注意的是，在AbstractApplicationContext的invokeBeanFactoryPostProcessors方法中，会先从ApplicationContext中获取已有的BeanFactoryPostProcessor（当然也包括BeanDefinitionRegistryPostProcessor）列表，然后在后续的逻辑中，会先调用这些后置处理器，然后再调用从BeanFactory中获取的后置处理器。这些直接从ApplicationContext中获取的后置处理器是通过addBeanFactoryPostProcessor方法直接添加到容器中的，在Spring的环境下这是空的，但是在SpringBoot的环境中，在容器刷新之前会添加，这个在分析SpringBoot源码的时候再细说。所以说这两个后置处理器的来源有两个，一个是context本身，一个是BeanFactory，他们的调用顺序都是这样的：
[from context]调用从context获取的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法，这里没有进行排序，而是按照传入list的顺序调用 [from beanFactory]调用实现了PriorityOrdered接口的**BeanDefinitionRegistryPostProcessor**的postProcessBeanDefinitionRegistry方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用实现了Ordered接口的**BeanDefinitionRegistryPostProcessor**的postProcessBeanDefinitionRegistry方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用没有实现上述两个接口的**BeanDefinitionRegistryPostProcessor**的postProcessBeanDefinitionRegistry方法，如果有多个处理器，那么顺序得不到保证 [from beanFactory]调用从context获取的**BeanDefinitionRegistryPostProcessor**的**postProcessBeanFactory**方法（因为BeanDefinitionRegistryPostProcessor也继承了BeanFactoryPostProcessor） [**from context**]调用**从context获取**的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，按照传入list的顺序调用 [from beanFactory]调用实现了PriorityOrdered接口的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用实现了Ordered接口的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，如果有多个实现了该接口的处理器，那么根据getOrder的返回值排序 [from beanFactory]调用没有实现上述两个接口的**BeanFactoryPostProcessor**的postProcessBeanFactory方法，如果有多个处理器，那么顺序得不到保证 注：在该方法的逻辑中，实现了BeanDefinitionRegistry接口的BeanFactory和没有实现该接口的BeanFactory调用逻辑不一样，如果BeanFactory没有实现BeanDefinitionRegistry，那么就不会有BeanDefinitionRegistryPostProcessor的调用，不过我们使用的都是DefaultListableBeanFactory，所以不考虑没有实现BeanDefinitionRegistry的情况</content></entry><entry><title>Spring源码-循环依赖和三级缓存</title><url>/post/spring%E6%BA%90%E7%A0%81-%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E5%92%8C%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98/</url><categories><category>Spring源码</category></categories><tags><tag>Spring源码</tag></tags><content type="html"> Spring源码（循环依赖和三级缓存）
目录 循环依赖 多级缓存 一级缓存 二级缓存 当循环依赖遇上AOP 三级缓存 Spring三级缓存源码实现 总结
 BeanFactory作为bean工厂管理我们的单例bean，那么肯定需要有个缓存来存储这些单例bean，在Spring中就是一个Map结构的缓存，key为beanName，value为bean。在获取一个bean的时候，先从缓存中获取，如果没有获取到，那么触发对象的实例化和初始化操作，完成之后再将对象放入缓存中，这样就能实现一个简单的bean工厂。 由于Spring提供了依赖注入的功能，支持将一个对象自动注入到另一个对象的属性中，这就可能出现循环依赖（区别于dependsOn）的问题：A对象在创建的时候依赖于B对象，而B对象在创建的时候依赖于A对象。 可以看到，由于A依赖B，所以创建A的时候触发了创建B，而B又依赖A，又会触发获取A，但是此时A正在创建中，还不在缓存中，就引发了问题。
一级缓存  前面引出了循环依赖的问题，那么该如何解决呢？其实很简单，单单依赖一级缓存就能解决。对于一级缓存，我们不再等对象初始化完成之后再存入缓存，而是等对象实例化完成后就存入一级缓存，由于此时缓存中的bean还没有进行初始化操作，可以称之为早期对象，也就是将A对象提前暴露了。这样B在创建过程中获取A的时候就能从缓存中获取到A对象，最后在A对象初始化工作完成后再更新一级缓存即可，这样就解决了循环依赖的问题。 但是这样又引出了另一个问题：早期对象和完整对象都存在于一级缓存中，如果此时来了其它线程并发获取bean，就可能从一级缓存中获取到不完整的bean，这明显不行，那么我们不得已只能在从一级缓存获取对象处加一个互斥锁，以避免这个问题。 而加互斥锁也带来了另一个问题，容器刷新完成后的普通获取bean的请求都需要竞争锁，如果这样处理，在高并发场景下使用spring的性能一定会极低。
二级缓存  既然只依赖一级缓存解决循环依赖需要靠加锁来保证对象的安全发布，而加锁又会带来性能问题，那该如何优化呢？答案就是引入另一个缓存。这个缓存也是一个Map结构，key为beanName，value为bean，而这个缓存可以称为二级缓存。我们将早期对象存到二级缓存中，一级缓存还是用于存储完整对象（对象初始化工作完成后），这样在接下来B创建的过程中获取A的时候，先从一级缓存获取，如果一级缓存没有获取到，则从二级缓存获取，虽然从二级缓存获取的对象是早期对象，但是站在对象内存关系的角度来看，二级缓存中的对象和后面一级缓存中的对象（指针）都指向同一对象，区别是对象处于不同的阶段，所以不会有什么问题。 既然获取bean的逻辑是先从一级缓存获取，没有的话再从二级缓存获取，那么也可能出现其它线程获取到不完整对象的问题，所以还是需要加互斥锁。不过这里的加锁逻辑可以下沉到二级缓存，因为早期对象存储在二级缓存中，从一级缓存获取对象不用加锁，这样的话当容器初始化完成之后，普通的getBean请求可以直接从一级缓存获取对象，而不用去竞争锁。
当循环依赖遇上AOP  似乎二级缓存已经解决了循环依赖的问题，看起来也非常简单，但是不要忘记Spring提供的另一种特性：AOP。Spring支持以CGLIB和JDK动态代理的方式为对象创建代理类以提供AOP支持，在前面总结bean生命周期的文章中已经提到过，代理对象的创建(通常)是在bean初始化完成之后进行（通过BeanPostProcessor后置处理器）的，而且按照正常思维来看，一个代理对象的创建也应该在原对象完整的基础上进行，但是当循环依赖遇上了AOP就不那么简单了。 还是在前面A和B相互依赖的场景中，试想一下如果A需要被代理呢？由于二级缓存中的早期对象是原对象，而代理对象是在A初始化完成之后再创建的，这就导致了B对象中引用的A对象不是代理对象，于是出现了问题。 要解决这问题也很简单，把代理对象提前创建不就行了？也就是如果没有循环依赖，那么代理对象还是在初始化完成后创建，如果有循环依赖，那么就提前创建代理对象。那么怎么判断发生了循环依赖呢？在B创建的过程中获取A的时候，发现二级缓存中有A，就说明发生了循环依赖，此时就为A创建代理对象，将其覆盖到二级缓存中，并且将代理对象复制给B的对应属性，解决了问题。当然，最终A初始化完成之后，在一级缓存中存放的肯定是代理对象。 如果在A和B相互依赖的基础上，还有一个对象C也依赖了A：A依赖B，B依赖A，A依赖C，C依赖A。那么在为A创建代理对象的时候，就要注意不能重复创建。
可以在对象实例化完成之后，将其beanName存到一个Set结构中，标识对应的bean正在创建中，而当其他对象创建的过程依赖某个对象的时候，判断其是否在这个set中，如果在就说明发生了循环依赖。
三级缓存  虽然仅仅依靠二级缓存能够解决循环依赖和AOP的问题，但是从解决方案上看来，维护代理对象的逻辑和getBean的逻辑过于耦合，Spring没有采取这种方案，而是引入了另一个三级缓存。三级缓存的key还是为beanName，但是value是一个函数（ObjectFactory#getBean方法），在该函数中执行获取早期对象的逻辑：getEarlyBeanReference方法。 在getEarlyBeanReference方法中，Spring会调用所有SmartInstantiationAwareBeanPostProcessor的getEarlyBeanReference方法，通过该方法可以修改早期对象的属性或者替换早期对象。这个是Spring留给开发者的另一个扩展点，虽然我们很少使用，不过在循环依赖遇到AOP的时候，代理对象就是通过这个后置处理器创建。
 那么三级缓存在spring中是如何体现的呢？我们来到DefaultSingletonBeanRegistry中：
//一级缓存，也就是单例池，存储最终对象 private final Map&lt;String, Object> singletonObjects = new ConcurrentHashMap&lt;>(256); //二级缓存，存储早期对象 private final Map&lt;String, Object> earlySingletonObjects = new HashMap&lt;>(16); //三级缓存，存储的是一个函数接口， private final Map&lt;String, ObjectFactory&lt;?>> singletonFactories = new HashMap&lt;>(16);  其实所谓三级缓存，在源码中就是3个Map，一级缓存用于存储最终单例对象，二级缓存用于存储早期对象，三级缓存用于存储函数接口。 在容器刷新时调用的十几个方法中，finishBeanFactoryInitialization方法主要用于实例化单例bean，在其逻辑中会冻结BeanDefinition的元数据，接着调用beanFactory的preInstantiateSingletons方法，循环所有被管理的beanName，依次创建Bean，我们这里主要关注创建bean的逻辑，也就是AbstractBeanFactory的doGetBean方法（该方法很长，这里只贴了部分代码）：
protected &lt;T> T doGetBean(final String name, @Nullable final Class&lt;T> requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException { //解析FactoryBean的name(&amp;)和别名 final String beanName = transformedBeanName(name); Object bean; //尝试从缓存中获取对象 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) { //包含了处理FactoryBean的逻辑，可以通过&amp;+beanName获取原对象，通过beanName获取真实对象 //FactoryBean的真实bean有单独的缓存factoryBeanObjectCache（Map结构）存放 //如果是普通的bean，那么直接返回对应的对象 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { //只能解决单例对象的循环依赖 if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } //如果存在父工厂，并且当前工厂中不存在对应的BeanDefinition，那么尝试到父工厂中寻找 //比如spring mvc整合spring的场景 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) { //bean的原始名称 String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) { return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); } else if (args != null) { return (T) parentBeanFactory.getBean(nameToLookup, args); } else { return parentBeanFactory.getBean(nameToLookup, requiredType); } } try { //处理dependsOn的依赖(不是循环依赖) String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { if (isDependent(beanName, dep)) { //循环depends-on 抛出异常 throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Circular depends-on relationship between '" + beanName + "' and '" + dep + "'"); } registerDependentBean(dep, beanName); try { getBean(dep); } catch (NoSuchBeanDefinitionException ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, "'" + beanName + "' depends on missing bean '" + dep + "'", ex); } } } //创建单例bean if (mbd.isSingleton()) { //把beanName 和一个ObjectFactory类型的singletonFactory传入getSingleton方法 //ObjectFactory是一个函数，最终创建bean的逻辑就是通过回调这个ObjectFactory的getObject方法完成的 sharedInstance = getSingleton(beanName, () -> { try { //真正创建bean的逻辑 return createBean(beanName, mbd, args); } catch (BeansException ex) { destroySingleton(beanName); throw ex; } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } return (T) bean; }  对于单例对象，调用doGetBean方法的时候会调用getSingleton方法，该方法的入参是beanName和一个ObjectFactory的函数，在getSingleton方法中通过回调ObjectFactory的getBean方法完成bean的创建，那我们来看看getSingleton方法(部分代码)：
public Object getSingleton(String beanName, ObjectFactory&lt;?> singletonFactory) { Assert.notNull(beanName, "Bean name must not be null"); //互斥锁 synchronized (this.singletonObjects) { //首先尝试从尝试从单例缓存池中获取对象，如果获取到了对象则直接返回，否则进入创建的逻辑 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { //在创建之前将要创建的beanName存入singletonsCurrentlyInCreation中，这个是一个Map结构，用于标识单例正在创建中 beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) { this.suppressedExceptions = new LinkedHashSet&lt;>(); } try { //回调singletonFactory的getObject方法，该函数在外层doGetBean方法中传入 //实际调用了createBean方法 singletonObject = singletonFactory.getObject(); newSingleton = true; } finally { if (recordSuppressedExceptions) { this.suppressedExceptions = null; } //将beanName从正在创建单例bean的集合singletonsCurrentlyInCreation中移除 afterSingletonCreation(beanName); } if (newSingleton) { //将创建好的单例bean放入一级缓存，并且从二级缓存、三级缓存中移除 //添加到registeredSingletons中 addSingleton(beanName, singletonObject); } } return singletonObject; } }  getSingleton方法的主线逻辑很简单，就是通过传入的函数接口创建单例bean，然后存入一级缓存中，同时清理bean在二级缓存、三级缓存中对应的数据。前面已经分析过了，传入的函数接口调用的是createBean方法，那么我们又来到createBean方法，createBean方法主要调用doCreateBean方法，在doCreateBean调用之前会先调用nstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation拦截bean的实例化，如果这里的后置处理器返回了bean，则不会到后面的doCreateBean方法中，不过我们这里不用关心这个逻辑，直接跳到doCreateBean方法（部分代码）：
protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { //创建bean，也是就是实例化，会把实例化后的bean包装在BeanWrapper中 instanceWrapper = createBeanInstance(beanName, mbd, args); } //eanWrapper中获取到对象 final Object bean = instanceWrapper.getWrappedInstance(); Class&lt;?> beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) { mbd.resolvedTargetType = beanType; } synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { //MergedBeanDefinitionPostProcessor后置处理器的处理 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Post-processing of merged bean definition failed", ex); } mbd.postProcessed = true; } } //判断该对象是否支持提前暴露，核心条件就是要求单例对象 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { //把我们的早期对象包装成一个singletonFactory对象 该对象提供了一个getObject方法,该方法内部调用getEarlyBeanReference方法 //将早期对象存入三级缓存，三级缓存存的是一个接口实现（ObjectFactory接口），其getBean方法会调用getEarlyBeanReference方法 addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); } // Initialize the bean instance. Object exposedObject = bean; try { //属性赋值操作，这里就可能出现循环依赖问题 populateBean(beanName, mbd, instanceWrapper); //实例化操作：调用三个Aware、后置处理器beforeInitialization（包含@PostConstruct的实现）、afterPropertiesSet、init-method、后置处理器afterInitialization等 //个方法里的后置处理器可能会改变exposeObject exposedObject = initializeBean(beanName, exposedObject, mbd); } catch (Throwable ex) { if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Initialization of bean failed", ex); } } if (earlySingletonExposure) { //这里的入参为false,表示只从一级缓存和二级缓存中获取 //非循环依赖的情况下，不会调用到三级缓存，三级缓存中的接口会在单例对象入一级缓存的时候移除 Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { //如果获取到了早期暴露对象earlySingletonReference不为空 if (exposedObject == bean) { //如果exposeObject在经过initializeBean方法后没有改变，那么说明 //没有被后置处理器修改，那么用earlySingletonReference替换exposeObject exposedObject = earlySingletonReference; } } } return exposedObject; }  这个方法的逻辑就是先实例化对象，如果对象支持暴露早期对象，那么会将早期对象作为入参传入getEarlyBeanReference方法，然后包装成一个ObjectFactory存入三级缓存，接着再调用populateBean方法填充对象的属性。而在填充对象属性的过程中，就可能发现循环依赖，比如当前正在创建A，然后在populateBean方法中发现了依赖B，那么就会调用getBean(B)，B也会走上面A走的这个流程，当B也走到populateBean方法填充属性的时候，又发现依赖了A，那么又会调用getBean(A)。那么在populateBean创建A的时候，会从三级缓存中获取bean，如果A需要被代理，那么会创建代理对象，这样B中的A属性就是代理对象了，接着把三级缓存获取的对象存入二级缓存中。 在B处理完成之后就回到了A的逻辑，假设populateBean(A)的逻辑完成了，那么接着进入initializeBean方法进行A的初始化操作，注意这里执行初始化操作的对象是A原对象，代理对象存储在二级缓存中。由于initializeBean方法会调用后置处理器的before-afterInitialization方法，这个后置处理器可能会改变对象，所以在后面的逻辑中，如果从二级缓存中获取到了A的代理对象，会判断原对象经过后置处理器后有没有变化，如果还是原对象，那么用二级缓存中的代理对象覆盖原对象，所以doCreateBean方法返回的就是代理对象，最终存入一级缓存中。流程就是：创建A实例对象->设置A的B属性->创建B实例对象->设置B的A属性->创建A的代理对象存入二级缓存->初始化A对象->从二级缓存取出A的代理对象覆盖A对象->A的代理对象存入一级缓存 我们需要关注的就是在B的populateBean逻辑里调用getBean(A)是什么样的逻辑。当然也是getBean->doGetBean->getSingleton这个逻辑，我们着重关注一下getSingleton方法的实现：
protected Object getSingleton(String beanName, boolean allowEarlyReference) { //首先从一级缓存中获取 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) { //如果从一级缓存没有获取到，并且获取的对象正在创建中，这里已经能够确定发生了循环依赖 synchronized (this.singletonObjects) { //为了防止其它线程获取到不完整工单对象，这里使用同步锁控制 //从二级缓存中获取早期对象 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) { //如果二级缓存中也没有获取到，且allowEarlyReference为true（这里传入的是true） //那么尝试从三级缓存中获取 ObjectFactory&lt;?> singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { //从三级缓存中获取到了对象，注意三级缓存存储的是ObjectFactory //调用getObject方法，前面提到了早期暴露的对象存入三级缓存的ObjectFactory的getBean方法调用 //的是getEarlyBeanReference方法，所以这里会调用getEarlyBeanReference方法 singletonObject = singletonFactory.getObject(); //把早期对象存入二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); //三级缓存中的接口没用了，直接移除 this.singletonFactories.remove(beanName); } } } } return singletonObject; }  在getSingleton方法的逻辑中，先从一级缓存获取，如果一级缓存没有找到，那么如果获取的bean正在创建中，则从二级缓存获取，如果二级缓存没有找到，那么从三级缓存获取，三级缓存中存的是ObjectFactory实现，最终会调用其getBean方法获取bean，然后存入二级缓存中，同时清除三级缓存。同时提供了一个allowEarlyReference参数控制是否能从三级缓存中获取。 对于循环依赖的情况，getBean(A)->存入正在创建缓存->存入三级缓存->populateBean(A)->getBean(B)->populateBean(B)->getBean(A)->getSingleton(A)，当在populateBean(B)的过程中调用getSingleton(A)的时候，明显一级缓存和二级缓存都为空，但是三级缓存不为空，所以会通过三级缓存获取bean，三级缓存的创建逻辑如下：
boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); }  所以我们直接来到getEarlyBeanReference方法获取早期对象：
protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) { Object exposedObject = bean; //如果容器中有InstantiationAwareBeanPostProcessors后置处理器 if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof SmartInstantiationAwareBeanPostProcessor) { //找到SmartInstantiationAwareBeanPostProcessor后置处理器 SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; //调用SmartInstantiationAwareBeanPostProcessor的getEarlyBeanReference方法 exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); } } } return exposedObject; }  这个getEarlyBeanReference方法的逻辑很简单，但是非常重要，该方法主要就是对SmartInstantiationAwareBeanPostProcessor后置处理器的调用，而循环依赖时的AOP就是通过这个SmartInstantiationAwareBeanPostProcessor的getEarlyBeanReference方法实现的，相关的具体类是AnnotationAwareAspectJAutoProxyCreator，这个留待AOP原理分析的文章中详细说明。
 多级缓存并不只是为了解决循环依赖和AOP的问题，还考虑到了逻辑的分离、结构的扩展性和保证数据安全前提下的效率问题等。由于存在二级缓存提前暴露不完整对象的情况，所以为了防止getSingleton方法返回不完整的bean，在该方法中使用了synchronized加锁，而为了不影响容器刷新完成后正常获取bean，从一级缓存获取对象没有加锁，事实上也不用加锁，因为一级缓存中要么没有对象，要么就是完整的对象。 通常情况下，容器刷新会触发单例对象的实例化和初始化，大致流程如下：循环依赖发生在对象都属性赋值，也就是populateBean阶段，在对象实例化完成后将其包装成一个函数接口ObjectFactory存入三级缓存，通过getEarlyBeanReference方法触发SmartInstantiationAwareBeanPostProcessor后置处理器的执行，如果循环依赖遇上了AOP，那么就在这里进行处理。 这里每个单例对象实例化之后存入三级缓存，即使没有发生循环依赖，这个是为之后可能发生的循环依赖做准备，如果最终没有发生循环依赖，那么对象实例化之后，正常初始化，然后存入一级缓存即可，而在存入一级缓存的时候会清空二级和三级缓存。</content></entry><entry><title>Sql问题记录一（distinct、group by）</title><url>/post/sql%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%E4%B8%80distinctgroup-by/</url><categories><category>Sql</category><category>Mysql</category></categories><tags><tag>Sql</tag><tag>Mysql</tag></tags><content type="html"> mysql使用distinct问题记录
distinct可以单字段去重，也可以多字段去重，
假如有表 A,其有id，name,sex,addr,tel,createTime等如下字段，其中id是主键，故唯一
那么我们就可以对其他字段进行去重操作了。
1、按单字段去重，
 select distinct name from A
 这样，可以仅仅可以查询name这一列，且name没有相同数据。
2、多字段去重
 select distinct name, sex, addr from A
 mysql会对dintinct所有字段当作整体去重，这样，只有当这三个字段全部相同时，才能成功去重，有一个
字段不同就不会过滤掉。
如果我们去重后想查看所有字段，想写出类似这样一个sql：
select distinct(name,sex,addr) name,sex,addr,createTime from A 这种写法是不被允许的，但是用group by来实现
select * from A where id in (select max(id) from A group by name,sex,addr)
子表是按group by 分组，也就相当于去重，去重之后，只找到一个最大的id,然后select * in 这些id中就可以
完成去重后查看所有的字段了。</content></entry><entry><title>关于IDEA生成jar包后FileEncoding依然是GBK</title><url>/post/%E5%85%B3%E4%BA%8Eidea%E7%94%9F%E6%88%90jar%E5%8C%85%E5%90%8Efileencoding%E4%BE%9D%E7%84%B6%E6%98%AFgbk/</url><categories><category>intellij-idea</category><category>java</category><category>ide</category></categories><tags><tag>intellij-idea</tag><tag>java</tag><tag>ide</tag></tags><content type="html"> 关于IDEA生成jar包后FileEncoding依然是GBK
在启动类增加 Properties类 @SpringBootApplication public class TokenApplication { public static void main(String[] args) { Properties properties = System.getProperties(); properties.forEach((key,value)->{ System.out.println("key = " + key); System.out.println("value = " + value); }); SpringApplication.run(TokenApplication.class,args); } } 打印在控制台我们可以看到 file.encoding 的值 为UTF-8 ，说明我们的idea的file.encoding 是为UTF-8 如若不是可修改： 然后我我们使用maven 将其项目打成jar包，使用java-jar 运行jar包 我们看到 其值变为了GBK，在普通的运行当中我认为是不会出错，因今天我们的项目使用jar包运行，在调用python算法时，报utf-8 codec can&rsquo;t decode byte 此错误，是因为他转为GBK（在idea运行，调用算法没有报错） 3.1 （第一种方法，较麻烦）换用运行命令 我们之前使用的是java -jar 我们在他后边加上 -Dfile.encoding=utf-8 新命令： java -jar -Dfile.encoding=utf-8 jar包 运行 我们看到打印的结果为 其已经修改， 这时候他就已经是UTF-8了 这样我们就解决了其file.encoding 为GBK 的问题，但是我们不能总是java -jar 在加上他 我们可以写一个cmd脚本来运行此命令，但同样我们可以使用第二种更为简单的方法 3.2（第二种方法，简单）配置环境变量 同样我们可以再电脑的环境变量中添加 key：JAVA_TOOL_OPTIONS value： -Dfile.encoding=utf-8 添加之后我们保存 ，这时候我们再泳普通的 java -jar 来运行这个jar包 我们看到他已经变成了 utf-8 ，这时候我们的项目在调用python算法时就不会报错了，到这里也就解决了 无论如何在idea中设置jar包的file.encoding 为GBK的问题。 作者：如果问题，请评论，因是第一次这么解决，不知会不会造成其他影响，如有其他更好建议，欢迎评论。多多指教。</content></entry><entry><title>Centos安装 Docker</title><url>/post/centos%E5%AE%89%E8%A3%85docker/</url><categories><category>Linux</category><category>Docker</category></categories><tags><tag>Linux</tag><tag>Docker</tag></tags><content type="html"> Centos安装 Docker
Centos安装 Docker 从 2017 年 3 月开始 docker 在原来的基础上分为两个分支版本: Docker CE 和 Docker EE。
Docker CE 即社区免费版，Docker EE 即企业版，强调安全，但需付费使用。
本文介绍 Docker CE 的安装使用。
移除旧的版本：
$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 安装一些必要的系统工具：
sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息：
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新 yum 缓存：
# centos 7 sudo yum makecache fast # CentOS 8没有fast这个命令 sudo yum makecache 安装 Docker-ce：
sudo yum -y install docker-ce 查看已安装docker版本
docker version 启动 Docker 后台服务
sudo systemctl start docker 开机启动
sudo systemctl enable docker 镜像加速 鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决。
可以使用阿里云的docker镜像地址：https://7qyk8phi.mirror.aliyuncs.com
新版的 Docker 使用 /etc/docker/daemon.json（Linux，没有请新建）。
请在该配置文件中加入：
（没有该文件的话，请先建一个）
{ "registry-mirrors": ["https://7qyk8phi.mirror.aliyuncs.com"] } 重启docker
sudo systemctl daemon-reload sudo systemctl restart docker 检查加速器是否生效 配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行 docker info，如果从结果中看到了如下内容，说明配置成功。
Registry Mirrors: https://7qyk8phi.mirror.aliyuncs.com/ 下载docker-compose #运行此命令以下载 Docker Compose 的当前稳定版本 sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose #对二进制文件应用可执行权限 sudo chmod +x /usr/local/bin/docker-compose #测试安装 docker-compose --version #若有docker-compose version 1.29.2, build 5becea4c，则安装成功</content></entry><entry><title>Docker与Docker-compose</title><url>/post/docker%E4%B8%8Edocker-compose%E8%AF%A6%E8%A7%A3/</url><categories><category>Docker</category><category>Docker-compose</category></categories><tags><tag>Docker</tag><tag>Docker-compose</tag></tags><content type="html"> Docker与Docker-compose详解
1、Docker是什么？
在计算机中，虚拟化(英语: Virtualization) 是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。
在实际的生产环境中。虚拟化技术主要用来解决高性能的物理硬件产能过利和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件资源的充分利用。
虚拟化技术种类很多，例如:软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化、桌面虚拟化、服务虚拟化、虚拟机等等。.
Docker和传统虚拟机的区别
2、Docker的安装
2.1、Windows下的安装
直接在官网下载windows包双击运行即可，对于win10来说需要开启Hype-v，直接百度打开即可。
2.2、Linux下的安装
curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh --mirror Aliyun # 安装报错 Problem: problem with installed package buildah # 执行语句 yum erase podman buildah # 再进行安装 systemctl status docker systemctl restart docker docker info systemctl enable docker # 建立docker组 sudo groupadd docker sudo usermod -aG docker $USER # 重启服务 systemctl restart docker 2.3、核心概念
仓库 1. 远程仓库：开发者镜像及官方镜像 1. 本地仓库：只保存当前自己使用过的镜像及自定义镜像 1. 作用：用来存放docker镜像位置 5. 镜像 5. 作用：一个镜像就代表一个软件 7. 容器 7. 作用：一个幢像运行一次就会生成一个实例就是生成一个容器 2.4、Aliyun服务加速
docker提供了一个远程仓库，主要是用来存放镜像的，而我们所需要的镜像都需要去远程仓库进行拉取，dockerHub 地址： https://registry.hub.docker.com/_/mysql?tab=tags
，这里以mysql镜像为例，然后直接在虚拟机当中执行命令
# 获取最新版本的mysql docker pull mysql # 获取指定版本的mysql 8.0.18版 docker pull mysql:8.0.18 在这里的dockerhub是为全球服务的，速度难免会有点慢，这里可以配置阿里的镜像来进行获取docker远程仓库的镜像。阿里云服务加速配置 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors
，按照官网进行配置即可：
3、Docker的操作
3.1、Hello World
在安装docker之后，直接使用命令：docker run hello-world 表示直接运行hello-world这个镜像。而他的执行基本步骤如下：
3.2、Docker 的基本命令
3.2.1、docker引擎及帮助操作：
# 查看docker信息 docker info # 查看docker版本 docker version # 帮助命令 docker --help 3.2.2、镜像相关操作：
# 查看镜像 docker images docker images -a #展示所有镜像 docker images -q #只展示镜像的ID docker images mysql #只展示mysql镜像 # 下载镜像 docker pull 镜像名称:版本号 # 如 docker pull mysql:8.0.27 docker pull 镜像名称:@DIGEST #如：docker pull mysql:DIGEST:sha256:975b3b1a6df6bf66221d1702b76c4141a4cd09f93f22f70c32edc99a6c256fe8 # 搜索镜像 docker search 镜像 # docker search mysql # 搜索stars数在3000以上的image docker search mysql --filter=stars=3000 # 删除镜像 docker image rm 镜像名:版本或者id标识 # docker image rm mysql:8.0.27 docker image rm -f 镜像名:版本或者id标识 # 强制删除 # 简化删除 docker rmi 镜像名:版本 # 组合运用 # 清空本地仓库所有镜像 docker rmi -f $(docker images -q) 3.2.3、容器相关操作：
# 导入本地镜像 docker load -i 镜像文件 # 运行一个容器 docker run 镜像名称:版本号 # 运行容器与宿主机进行映射 docker run -p 8080:8080 镜像名称:版本号 # 启动容器映射端口，后台启动 docker run -p 8080:8080 -d 镜像名称:版本号 # 启动容器映射端口，后台启动，指定名称 docker run -p 8080:8080 --name 容器名称 -d 镜像名称:版本号 # 查看正在运行的容器 docker ps # 查看运行容器的历史记录 docker ps -a # 查看最近运行的两个容器 docker ps -a -n=2 # 查看正在运行的容器id docker ps -q # 查看所有容器的id docker ps -aq # 容器的启动和停止 docker start 容器名称或者容器id docker restart 容器名称或者容器id docker stop 容器名称或者容器id docker kill 容器名称或者容器id # 容器的删除 docker rm 容器的id或者名称 docker rm -f 容器的id或者名称 docker rm -f $(docker ps -aq) # 查看日志 docker logs 容器id或名称 # 实时展示日志 docker logs -f 容器id或名称 # 加入时间戳展示实时展示日志 docker logs -tf 容器id或名称 # 查看最后n行日志 docker logs --tail 5 容器id或名称 # 查看容器的内部进程 docker top 容器id或名称 # 与容器内部进行交互 docker exec -it 容器id或名称 bash # 从容器复制文件到操作系统 docker cp 容器id:路径 操作系统下的路径 # 从操作系统复制文件到容器当中 docker cp 操作系统下的路径 容器id:路径 在这里的文件复制主要还是运用到本地项目打包后的部署，比如说这里一个项目开发完成之后，打成一个jar包或者war包，丢给tomcat进行启动部署，而后直接将这个包给到tomcat镜像下的webapps目录下，重新启动tomcat或者重启容器，最后进行访问项目。
# 查看容器中的元数据 docker inspect 容器id # 数据卷（Volume）：实现宿主机系统和容器之间的文件共享 # 数据卷的使用： docker run -d -p 8080:8080 --name 容器名称 -v 操作系统下路径:容器下路径 镜像名称:版本 # aa代表一个数据卷的名字，名称可以自定义，docker在不存在时自动创建这个数据卷，并且同时自动映射宿主机当中的某个目录 # 同时在启动容器的时候会将aa对应容器目录中全部内容复制到aa映射目录当中 docker run -d -p 8080:8080 --name 容器名称 -v aa:容器下路径 镜像名称:版本 find / -name aa # 将容器打成一个新的镜像 docker commit -m "描述信息" -a "作者信息" 容器id或名称 打包的镜像名称:标签版本 # 将镜像备份出来 docker save 镜像名称:版本 -o 文件名 # 重新加载镜像 docker load -i 镜像文件 3.3、Docker 镜像分层原理
镜像是一种轻量级的，可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时所需的库、环境变量和配置文件。
Docker当中的镜像为什么这莫大？
Docker的设计：一个软件镜像不仅仅是原来软件包，镜像当中还包含软件包所需的操作系统依赖软件自身依赖以及自身软件包组成。
分层原理
很显然，在这里docker容器的设计简单来说，对于不同的环境都给抽离出来进行分层，就比如说很多的软件服务（比如说：Naocs、ES、Hadoop等等）都需要jdk的环境，那再进行拉取镜像的时候，这些镜像都会先检验jdk的环境，再进行后续的安装，那这里装个Naocs、ES、Hadoop要下载三次JDK，这显然浪费了很多的内存，所以在这里Docker采用了分层的原理，这里每一层的环境依赖都给分开了，再一次安装了jdk环境之后，后续安装的服务也要jdk依赖就不会再去拉取了，回直接使用本地有的jdk环境。
3.4、Docker 网络
在docker当中容器和容器之间也是可以进行通信的。就好比Linux中我们使用 ip addr 可以看到当前虚拟机的ip地址，在这里可以查看一下容器中的IP，docker exec -it 容器名 ip addr 会发现有一个对应的映射另一个映射。说明docker容器网络是通过veth-pair技术实现的。
并且在这里还可以通过docker inspect 容器名称或id 命令查看容器的元数据，这里也有该容器随机分配的ip地址。
而当我们启动多个容器之后，可以查看多个容器的ip地址，可以看到容器的ip地址都在同一个网段上，这就有点似曾相识的感觉了，在linux当中我们配置多台机器进行互相通信，那这里的容器通信那也是一样的不，直接进入到一台容器之内，使用ping命令，ping另外一个容器的ip。
再就是在启动容器之后，默认为分配的ip地址都同一个网桥上，而这里容器当中需要对网桥进行分割开又要如何操作呢？我们需要创建一个网桥，而后在启动容器的时候指定对应的网桥即可。
# 查看网桥 docker network ls # 创建网桥 docker network create 网桥名称 # 容器指定网桥挂载 docker run -p -d 8080:8080 --network 网桥名称 --name 容器名称 镜像:版本 # 在启动容器，生成的ip地址会和容器名称进行映射，这里除了使用ip进行访问，还可以使用容器名称进行访问 # 删除 docker network rm 网桥名称 # 网桥细节 docker inspect 网桥名称 3.5、Docker 数据卷
3.5.1、作用
是用来实现容器和宿主机之间的数据共享
3.5.2：特点
数据卷可以在容器之间进行共享和重用 对数据卷的修改会立即影响到对应的容器 对数据卷的修改不会影响镜像 数据卷默认一直存在，即使容器被删除 3.5.3、数据卷操作
# 自定义数据卷目录 docker run -v 绝对路径:容器内路径 # 自动创建数据卷 docker run -v 卷名:容器内路径 # 查看数据卷 docker volume ls # 查看数据卷的细节 docker volume inspect 卷名 # 创建数据卷 docker volume create 卷名 # 删除数据卷(没有使用的数据卷) docker volume prune # 删除指定的数据卷 docker volume rm 卷名 3.6、Docker 核心架构
4、Docker安装服务
4.1、mysql 的安装
首先我们需要确定服务的版本，拉取镜像到本地： dockerHub 拉取镜像描述文件
，在镜像的描述文件当中，会对服务的启动、查看服务日志、服务配置等等都有进行描述。
# 先获取mysql服务 docker pull mysql:8.0.18 服务的启动：这里需要指定运行的环境
# 基本启动 docker run -e MYSQL_ROOT_PASSWORD=root -d mysql:8.0.18 # 启动服务 后台运行 指定root用户账号密码(设置root账户的密码为root) 指定容器名称 docker run -d -p 3307:3306 --name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d mysql:8.0.18 # 启动服务 后台运行 指定root用户账号密码 指定容器名称 使用数据卷将数据持久化 # mysql 容器默认存储位置：/var/lib/mysql docker run -d -p 3307:3306 --name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d -v mysqldata:/var/lib/mysql mysql:8.0.18 # 启动服务 后台运行 指定root用户账号密码 指定容器名称 使用数据卷将数据持久化 已修改之后的配置文件启动 docker run -d -p 3307:3306 --name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d -v mysqldata:/var/lib/mysql -v mysqlconfig:/etc/mysql mysql:8.0.18 4.2、Tomcat 的安装
# 先获取镜像 docker pull tomcat:9.0-jdk8 # 服务启动 docker run -d -p 8080:8080 --name tomcat tomcat:9.0-jdk8 # 项目的部署目录 /usr/local/tomcat/webapps docker run -d -p 8080:8080 -v apps:/usr/local/tomcat/webapps --name tomcat tomcat:9.0-jdk8 # 配置文件目录 /usr/local/tomcat/conf docker run -d -p 8080:8080 -v apps:/usr/local/tomcat/webapps -v confs:/usr/local/tomcat/conf --name tomcat tomcat:9.0-jdk8 4.3、Redis的安装
# 拉取镜像 docker pull redis:6.2.6 # 启动服务 docker run -d -p 6379:6379 --name redis6 redis:6.2.6 # redis 持久化 docker run -d --name redis6 redis:6.2.6 redis-server --appendonly yes 4.4、ElasticSearch 的安装
5、Dockerfile
5.1、Dockerfile 概述
5.1.1、Dockerfile是什么？
Dockerfile是用来帮助自己构建一个自定义镜像
5.1.2、为什么会存在Dockerfile？
日常用户可以将自己应用进行打包成镜像，这样就可以让我们自己的应用在容器当中运行
5.1.3、Dockerfile构建镜像原理
5.2、Dockerfile 语法
FROM：构建一个自定义的镜像
# 新建一个dockerfile文件 vim Dockerfile # 写入内容 FROM centos:8 # 进行build docker build -t mycentos8:01 . # RUN : 对镜像进行扩展 docker run -it centos:7 # 不支持vim，对于vim的扩展，在原本的dockerfile文件当中加入 RUN yum install -y vim # 或者使用这种语法 RUN ["yum","install","-y","vim"] # EXPORT : 镜像暴露端口 EXPOSE 8888 #指定工作目录 WORKDIR /data # 复制文件 COPY aa.txt /data/aa # 添加内容 ADD bb.txt /data/bb ADD 下载地址 /data/tomcat 5.3、idea对Dockerfile支持
打开idea的settings，在以来当中找到docker的依赖，进行安装该依赖，安装之后重启idea就可以对Dockerfile文件进行编辑了。
第二个就是，Dockerfile文件都在linux上，在idea当中怎么进行编辑呢，可以选择Tools下的deployment的browse remote host 进行连接远程虚拟机，这里直接连接上去之后在右侧就会有虚拟机上的文件目录信息，并且可以直接在idea当中进行打开了。
6、Docker compose
6.1、Docker compose 概述
6.1.1、compose的作用
用来负责对Docker容器集群的快速编排
6.1.2、compose的定位
是用来定义和运行多个docker容器的应用 同时可以对多个容器进行编排
6.1.3、compose的核心概念
服务：一个应用的容器，服务可以存在多个 项目：由一组关联的应用容器组成的一个完整业务单元，在docker-compose.yml文件当中进行定义 6.1.4、compose的安装
github下载地址： https://github.com/docker/compose/releases
首先在github上面下载对应的版本包，下载之后将包上传到linux服务器上。将文件进行重命名并且复制到/usr/local/bin目录下，并且给该目录赋予权限。最后直接使用docker-compose -v命令查看版本进行校验是否安装成功
mv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose -v 6.1.5、docker和docker-compose直接的版本对应
docker官网地址： https://docs.docker.com/compose/compose-file/
使用命令 docker -v 查看docker的版本，可以在官网当中看到compose对docker版本的支持
6.2、Docker compose —— HelloWorld
在前面有说到compose当中的组成部分，分别是服务和项目，这里首先创建一个目录用来表示这第一个helloworld项目，在项目当中添加docker-compose.yml文件用来编写compose。
# 创建目录 mkdir hello cd hello # 新建docker-compose.yml文件 vim docker-compose.yml # 写入内容（这里在vim当中编辑yml文件挺难受的，可以在idea当中编辑远程主机的文件） version: "3.0" # 指定compose的版本 services: # 指定服务 tomcat: # 单个服务 image: tomcat:9.0.27-jdk8 # 服务镜像 ports: - 8081:8080 # 暴露对应的端口 # 保持文件内容后进行启动compose docker-compose up # 服务启动之后，可以直接进行访问8081端口 http://远程主机ip/8081 6.3、Docker compose —— 命令模板
version: "3.0" # compose版本 services: user: build: context: user # dockerfile的镜像 dockerfile: Dockerfile # 读取dockerfile文件进行打包获取镜像 container_name: user ports: - "8888:8888" networks: - hello depends_on: - tomcat tomcat: # 单个服务标识 container_name: tomcat # 启动后的容器名称 相当于 --name 指定的名称 image: tomcat:9.0.27-jdk8 # 镜像 ports: - 8081:8080 # 端口映射 volumes: - tomcatwebapps:/usr/local/tomcat/webapps # 指定对应的数据卷 networks: - hello # 指定网桥 depends_on: # 服务启动依赖 - tomcat1 # 服务标识 - mysql healthcheck: # 健康检查 test: ['CMD','curl','-f','http://localhost'] interval: 1m30s timeout: 10s retries: 3 sysctls: # 修改内核参数 - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits: # 修改最大进程数 nproc: 65335 nofile: soft: 20000 hard: 40000 tomcat1: container_name: tomcat2 image: tomcat:9.0.27-jdk8 ports: - 8082:8080 volumes: - tomcatwebapps1:/usr/local/tomcat/webapps networks: - hello mysql: container_name: mysql8 image: mysql:8.0.18 ports: - 3307:3306 # environment: # 指定启动的环境 # - MYSQL_ROOT_PASSWORD=root env_file: # 使用文件进行代替 - ./mysql.env # mysql.evn文件内容就是 MYSQL_ROOT_PASSWORD=root volumes: - mysqldata:/var/lib/mysql - mysqlconfig:/etc/mysql networks: - hello # 数据卷都要在这统计管理 volumes: tomcatwebapps: tomcatwebapps1: # external: # 使用自定义数据卷名称 默认命名为 项目名_数据卷名 自定义后为 数据卷名 # true mysqldata: mysqlconfig: # 统一管理网桥 networks: hello: 6.4、Docker compose 指令
6.4.1、模板指令与指令
模板指令：用来书写在docker-compose.yml文件当中的指令，是用来为服务进行服务的 指令：用来对整个docker-compose.yml对应的这个项目进行操作 6.4.2、常用指令
6.5、Docker 可视化工具 —— portainer
直接在dockerHub上面拉取镜像启动服务
docker pull portainer/portainer:1.24.2 docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer:1.24.2 直接访问远程虚拟机的9000端口，注册一个账号，链接到本地虚拟机的服务，就可以看到所提供的web可视化页面了。
同样的我们还可以将这个服务的启动加到docker-componse当中进行启动：
# 加入服务 portainer: container_name: portainer image: portainer/portainer:1.24.2 ports: - 8000:8000 - 9000:9000 volumes: - /var/run/docker.sock:/var/run/docker.sock - portainer_data:/data # 数据卷 volumes: portainer_data:</content></entry><entry><title>Seata四种模式（XA、AT、TCC、SAGA）</title><url>/post/seata%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8Fxaattccsaga/</url><categories><category>分布式事务</category><category>中间件</category><category>Seata</category></categories><tags><tag>分布式事务</tag><tag>中间件</tag><tag>Seata</tag></tags><content type="html"> Seata四种模式（XA、AT、TCC、SAGA）
Seata四种模式（XA、AT、TCC、SAGA） 1、XA模式 XA模式原理： XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA 规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对 XA 规范 提供了支持。
如果有失败的就会回滚事务
seata的XA模式 seata的XA模式做了一些调整，但大体相似：
RM一阶段的工作：
注册分支事务到TC 执行分支业务sql但不提交 报告执行状态到TC TC二阶段的工作：
TC检测各分支事务执行状态
如果都成功，通知所有RM提交事务
如果有失败，通知所有RM回滚事务
RM二阶段的工作：
接收TC指令，提交或回滚事务 xa模式的优点：
事务的强一致性，满足ACID原则。
常用数据库都支持，实现简单，并且没有代码侵入
xa模式的缺点：
因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差 依赖关系型数据库实现事务 实现XA模式
Seata的starter已经完成了XA模式的自动装配，实现非常简单，步骤如下： 修改application.yml文件（每个参与事务的微服务），开启XA模式：
配置seata的注册中心 seata: data-source-proxy-mode: XA # 选择XA模式 注意：是每一个微服务都需要
给发起全局事务的入口方法添加@GlobalTransactional注解，本例中是OrderServiceImpl中的create方法：
启动所有微服务，postman进行接口测试
先进行正确的测试，再继续错误的测试
错误设置，购买商品超出原来剩余的商品数就会让数据库报错测试是否可以事务回滚
注意：如果测试接口报错响应时间过长，那么就应该设置响应的时间大一点，如下图，然后重启seata
成功的可以查看seate的控制输出，可以看到事务回滚
IDEA输出
查看数据库的数据是否有被更新
2、AT模式 2.1、 AT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。 阶段一RM的工作：
注册分支事务 记录undo-log（数据快照，JSON格式） 执行业务sql并提交 报告事务状态
阶段二提交时RM的工作：
删除undo-log即可
阶段二回滚时RM的工作：
根据undo-log恢复数据到更新前
执行示例 例如，一个分支业务的SQL是这样的：update tb_account set money = money - 10 where id = 1
AT模式与XA模式最大的区别是什么
XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源。 XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚。 XA模式强一致；AT模式最终一致
2.2、AT模式的脏写问题 2.3、AT模式的写隔离 2.4、AT模式的优缺点 优点：
一阶段完成直接提交事务，释放数据库资源，性能比较好 利用全局锁实现读写隔离 没有代码侵入，框架自动完成回滚和提交 缺点：
两阶段之间属于软状态，属于最终一致 框架的快照功能会影响性能，但比XA模式要好很多 2.5、实现AT模式 AT模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。
1、创建相关的数据库文件
lock_table（全局锁）表导入到TC服务关联的数据库
DROPTABLEIFEXISTS`lock_table`;CREATETABLE`lock_table`(`row_key`varchar(128)CHARACTERSETutf8COLLATEutf8_general_ciNOTNULL,`xid`varchar(96)CHARACTERSETutf8COLLATEutf8_general_ciNULLDEFAULTNULL,`transaction_id`bigint(20)NULLDEFAULTNULL,`branch_id`bigint(20)NOTNULL,`resource_id`varchar(256)CHARACTERSETutf8COLLATEutf8_general_ciNULLDEFAULTNULL,`table_name`varchar(32)CHARACTERSETutf8COLLATEutf8_general_ciNULLDEFAULTNULL,`pk`varchar(36)CHARACTERSETutf8COLLATEutf8_general_ciNULLDEFAULTNULL,`gmt_create`datetimeNULLDEFAULTNULL,`gmt_modified`datetimeNULLDEFAULTNULL,PRIMARYKEY(`row_key`)USINGBTREE,INDEX`idx_branch_id`(`branch_id`)USINGBTREE)ENGINE=InnoDBCHARACTERSET=utf8COLLATE=utf8_general_ciROW_FORMAT=Compact;undo_log（记录快照）表导入到微服务关联的数据库
DROPTABLEIFEXISTS`undo_log`;CREATETABLE`undo_log`(`branch_id`bigint(20)NOTNULLCOMMENT'branch transaction id',`xid`varchar(100)CHARACTERSETutf8COLLATEutf8_general_ciNOTNULLCOMMENT'global transaction id',`context`varchar(128)CHARACTERSETutf8COLLATEutf8_general_ciNOTNULLCOMMENT'undo_log context,such as serialization',`rollback_info`longblobNOTNULLCOMMENT'rollback info',`log_status`int(11)NOTNULLCOMMENT'0:normal status,1:defense status',`log_created`datetime(6)NOTNULLCOMMENT'create datetime',`log_modified`datetime(6)NOTNULLCOMMENT'modify datetime',UNIQUEINDEX`ux_undo_log`(`xid`,`branch_id`)USINGBTREE)ENGINE=InnoDBCHARACTERSET=utf8COLLATE=utf8_general_ciCOMMENT='AT transaction mode undo table'ROW_FORMAT=Compact;2、修改application.yml文件，将事务模式修改为AT模式即可：
#配置seata的注册中心 seata: data-source-proxy-mode: AT #选择XA模式 3、重启并测试
查看当前的数据库库存数量继续超库存创建订单进行执行错误回滚测试
查看IDEA的错误日志
3、TCC模式 3.1、tcc模式原理 TCC模式与AT模式非常相似，每阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复。需要实现三个方法：
Try：资源的检测和预留； Confirm：完成资源操作业务；要求 Try 成功 Confirm 一定要能成功。 Cancel：预留资源释放，可以理解为try的反向操作。 举例，一个扣减用户余额的业务。假设账户A原来余额是100，需要余额扣减30元。
TCC的工作模型图：
3.2、小结 TCC模式的优点：
一阶段完成直接提交事务，释放数据库资源，性能好 相比AT模型，无需生成快照，无需使用全局锁，性能最强 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库 TCC模式的缺点：
有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦 软状态，事务是最终一致 需要考虑Confirm和Cancel的失败情况，做好幂等处理 3.2、TCC模式实现案例 不是所有的业务都适合TCC模式，如库存，金额等就比较适合
改造account-service服务，利用TCC实现分布式事务
需求如下：
修改account-service，编写try、confirm、cancel逻辑：
try业务：添加冻结金额，扣减可用金额 confirm业务：删除冻结金额 cancel业务：删除冻结金额，恢复可用金额 保证confirm、cancel接口的幂等性允许空回滚拒绝业务悬挂
TCC的空回滚和业务悬挂
当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执
行了cancel操作，这时cancel不能做回滚，就是空回滚。
对于已经空回滚的业务，如果以后继续执行try，就永远不可能confirm或cancel，这就是业务悬挂。应当阻止
执行空回滚后的try操作，避免悬挂
业务分析
为了实现空回滚、防止业务悬挂，以及幂等性要求。我们必须在数据库记录冻结金额的同时，记录当前事务id和执行状态，为此我们设计了一张表（添加在微服务的数据库seata-demo中）：
DROPTABLEIFEXISTS`account_freeze_tbl`;CREATETABLE`account_freeze_tbl`(`xid`varchar(128)CHARACTERSETutf8COLLATEutf8_general_ciNOTNULL,`user_id`varchar(255)CHARACTERSETutf8COLLATEutf8_general_ciNULLDEFAULTNULL,`freeze_money`int(11)UNSIGNEDNULLDEFAULT0,`state`int(1)NULLDEFAULTNULLCOMMENT'事务状态，0:try，1:confirm，2:cancel',PRIMARYKEY(`xid`)USINGBTREE)ENGINE=InnoDBCHARACTERSET=utf8COLLATE=utf8_general_ciROW_FORMAT=COMPACT;Try业务：记录冻结金额和事务状态到account_freeze表，扣减account表可用金额
Confirm业务：根据xid删除account_freeze表的冻结记录
Cancel业务：修改account_freeze表，冻结金额为0，state为2，修改account表，恢复可用金额
如何判断是否空回滚：cancel业务中，根据xid查询account_freeze，如果为null则说明try还没做，需要空回
滚
如何避免业务悬挂：try业务中，根据xid查询account_freeze ，如果已经存在则证明Cancel已经执行，拒绝执
行try业务
业务实现
1、声明TCC接口 @BusinessActionContextParameter()注解的参数才可以被BusinessActionContext获取到 package cn.itcast.account.service; import io.seata.rm.tcc.api.BusinessActionContext; import io.seata.rm.tcc.api.BusinessActionContextParameter; import io.seata.rm.tcc.api.LocalTCC; import io.seata.rm.tcc.api.TwoPhaseBusinessAction; /** * 项目名称：seata-demo * 描述：TCC实现接口 * * @author zhong * @date 2022-06-08 16:05 */ @LocalTCC public interface AccountTCCService { /** * 定义try,注释里面的值必须是与方法同名 * @param userId * @param money */ @TwoPhaseBusinessAction(name = "deduct",commitMethod = "confirm",rollbackMethod = "cancel") void deduct(@BusinessActionContextParameter(paramName = "userId") String userId, @BusinessActionContextParameter(paramName = "money")int money); /** * 定义Confirm * @param ctx 获取事务类型参数 * @return */ boolean confirm(BusinessActionContext ctx); /** * 定义Cancel * @param ctx * @return */ boolean cancel(BusinessActionContext ctx); } 说明：对于account_freeze_tbl数据库表的操作与其他业务一样的，使用MP的CURD进行快速开发，需要实体类、mapper
2.、创建接口实现类
package cn.itcast.account.service.impl; import cn.itcast.account.entity.AccountFreeze; import cn.itcast.account.mapper.AccountFreezeMapper; import cn.itcast.account.mapper.AccountMapper; import cn.itcast.account.service.AccountTCCService; import io.seata.core.context.RootContext; import io.seata.rm.tcc.api.BusinessActionContext; import lombok.extern.slf4j.Slf4j; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Transactional; /** * 项目名称：seata-demo * 描述： * * @author zhong * @date 2022-06-08 16:25 */ @Slf4j @Service public class AccountTCCServiceImpl implements AccountTCCService { /** * 注入可用余额dao */ @Autowired private AccountMapper accountMapper; /** * 注入冻结表dao */ @Autowired private AccountFreezeMapper freezeMapper; /** * 资源检测和预留 * 在数据库中设置了非负数字段限定，这里可以直接简化一步，如果为负数就会报错 * @param userId * @param money */ @Override @Transactional public void deduct(String userId, int money) { // 获取全局事务id String xid = RootContext.getXID(); // 业务悬挂判断，如果freeze中有冻结记录，一定是CANCEL执行过，我要拒绝业务 AccountFreeze accountFreeze = freezeMapper.selectById(xid); if(accountFreeze != null){ // 一定是CANCEL执行过，我要拒绝业务 return; } // 1、扣减可用余额 accountMapper.deduct(userId,money); // 2、记录冻结金额，事务状态 AccountFreeze freeze = new AccountFreeze(); freeze.setXid(xid); freeze.setUserId(userId); freeze.setState(AccountFreeze.State.TRY); freeze.setFreezeMoney(money); freezeMapper.insert(freeze); } @Override public boolean confirm(BusinessActionContext ctx) { // 1、获取事务id String xid = ctx.getXid(); // 2、根据事务id删除冻结数据 int count = freezeMapper.deleteById(xid); return count == 1; } @Override public boolean cancel(BusinessActionContext ctx) { // 0、查询冻结记录 String xid = ctx.getXid(); // 获取用户id String userId = ctx.getActionContext("userId").toString(); AccountFreeze freeze = freezeMapper.selectById(xid); // 空回滚的判断，为null代表没有try没有执行 if(freeze == null){ freeze = new AccountFreeze(); freeze.setXid(xid); freeze.setUserId(userId); freeze.setState(AccountFreeze.State.CANCEL); freeze.setFreezeMoney(0); freezeMapper.insert(freeze); } // 幂等判断 if(freeze.getState() == AccountFreeze.State.CANCEL){ // 已经执行过了一次CANCEL，无需重复处理 return true; } // 1、恢复可用余额 accountMapper.refund(freeze.getUserId(), freeze.getFreezeMoney()); // 2、将冻结金额清零，修改状态 freeze.setFreezeMoney(0); freeze.setState(AccountFreeze.State.CANCEL); int count = freezeMapper.updateById(freeze); return count == 1; } } 3、修改业务层连接方式 改为上面定义的TCCM业务实现
4、测试数据 直接测试错误的信息
当出现错误的时候会将会数据保存到数据库，如果是成功的那么就会删除
IDEA的报错输出
4、SAGA模式
Saga模式是SEATA提供的长事务解决方案。也分为两个阶段：
一阶段：直接提交本地事务 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚 Saga模式优点：
事务参与者可以基于事件驱动实现异步调用，吞吐高
一阶段直接提交事务，无锁，性能好
不用编写TCC中的三个阶段，实现简单
缺点：
软状态持续时间不确定，时效性差 没有锁，没有事务隔离，会有脏写</content></entry><entry><title>分布式事务：两阶段提交与三阶段提交</title><url>/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E4%B8%8E%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/</url><categories><category>分布式事务</category><category>cap理论</category></categories><tags><tag>分布式事务</tag><tag>cap理论</tag></tags><content type="html"> 分布式事务：两阶段提交与三阶段提交
分布式事务：两阶段提交与三阶段提交 在分布式系统中著有 CAP 理论，该理论由加州大学伯克利分校的 Eric Brewer 教授提出，阐述了在一个分布式系统中不可能同时满足 一致性（Consistency）、可用性（Availability），以及 分区容错性（Partition tolerance）。
C：一致性 在分布式系统中数据往往存在多个副本，一致性描述的是这些副本中的数据在内容和组织上的一致。
A：可用性 可用性描述了系统对用户的服务能力，所谓可用是指在用户容忍的时间范围内返回用户期望的结果。
P：分区容错性 分布式系统通常由多个节点构成，由于网络是不可靠的，所以存在分布式集群中的节点因为网络通信故障导致被孤立成一个个小集群的可能性，即网络分区，分区容错性要求在出现网络分区时系统仍然能够对外提供一致性的可用服务。
对于一个分布式系统而言，我们要始终假设网络是不可靠的，因此分区容错性是对一个分布式系统最基本的要求，我们的切入点更多的是尝试在可用性和一致性之间寻找一个平衡点，但这也并非要求我们在系统设计时一直建立在网络出现分区的场景之上，然后对一致性和可用性在选择时非此即彼。实际上 Eric Brewer 在 2012 年就曾指出 CAP 理论证明不能同时满足一致性、可用性，以及分区容错性的观点在实际系统设计指导上存在一定的误导性。传统对于 CAP 理论的理解认为在设计分布式系统时必须满足 P，然后在 C 和 A 之间进行取舍，这是片面的，实际中网络出现分区的可能性还是比较小的，尤其是目前网络环境正在变得越来越好，甚至许多系统都拥有专线支持，所以在网络未出现分区时，还是应该兼顾 A 和 C；另外就是对于一致性、可用性，以及分区容错性三者在度量上也应该有一个评定范围，最简单的以可用性来说，当有多少占比请求出现响应超时才可以被认为是不满足可用性，而不是一出现超时就认为是不可用的；最后我们需要考虑的一点就是分布式系统一般都是一个比较大且复杂的系统，我们应该从更小的粒度上对各个子系统进行评估和设计，而不是简单的从整体上认为需要满足 P，而在 A 和 C 之间做取舍，一些子系统可能需要尽可能同时满足三者。
让分布式集群始终对外提供可用的一致性服务一直是富有挑战和趣味的一项任务。暂且抛开可用性，拿一致性来说，对于关系型数据库我们通常利用事务来保证数据的强一致性，当我们的数据量越来越大，大到单库已经无法承担时，我们不得不采取分库分表的策略对数据库实现水平拆分，构建分布式数据库集群，这样可以将一个数据库的压力分摊到多个数据库，极大的提升了数据库的存储和响应能力，但是拆分之后也为我们使用数据库带来了许多的限制，比如主键的全局唯一、联表查询、数据聚合等等，另外一个相当棘手的问题就是数据库的事务由原先的单库事务变成了现在的分布式事务。
分布式事务的实现并不是很难，比如下文要展开的两阶段提交（2PC：Two-Phrase Commit）和三阶段提交（3PC：Three-Phrase Commit）都给我们提供了思路，但是如果要保证数据的强一致性，并要求对外提供可用的服务，就变成了一个几乎不可能的任务（至少目前是），因此很多分布式系统对于数据强一致性都敬而远之。
两阶段提交协议（2PC：Two-Phrase Commit） 两阶段提交协议的目标在于在分布式系统中保证数据的一致性，许多分布式系统采用该协议提供对分布式事务的支持（提供但不一定有人用，呵呵~）。顾名思义，该协议将一个分布式的事务过程拆分成两个阶段：投票阶段和事务提交阶段。为了让整个数据库集群能够正常的运行，该协议指定了一个“协调者”单点，用于协调整个数据库集群的运行，为了简化描述，我们将数据库里面的各个节点称为“参与者”，三阶段提交协议中同样包含“协调者”和“参与者”这两个定义。
第一阶段：投票阶段
该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下：
协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果。 事务参与者收到请求之后，执行事务，但不提交，并记录事务日志。 参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。 第二阶段：事务提交阶段
在第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在三种可能：
所有的参与者回复能够正常执行事务 一个或多个参与者回复事务执行失败 协调者等待超时。 对于第一种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：
协调者向各个参与者发送commit通知，请求提交事务。 参与者收到事务提交通知之后，执行commit操作，然后释放占有的资源。 参与者向协调者返回事务commit结果信息。 对于第二、三种情况，协调者均认为参与者无法正常成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下：
协调者向各个参与者发送事务rollback通知，请求回滚事务。 参与者收到事务回滚通知之后，执行rollback操作，然后释放占有的资源。 参与者向协调者返回事务rollback结果信息。 两阶段提交协议解决的是分布式数据库数据强一致性问题，其原理简单，易于实现，但是缺点也是显而易见的，主要缺点如下：
单点问题 协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，那么就会影响整个数据库集群的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。
同步阻塞 两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率及其低下。
数据不一致性 两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。
三阶段提交协议（2PC：Three-Phrase Commit） 针对两阶段提交存在的问题，三阶段提交协议通过引入一个“预询盘”阶段，以及超时策略来减少整个集群的阻塞时间，提升系统性能。三阶段提交的三个阶段分别为：can_commit，pre_commit，do_commit。
第一阶段：can_commit
该阶段协调者会去询问各个参与者是否能够正常执行事务，参与者根据自身情况回复一个预估值，相对于真正的执行事务，这个过程是轻量的，具体步骤如下：
协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复 各个参与者依据自身状况回复一个预估值，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否则返回否定信息 第二阶段：pre_commit
本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有三种：
所有的参与者都返回确定信息 一个或多个参与者返回否定信息 协调者等待超时 针对第一种情况，协调者会向所有参与者发送事务执行请求，具体步骤如下：
协调者向所有的事务参与者发送事务执行通知 参与者收到通知后，执行事务，但不提交 参与者将事务执行情况返回给客户端 在上面的步骤中，如果参与者等待超时，则会中断事务。 针对第二、三种情况，协调者认为事务无法正常执行，于是向各个参与者发出abort通知，请求退出预备状态，具体步骤如下：
协调者向所有事务参与者发送abort通知 参与者收到通知后，中断事务 第三阶段：do_commit
如果第二阶段事务未中断，那么本阶段协调者将会依据事务执行返回的结果来决定提交或回滚事务，分为三种情况：
所有的参与者都能正常执行事务 一个或多个参与者执行事务失败 协调者等待超时 针对第一种情况，协调者向各个参与者发起事务提交请求，具体步骤如下：
协调者向所有参与者发送事务commit通知 所有参与者在收到通知之后执行commit操作，并释放占有的资源 参与者向协调者反馈事务提交结果 针对第二、三种情况，协调者认为事务无法正常执行，于是向各个参与者发送事务回滚请求，具体步骤如下：
协调者向所有参与者发送事务rollback通知 所有参与者在收到通知之后执行rollback操作，并释放占有的资源 参与者向协调者反馈事务提交结果 在本阶段如果因为协调者或网络问题，导致参与者迟迟不能收到来自协调者的commit或rollback请求，那么参与者将不会如两阶段提交中那样陷入阻塞，而是等待超时后继续commit。相对于两阶段提交虽然降低了同步阻塞，但仍然无法避免数据的不一致性。
在分布式数据库中，如果期望达到数据的强一致性，那么服务基本没有可用性可言，这也是为什么许多分布式数据库提供了跨库事务，但也只是个摆设的原因，在实际应用中我们更多追求的是数据的弱一致性或最终一致性，为了强一致性而丢弃可用性是不可取的。</content></entry><entry><title>Java IO模型</title><url>/post/java-io%E6%A8%A1%E5%9E%8B/</url><categories><category>java base</category><category>IO</category></categories><tags><tag>java</tag><tag>IO模型</tag><tag>底层原理</tag></tags><content type="html"> Java IO模型
Java IO模型 1. 引言 同步异步I/O，阻塞非阻塞I/O是程序员老生常谈的话题了，也是自己一直以来懵懵懂懂的一个话题。比如：何为同步异步？何为阻塞与非阻塞？二者的区别在哪里？阻塞在何处？为什么会有多种IO模型，分别用来解决问题？常用的框架采用的是何种I/O模型？各种IO模型的优劣势在哪里，适用于何种应用场景？
简而言之，对于I/O的认知，不能仅仅停留在字面上认识，了解内部玄机，才能深刻理解I/O，才能看清I/O相关问题的本质。
2. I/O 的定义 I/O 的全称是Input/Output。虽常谈及I/O，但想必你也一时不能给出一个完整的定义。搜索了谷歌，发现也尽是些冗长的论述。要想厘清I/O这个概念，我们需要从不同的视角去理解它。
2.1. 计算机视角 冯•诺伊曼计算机的基本思想中有提到计算机硬件组成应为五大部分：控制器，运算器，存储器，输入和输出。其中输入是指将数据输入到计算机的设备，比如键盘鼠标；输出是指从计算机中获取数据的设备，比如显示器；以及既是输入又是输出设备，硬盘，网卡等。
用户通过操作系统才能完成对计算机的操作。计算机启动时，第一个启动的程序是操作系统的内核，它将负责计算机的资源管理和进程的调度。换句话说：操作系统负责从输入设备读取数据并将数据写入到输出设备。
所以I/O之于计算机，有两层意思：
I/O设备 对I/O设备的数据读写 对于一次I/O操作，必然涉及2个参与方，一个输入端，一个输出端，而又根据参与双方的设备类型，我们又可以分为磁盘I/O，网络I/O（一次网络的请求响应，网卡）等。
2.2. 程序视角 应用程序作为一个文件保存在磁盘中，只有加载到内存到成为一个进程才能运行。应用程序运行在计算机内存中，必然会涉及到数据交换，比如读写磁盘文件，访问数据库，调用远程API等等。但我们编写的程序并不能像操作系统内核一样直接进行I/O操作。
因为为了确保操作系统的安全稳定运行，操作系统启动后，将会开启保护模式：将内存分为内核空间（内核对应进程所在内存空间）和用户空间，进行内存隔离。我们构建的程序将运行在用户空间，用户空间无法操作内核空间，也就意味着用户空间的程序不能直接访问由内核管理的I/O，比如：硬盘、网卡等。
但操作系统向外提供API，其由各种类型的系统调用（System Call）组成，以提供安全的访问控制。 所以应用程序要想访问内核管理的I/O，必须通过调用内核提供的系统调用(system call）进行间接访问。
所以I/O之于应用程序来说，强调的通过向内核发起系统调用完成对I/O的间接访问。换句话说应用程序发起的一次IO操作实际包含两个阶段：
IO调用阶段：应用程序进程向内核发起系统调用 IO执行阶段：内核执行IO操作并返回 2.1. 准备数据阶段：内核等待I/O设备准备好数据 2.2. 拷贝数据阶段：将数据从内核缓冲区拷贝到用户空间缓冲区 怎么理解准备数据阶段呢？ 对于写请求：等待系统调用的完整请求数据，并写入内核缓冲区； 对于读请求：等待系统调用的完整请求数据；（若请求数据不存在于内核缓冲区）则将外围设备的数据读入到内核缓冲区。
而应用程序进程在发起IO调用至内核执行IO返回之前，应用程序进程/线程所处状态，就是我们下面要讨论的第二个话题阻塞IO与非阻塞IO。
3. IO 模型之阻塞I/O(BIO) 应用程序中进程在发起IO调用后至内核执行IO操作返回结果之前，若发起系统调用的线程一直处于等待状态，则此次IO操作为阻塞IO。阻塞IO简称BIO，Blocking IO。其处理流程如下图所示：
从上图可知当用户进程发起IO系统调用后，内核从准备数据到拷贝数据到用户空间的两个阶段期间用户调用线程选择阻塞等待数据返回。
因此BIO带来了一个问题：如果内核数据需要耗时很久才能准备好，那么用户进程将被阻塞，浪费性能。为了提升应用的性能，虽然可以通过多线程来提升性能，但线程的创建依然会借助系统调用，同时多线程会导致频繁的线程上下文的切换，同样会影响性能。所以要想解决BIO带来的问题，我们就得看到问题的本质，那就是阻塞二字。
4. IO 模型之非阻塞I/O(NIO) 那解决方案自然也容易想到，将阻塞变为非阻塞，那就是用户进程在发起系统调用时指定为非阻塞，内核接收到请求后，就会立即返回，然后用户进程通过轮询的方式来拉取处理结果。也就是如下图所示：
应用程序中进程在发起IO调用后至内核执行IO操作返回结果之前，若发起系统调用的线程不会等待而是立即返回，则此次IO操作为非阻塞IO模型。非阻塞IO简称NIO，Non-Blocking IO。
然而，非阻塞IO虽然相对于阻塞IO大幅提升了性能，但依旧不是完美的解决方案，其依然存在性能问题，也就是频繁的轮询导致频繁的系统调用，会耗费大量的CPU资源。比如当并发很高时，假设有1000个并发，那么单位时间循环内将会有1000次系统调用去轮询执行结果，而实际上可能只有2个请求结果执行完毕，这就会有998次无效的系统调用，造成严重的性能浪费。有问题就要解决，那NIO问题的本质就是频繁轮询导致的无效系统调用。
5. IO模型之IO多路复用 解决NIO的思路就是降解无效的系统调用，如何降解呢？我们一起来看看以下几种IO多路复用的解决思路。
5.1. IO多路复用之select/poll 系统调用 select()或Poll()会一直阻塞，直到一个或多个文件描述符集合成为就绪态。
select()函数：
poll()函数：
系统调用 poll()执行的任务同 select()很相似。两者间主要的区别在于我们要如何指定待检查的文件描述符。在 select()中，我们提供三个集合，在每个集合中标明我们感兴趣的文件描述符。而在 poll()中我们提供一列文件描述符，并在每个文件描述符上标明我们感兴趣的事件
select()和 poll()之间的一些区别：
select()所使用的数据类型 fd_set 对于被检查的文件描述符数量有一个上限，在 Linux 下，这个上限值默认为 1024，修改这个上限需要重新编译。与之相反，poll()对于被检查的文件描述符数量本质上是没有限制的
select()的参数 fd_set 同时也是保存调用结果的地方，如果要在循环中重复调用select()的话，我们必须每次都要重新初始化 fd_set。而 poll()通过独立的两个字段 events（针对输入）和 revents（针对输出）来处理，从而避免每次都要重新初始化参数
Select是内核提供的系统调用，它支持一次查询多个系统调用的可用状态，当任意一个结果状态可用时就会返回，用户进程再发起一次系统调用进行数据读取。换句话说，就是NIO中N次的系统调用，借助Select，只需要发起一次系统调用就够了。其IO流程如下所示：
select/epoll 虽然解决了NIO重复无效系统调用用的问题，但同时又引入了新的问题。问题是：
每次调用 select()或 poll()，内核都必须检查所有被指定的文件描述符，看它们是否处于就绪态。当检查大量处于密集范围内的文件描述符时，该操作耗费的时间将大大超过接下来的操作
每次调用 select()或 poll()时，程序都必须传递一个表示所有需要被检查的文件描述符的数据结构到内核，内核检查过描述符后，修改这个数据结构并返回给程序。内核与用户进程数据传递较多
select()或 poll()调用完成后，程序必须检查返回的数据结构中的每个元素，以此查明
哪个文件描述符处于就绪态了
换句话说，select/poll虽然减少了用户进程的发起的系统调用，但内核的工作量只增不减。在高并发的情况下，内核的性能问题依旧。所以select/poll的问题本质是：内核存在无效的循环遍历。
5.2. IO多路复用之epoll 针对select/pool引入的问题，我们把解决问题的思路转回到内核上，如何减少内核重复无效的循环遍历呢？变主动为被动，基于事件驱动来实现。其流程图如下所示：
epoll API 由以下 3 个系统调用组成：
epoll_create()创建一个 epoll 实例，返回代表该实例的文件描述符。
epoll_ctl()操作同 epoll 实例相关联的兴趣列表。通过 epoll_ctl()，我们可以增加新的描述符到列表中，将已有的文件描述符从该列表中移除，以及修改代表文件描述符上事件类型的位掩码。
epoll_wait()返回与 epoll 实例相关联的就绪列表中的成员。单个 epoll_wait()调用能返回多个就绪态文件描述符的信息。
epoll与select/poll比较：
每次调用 select()和 poll()时，内核必须检查所有在调用中指定的文件描述符。与之相反，当通过 epoll_ctl()指定了需要监视的文件描述符时，内核会在与打开的文件描述上下文相关联的列表中记录该描述符。之后每当执行 I/O 操作使得文件描述符成为就绪态时，内核就在 epoll 描述符的就绪列表中添加一个元素。（单个打开的文件描述上下文中的一次 I/O 事件可能导致与之相关的多个文件描述符成为就绪态。）之后的epoll_wait()调用从就绪列表中简单地取出这些元素。
每次调用 select()或 poll()时，我们传递一个标记了所有待监视的文件描述符的数据结构给内核，调用返回时，内核将所有标记为就绪态的文件描述符的数据结构再传回给我们。与之相反，在 epoll 中我们使用 epoll_ctl()在内核空间中建立一个数据结构，该数据结构会将待监视的文件描述符都记录下来。一旦这个数据结构建立完成，稍后每次调用 epoll_wait()时就不需要再传递任何与文件描述符有关的信息给内核了，而调用返回的信息中只包含那些已经处于就绪态的描述符
epoll，已经大大优化了IO的执行效率，但在IO执行的第一阶段：数据准备阶段都还是被阻塞的。所以这是一个可以继续优化的点。
6. IO 模型之信号驱动IO(SIGIO) 信号驱动IO与BIO和NIO最大的区别就在于，在IO执行的数据准备阶段，不会阻塞用户进程。 如下图所示：当用户进程需要等待数据的时候，会向内核发送一个信号，告诉内核我要什么数据，然后用户进程就继续做别的事情去了，而当内核中的数据准备好之后，内核立马发给用户进程一个信号，说”数据准备好了，快来查收“，用户进程收到信号之后，立马调用recvfrom，去查收数据。
乍一看，信号驱动式I/O模型有种异步操作的感觉，但是在IO执行的第二阶段，也就是将数据从内核空间复制到用户空间这个阶段，用户进程还是被阻塞的。
综上，你会发现，不管是BIO还是NIO还是SIGIO，它们最终都会被阻塞在IO执行的第二阶段。 那如果能将IO执行的第二阶段变成非阻塞，那就完美了。
7. IO 模型之异步IO(AIO) 异步IO真正实现了IO全流程的非阻塞。用户进程发出系统调用后立即返回，内核等待数据准备完成，然后将数据拷贝到用户进程缓冲区，然后发送信号告诉用户进程IO操作执行完毕（与SIGIO相比，一个是发送信号告诉用户进程数据准备完毕，一个是IO执行完毕）。其流程如下：
所以，之所以称为异步IO，取决于IO执行的第二阶段是否阻塞。因此前面讲的BIO，NIO和SIGIO均为同步IO。</content></entry><entry><title>NIO的阻塞IO模式、非阻塞IO模式、IO多路复用模式的使用</title><url>/post/nio%E7%9A%84%E9%98%BB%E5%A1%9Eio%E6%A8%A1%E5%BC%8F%E9%9D%9E%E9%98%BB%E5%A1%9Eio%E6%A8%A1%E5%BC%8Fio%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E6%A8%A1%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url><categories><category>java base</category><category>IO</category></categories><tags><tag>java</tag><tag>IO模型</tag><tag>java NIO</tag></tags><content type="html"> NIO的阻塞IO模式、非阻塞IO模式、IO多路复用模式的使用
NIO的阻塞IO模式、非阻塞IO模式、IO多路复用模式的使用 NIO虽然称为Non-Blocking IO（非阻塞IO），但它支持阻塞IO、非阻塞IO和IO多路复用模式这几种方式的使用。
同步阻塞模式 NIO服务器端
@Slf4j public class NIOBlockingServer { public static void main(String[] args) throws IOException { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(true); // 设置SocketChannel为阻塞模式（默认就是阻塞模式） serverSocketChannel.bind(new InetSocketAddress(8080)); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); while (true) { // 如果没有接收到新的线程，这里会阻塞，无法及时处理其他已连接Channel的请求 SocketChannel socketChannel = serverSocketChannel.accept(); log.info("receive connection from client. client:{}",socketChannel.getRemoteAddress()); socketChannel.configureBlocking(true); // 设置SocketChannel为阻塞模式（默认就是阻塞模式） // 如果读不到数据，这里会阻塞，无法及时处理其他Channel的请求 int length = socketChannel.read(byteBuffer); log.info("receive message from client. client:{} message:{}",socketChannel.getRemoteAddress(),new String(byteBuffer.array(),0,length,"UTF-8")); byteBuffer.clear(); } } } NIO客户端
@Slf4j public class NIOClient { @SneakyThrows public static void main(String[] args) { SocketChannel socketChannel=SocketChannel.open(); try { socketChannel.connect(new InetSocketAddress("127.0.0.1", 8080)); log.info("client connect finished"); ByteBuffer writeBuffer=ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)); socketChannel.write(writeBuffer); log.info("client send finished"); } catch (Exception e) { e.printStackTrace(); } finally { socketChannel.close(); } } } NIO阻塞模式的使用，乍一看怎么跟BIO的使用方法很像？不是很像，简直是一模一样~
以Run模式启动NIO服务端 在客户端的 socketChannel.write(writeBuffer);处打上断点，以Debug模式运行一个客户端A，执行到断点时，服务端已经接收到客户端A的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:64334 ） 再以Debug模式运行一个客户端B，服务端没反应，因为这时客户端A还没发送数据，所以服务端目前是在 int length = socketChannel.read(byteBuffer) 的地方阻塞了（还在等着接收客户端A发送数据） 再以Debug模式运行一个客户端C，服务端同样没反应 让客户端A继续运行完，发现服务端读取到客户端A的数据（打印了receive message from client. client:/127.0.0.1:64334 message:hello ）后，才能接收到客户端B的连接（打印了receive connection from client. client:/127.0.0.1:64358 ） 让客户端B继续运行完，发现服务端读取到客户端B的数据（打印了receive message from client. client:/127.0.0.1:64358 message:hello ）后，才能接收到客户端C的连接（打印了receive connection from client. client:/127.0.0.1:64369 ） 因此，NIO的阻塞IO模式跟BIO一样，最大的缺点就是阻塞。
同步非阻塞模式 通过前面的学习我们知道，异步IO和同步IO最大的区别就是： 同步IO在做完一件事（比如：处理客户端连接请求+写请求）之前，只能等待，无法做其他事情； 而异步是在客户端某个事件没有就绪时，我服务端可以先处理其他的客户端请求，不用一直等着。
NIO服务端
@Slf4j public class NIONonBlockingServer { public static void main(String[] args) throws IOException, InterruptedException { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(8080)); List&lt;SocketChannel> socketChannelList = new ArrayList&lt;>(); while (true) { // 如果没有接收到新的线程，这里不会阻塞，会返回null，可以让线程继续处理其他Channel的请求 SocketChannel socketChannel = serverSocketChannel.accept(); if (Objects.nonNull(socketChannel)) { log.info("receive connection from client. client:{}", socketChannel.getRemoteAddress()); socketChannel.configureBlocking(false); socketChannelList.add(socketChannel); } for (SocketChannel channel : socketChannelList) { // 如果没有读到数据，这里也不会阻塞，会返回0，表示没有读到数据，可以让线程继续处理其他Channel的请求 ByteBuffer byteBuffer = ByteBuffer.allocate(10); int length = channel.read(byteBuffer); if (length > 0) { log.info("receive message from client. client:{} message:{}", channel.getRemoteAddress() , new String(byteBuffer.array(), 0, length, "UTF-8")); } byteBuffer.clear(); } // 为了避免没有客户端请求时循环过于频繁，把所有就绪的事件循环处理完后，停顿1秒再继续执行 Thread.sleep(1000); } } } NIO客户端
@Slf4j public class NIOClient { @SneakyThrows public static void main(String[] args) { SocketChannel socketChannel=SocketChannel.open(); try { socketChannel.connect(new InetSocketAddress("127.0.0.1", 8080)); log.info("client connect finished"); ByteBuffer writeBuffer=ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)); socketChannel.write(writeBuffer); log.info("client send finished"); } catch (Exception e) { e.printStackTrace(); } finally { socketChannel.close(); } } } 以Run模式启动NIO服务端
在客户端的 socketChannel.write(writeBuffer);处打上断点，以Debug模式运行一个客户端A，执行到断点时，服务端已经接收到客户端A的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:53004 ）
再以Debug模式运行一个客户端B，服务端也接收到客户端B的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:53032 ）
再以Debug模式运行一个客户端C，服务端也接收到客户端B的请求（在控制台打印了 receive connection from client. client:/127.0.0.1:53032 ） 如下图：
继续运行客户端A、B、C，可以看到服务端也可以正常接收它们发来的数据：
2022-07-30 16:31:07.987 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive connection from client. client:/127.0.0.1:53004 2022-07-30 16:31:13.014 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive connection from client. client:/127.0.0.1:53032 2022-07-30 16:31:18.039 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive connection from client. client:/127.0.0.1:53060 2022-07-30 16:33:12.919 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive message from client. client:/127.0.0.1:53004 message:hello 2022-07-30 16:33:18.940 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive message from client. client:/127.0.0.1:53032 message:hello 2022-07-30 16:33:19.942 INFO [danny-codebase,,,,,main] com.code.io.blog.NIONonBlockingServer - receive message from client. client:/127.0.0.1:53060 message:hello NIO非阻塞模式这种用法跟 BIO多线程处理请求的方式类似，让服务端可以同时处理多个客户端请求，即使某一个客户端的读/写事件未就绪也不会阻塞线程（比如上面服务端执行serverSocketChannel.accept()时如果没有客户端连接不会阻塞而是会返回null；执行channel.read(byteBuffer)时如果读不到数据不会阻塞而是会返回0），而是会继续处理其他客户端的请求。
需要注意的是，这里的非阻塞，是指serverSocketChannel执行accept()、socketChannel执行read()时是非阻塞的（会立刻返回结果）。但是在客户端有就绪事件，处理客户端的请求时，比如服务端接收客户端连接请求的过程、服务端读取数据（数据拷贝）的过程，是阻塞的。
IO多路复用模式 看完NIO非阻塞模式的使用方法你是不是就觉得万无一失了？No！这种方式也有一个很大的缺点就是，当一直没有客户端事件就绪时，服务端线程就会一直循环，白白占用了CPU资源，所以上面代码中为了减小CPU消耗，在每次处理完所有Channel的就绪事件后，会调用Thread.sleep(1000);让服务端线程休息1秒再执行。那有没有什么方法可以在没有客户端事件就绪时，服务端线程等待，当有了请求再继续工作呢？
有，那就是IO多路复用模式，相对于上面的非阻塞模式，IO多路复用模式主要是引入了Selector选择器，且需要把Channel设置为非阻塞模式（默认是阻塞的）。
Selector可以作为一个观察者，可以把已知的Channel（无论是服务端用来监听客户端连接的ServerSocketChannel，还是服务端和客户端用来读写数据的SocketChannel）及其感兴趣的事件（READ、WRITE、CONNECT、ACCEPT）包装成一个SelectionKey，注册到Selector上，Selector就会监听这些Channel注册的事件（监听的时候如果没有事件就绪，Selector所在线程会被阻塞），一旦有事件就绪，就会返回这些事件的列表，继而服务端线程可以依次处理这些事件。
服务端例子如下：
@Slf4j public class NioSelectorServer { public static void main(String[] args) throws Exception { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress("127.0.0.1", 8080), 50); Selector selector = Selector.open(); SelectionKey serverSocketKey = serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { // 从Selector中获取事件（客户端连接、客户端发送数据……），如果没有事件发生，会阻塞 int count = selector.select(); log.info("select event count:" + count); Set&lt;SelectionKey> selectionKeys = selector.selectedKeys(); // Iterator&lt;SelectionKey> iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); // 有客户端请求建立连接 if (selectionKey.isAcceptable()) { handleAccept(selectionKey); } // 有客户端发送数据 else if (selectionKey.isWritable()) { handleRead(selectionKey); } // select 在事件发生后，就会将相关的 key 放入 Selector 中的 selectedKeys 集合，但不会在处理完后从 selectedKeys 集合中移除，需要我们自己手动删除 iterator.remove(); } } } private static void handleAccept(SelectionKey selectionKey) throws IOException { ServerSocketChannel serverSocketChannel = (ServerSocketChannel) selectionKey.channel(); SocketChannel socketChannel = serverSocketChannel.accept(); if (Objects.nonNull(socketChannel)) { log.info("receive connection from client. client:{}", socketChannel.getRemoteAddress()); // 设置客户端Channel为非阻塞模式，否则在执行socketChannel.read()时会阻塞 socketChannel.configureBlocking(false); Selector selector = selectionKey.selector(); socketChannel.register(selector, SelectionKey.OP_READ); } } private static void handleRead(SelectionKey selectionKey) throws IOException { SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(8); int length = socketChannel.read(readBuffer); if (length > 0) { log.info("receive message from client. client:{} message:{}", socketChannel.getRemoteAddress() , new String(readBuffer.array(), 0, length, "UTF-8")); } else if (length == -1) { // 客户端正常断开(socketChannel.close())时，在服务端也会产生读事件，且读到的数据长度为-1 socketChannel.close(); return; } } } SelectionKey表示一对Selector和Channel的关系，从SelectionKey中可以获得已经准备好数据的Channel。 SelectionKey.OP_ACCEPT —— 针对服务端，接收连接就绪事件，表示服务器监听到了客户连接 SelectionKey.OP_CONNECT —— 针对客户端，连接就绪事件，表示客户与服务器的连接已经建立就绪 SelectionKey.OP_READ —— 读就绪事件，表示通道中已经有了可读的数据，可以执行读操作 SelectionKey.OP_WRITE —— 写就绪事件，表示已经可以向通道写数据了（通道目前可以用于写操作）
以Debug模式启动服务端，初始化完ServerSocketChannel后，手动设置了ServerSocketChannel的阻塞模式为非阻塞，并且为ServerSocketChannel在Selector上注册了一个ACCEPT事件，当有客户端向服务端请求连接时会触发该事件。当执行到int count = selector.select();时，服务端阻塞，等待客户端连接 以Debug模式运行一个客户端A，当执行完socketChannel.connect(new InetSocketAddress("127.0.0.1", 8080));时，服务端selector.select()方法返回了就绪的IO事件数量为1（就是客户端A的请求连接事件） 当服务端接收到客户端A的连接后，把客户端连接——SocketChannel设置为非阻塞，并且在Selector实例上注册一个读事件，这时客户端连接SocketChannel会对读事件感兴趣，当这个客户端发送数据时，会唤醒Selector。当服务端下一次循环再次执行到int count = selector.select();时，会再次阻塞，等待客户端的IO事件 客户端A继续执行完socketChannel.write(writeBuffer);后，服务端selector.select()方法返回了就绪的IO事件数量为1（就是客户端A的写数据事件） 当服务端在读取客户端A的数据时（下次执行selector.select()之前），同时启动客户端B、客户端C（或者再多开几个线程，否则可能模拟不出来），等服务端下次执行selector.select()时，返回的就绪的IO事件数量可能有多个，然后可以根据 selectionKey.isAcceptable()、 selectionKey.isReadable()、selectionKey.isWritable()来分别处理对应的事件。 但是，如果客户端连接或读写时间过长，也只能一个一个处理。NIO只是把BIO中等待的时间（比如socket.getInputStream().read()）充分利用，为在多核CPU机器上的运行提高了效率，可以用多线程+NIO的IO多路复用模式来处理。</content></entry><entry><title>装饰者模式</title><url>/post/%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/</url><categories><category>设计模式</category></categories><tags><tag>设计模式</tag></tags><content type="html"> 装饰者模式
装饰者模式 ​ 装饰者模式的核心思想是通过创建一个装饰对象（即装饰者），动态扩展目标对象的功能，并且不会改变目标对象的结构，提供了一种比继承更灵活的替代方案。
​ 我们在进行软件开发时要想实现可维护、可扩展，就需要尽量复用代码，并且降低代码的耦合度，而设计模式就是一种可以提高代码可复用性、可维护性、可扩展性以及可读性的解决方案。
大家熟知的23种设计模式，可以分为创建型模式、结构型模式和行为型模式三大类。其中，结构型模式用于设计类或对象的组合方式，以便实现更加灵活的结构。结构型模式又可划分为类结构型模式和对象结构型模式，前者通过继承来组合接口或类，后者通过组合或聚合来组合对象
装饰者模式的核心思想是通过创建一个装饰对象（即装饰者），动态扩展目标对象的功能，并且不会改变目标对象的结构，提供了一种比继承更灵活的替代方案。需要注意的是，装饰对象要与目标对象实现相同的接口，或继承相同的抽象类；另外装饰对象需要持有目标对象的引用作为成员变量，而具体的赋能任务往往通过带参构造方法来完成。
▐ 结构 装饰者模式包含四种类，分别是抽象构件类、具体构件类、抽象装饰者类、具体装饰者类，它们各自负责完成特定任务，并且相互之间存在紧密联系。
▐ 使用 有了上述的基本概念，我们将装饰者模式的使用步骤概括为：
step1：创建抽象构件类，定义目标对象的抽象类、将要扩展的功能定义成抽象方法；
step2：创建具体构件类，定义目标对象的实现类，实现抽象构件中声明的抽象方法；
step3：创建抽象装饰者类，维护一个指向抽象构件的引用，并传入构造函数以调用具体构件的实现方法，给具体构件增加功能；
step4：创建具体装饰者类，可以调用抽象装饰者类中定义的方法，并定义若干个新的方法，扩展目标对象的功能。
我们在淘宝上购物时，经常会遇到很多平台和商家的优惠活动：满减、聚划算站内的百亿补贴券、店铺折扣等等。那么在商品自身原价的基础上，叠加了多种优惠活动后，后台应该怎样计算最终的下单结算金额呢？下面就以这种优惠叠加结算的场景为例，简单分析装饰者模式如何使用。
// 定义抽象构件：抽象商品 public interface ItemComponent { // 商品价格 public double checkoutPrice(); } // 定义具体构件：具体商品 public class ConcreteItemCompoment implements ItemComponent { // 原价 @Override public double checkoutPrice() { return 200.0; } } // 定义抽象装饰者：创建传参(抽象构件)构造方法，以便给具体构件增加功能 public abstract class ItemAbsatractDecorator implements ItemComponent { protected ItemComponent itemComponent; public ItemAbsatractDecorator(ItemComponent myItem) { this.itemComponent = myItem; } @Overrid public double checkoutPrice() { return this.itemComponent.checkoutPrice(); } } // 定义具体装饰者A：增加店铺折扣八折 public class ShopDiscountDecorator extends ItemAbsatractDecorator { public ShopDiscountDecorator(ItemComponent myItem) { super(myItem); } @Override public double checkoutPrice() { return 0.8 * super.checkoutPrice(); } } // 定义具体装饰者B：增加满200减20功能，此处忽略判断逻辑 public class FullReductionDecorator extends ItemAbsatractDecorator { public FullReductionDecorator(ItemComponent myItem) { super(myItem); } @Override public double checkoutPrice() { return super.checkoutPrice() - 20; } } // 定义具体装饰者C：增加百亿补贴券50 public class BybtCouponDecorator extends ItemAbsatractDecorator { public BybtCouponDecorator(ItemComponent myItem) { super(myItem); } @Override public double checkoutPrice() { return super.checkoutPrice() - 50; } } //客户端调用 public class userPayForItem() { public static void main(String[] args) { ItemCompoment item = new ConcreteItemCompoment(); System.out.println("宝贝原价：" + item.checkoutPrice() + " 元"）; item = new ShopDiscountDecorator(item); System.out.println("使用店铺折扣后需支付：" + item.checkoutPrice() + " 元"）; item = new FullReductionDecorator(item); System.out.println("使用满200减20后需支付：" + item.checkoutPrice() + " 元"）; item = new BybtCouponDecorator(item); System.out.println("使用百亿补贴券后需支付：" + item.checkoutPrice() + " 元"）; } } ▐ 结果输出 宝贝原价：200.0 元 使用店铺折扣后需支付：160.0 元 使用满200减20后需支付：140.0 元 使用百亿补贴券后需支付：90.0 元 ▐ UML图 ▐ 比较分析
VS 继承 装饰者模式和继承关系都是要对目标类进行功能扩展，但装饰模式可以提供比继承更多的灵活性：继承是静态添加功能，在系统运行前就会确定下来；装饰者模式是动态添加、删除功能。
比如，一个对象需要具备 10 种功能，但客户端可能要求分阶段使用对象功能：在第一阶段只执行第 1-8 项功能，第二阶段执行第 3-10 项功能，这种场景下只需先定义好第 3-8 项功能方法。在程序运行的第一个阶段，使用具体装饰者 A 添加 1、2 功能；在第二个运行阶段，使用具体装饰者 B 添加 9、10 功能。而继承关系难以实现这种需求，它必须在编译期就定义好要使用的功能。
VS 代理模式 装饰者模式常常被拿来和代理模式比较，两者都要实现目标类的相同接口、声明一个目标对象，并且都可以在不修改目标类的前提下进行方法扩展，整体设计思路非常相似。那么两者的区别是什么呢？
首先，装饰者模式的重点在于增强目标对象功能，而代理模式的重点在于保护和隐藏目标对象。其中，装饰者模式需要客户端明确知道目标类，才能对其功能进行增强；代理模式要求客户端对目标类进行透明访问，借助代理类来完成相关控制功能（如日志记录、缓存设置等），隐藏目标类的具体信息。可见，代理类与目标类的关系往往在编译时就确定下来，而装饰者类在运行时动态构造而成。
其次，两者获取目标类的方式不同。装饰者模式是将目标对象作为参数传给构造方法，而代理模式是通过在代理类中创建目标对象的一个实例。
最后，通过上述示例可发现，装饰者模式会使用一系列具体装饰者类来增强目标对象的功能，产生了一种连续、叠加的效应；而代理模式是在代理类中一次性为目标对象添加功能。
﻿
VS 适配器模式 两者都属于包装式行为，即当一个类不能满足需求时，创建辅助类进行包装以满足变化的需求。但是装饰者模式的装饰者类和被装饰类都要实现相同接口，或者装饰类是被装饰类的子类；而适配器模式中，适配器和被适配的类可以有不同接口，并且可能会有部分接口重合。
▐ JDK源码赏析 Java I/O标准库是装饰者模式在Java语言中非常经典的应用实例。
如下图所示，InputStream 相当于抽象构件，FilterInputStream 类似于抽象装饰者，它的四个子类等同于具体装饰者。其中，FilterInputStream 中含有被装饰类 InputStream 的引用，其具体装饰者及各自功能为：PushbackInputStream 能弹出一个字节的缓冲区，可将输入流放到回退流中；DataInputStream 与 DataOutputStream搭配使用，用来装饰其它输入流，允许应用程序以一种与机器无关的方式从底层输入流中读取基本 Java 数据类型；BufferedInputStream 使用缓冲数组提供缓冲输入流功能，在每次调用 read() 方法时优先从缓冲区读取数据，比直接从物理数据源读取数据的速度更快；LineNumberInputStream 提供输入流过滤功能，可以跟踪输入流中的行号（以回车符、换行符标记换行）。
FilterInputStream 是所有装饰器类的抽象类，提供特殊的输入流控制。下面源码省略了 skip、available、mark、reset、markSupported 方法，这些方法也都委托给了 InputStream 类。其中， InputStream 提供装饰器类的接口，因而此类并没有对 InputStream 的功能做任何扩展，其扩展主要交给其子类来实现。
public class FilterInputStream extends InputStream { //维护一个 InputStream 对象 protected volatile InputStream in; //构造方法参数需要一个 inputStream protected FilterInputStream(InputStream in) { this.in = in; } //委托给 InputStream public int read() throws IOException { return in.read(); } //委托给 InputStream public void close() throws IOException { in.close(); } ....... } 由于源码太长，这里先以 PushbackInputStream 为例，展示 FilterInputStream 的具体装饰者的底层实现，大家感兴趣的话可以自行查阅其它源码哦。PushbackInputStream 内部维护了一个 pushback buf 缓冲区，可以帮助我们试探性地读取数据流，对于不想要的数据也可以返还回去。
public class PushbackInputStream extends FilterInputStream { //缓冲区 protected byte[] buf; protected int pos; private void ensureOpen() throws IOException { if (in == null) throw new IOException("Stream closed"); } //构造函数可以指定返回的字节个数 public PushbackInputStream(InputStream in, int size) { super(in); if (size &lt;= 0) { throw new IllegalArgumentException("size &lt;= 0"); } //初始化缓冲区的大小 this.buf = new byte[size]; //设置读取的位置 this.pos = size; } //默认回退一个 public PushbackInputStream(InputStream in) { this(in, 1); } public int read() throws IOException { //确保流存在 ensureOpen(); //如果要读取的位置在缓冲区里面 if (pos &lt; buf.length) { //返回缓冲区中的内容 return buf[pos++] &amp; 0xff; } //否则调用超类的读函数 return super.read(); } //读取指定的长度 public int read(byte[] b, int off, int len) throws IOException { ensureOpen(); if (b == null) { throw new NullPointerException(); } else if (off &lt; 0 || len &lt; 0 || len > b.length - off) { throw new IndexOutOfBoundsException(); } else if (len == 0) { return 0; } //缓冲区长度减去读取位置 int avail = buf.length - pos; //如果大于0，表明部分数据可以从缓冲区读取 if (avail > 0) { //如果要读取的长度小于可从缓冲区读取的字符 if (len &lt; avail) { //修改可读取值为实际要读的长度 avail = len; } //将buf中的数据复制到b中 System.arraycopy(buf, pos, b, off, avail); //修改pos的值 pos += avail; //修改off偏移量的值 off += avail; //修改len的值 len -= avail; } //如果从缓冲区读取的数据不够 if (len > 0) { //从流中读取 len = super.read(b, off, len); if (len == -1) { return avail == 0 ? -1 : avail; } return avail + len; } return avail; } //不读字符b public void unread(int b) throws IOException { ensureOpen(); if (pos == 0) { throw new IOException("Push back buffer is full"); } //实际就是修改缓冲区中的值，同时pos后退 buf[--pos] = (byte)b; } public void unread(byte[] b, int off, int len) throws IOException { ensureOpen(); if (len > pos) { throw new IOException("Push back buffer is full"); } //修改缓冲区中的值，pos后退多个 pos -= len; System.arraycopy(b, off, buf, pos, len); } public void unread(byte[] b) throws IOException { unread(b, 0, b.length); } } ▐ 优点
提供比继承更加灵活的扩展功能，通过叠加不同的具体装饰者的方法，动态地增强目标类的功能。
装饰者和被装饰者可以独立发展，不会相互耦合，比如说我们想再加一个炒河粉只需创建一个炒河粉类继承FastFood即可，而想要增加火腿肠配料就增加一个类去继承 Garnish 抽象装饰者。
▐ 缺点
使用装饰模式，可以比使用继承关系创建更少的类，使设计比较易于进行。然而，多层装饰会产生比继承更多的对象，使查错更加困难，尤其是这些对象都很相似。而且，当目标类被多次动态装饰后，程序的复杂性也会大大提升，难以维护。
▐ 适用场景
﻿
继承关系不利于系统维护，甚至不能使用继承关系的场景。比如，当继承导致类爆炸时、目标类被 final 修饰时，都不宜通过创建目标类的子类来扩展功能。
要求不影响其他对象，为特定目标对象添加功能。
要求动态添加、撤销对象的功能。
▐ 总结
装饰者模式也是一种比较容易理解和上手的设计模式，它可以对多个装饰者类进行花式排列组合，适应多变的用户需求。同时，装饰者模式也是符合开闭原则的，被装饰的对象和装饰者类互相独立、互不干扰。
在介绍装饰者模式的适用场景时，我们可以发现上述场景在实际工程中也比较常见，因此装饰者模式同样应用广泛。除了本文提到的 Java I/O，装饰者模式的典型应用实例还有：Spring cache 中的 TransactionAwareCacheDecorator 类、 Spring session 中的 ServletRequestWrapper 类、Mybatis 缓存中的 decorators 包等等。</content></entry><entry><title>git上传代码报错ssh: connect to host github.com port 22: Connection timed out解决办法</title><url>/post/git%E4%B8%8A%E4%BC%A0%E4%BB%A3%E7%A0%81%E6%8A%A5%E9%94%99ssh-connect-to-host-github.com-port-22-connection-timed-out%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url><categories><category>problem-solving</category><category>Git</category></categories><tags><tag>Git</tag><tag>problem-solving</tag></tags><content type="html"> git上传代码报错ssh: connect to host github.com port 22: Connection timed out解决办法
git上传代码报错ssh: connect to host github.com port 22: Connection timed out解决办法 当在远程库上设置了SSH 之后还是报错连接超时，问题如下
$ git push 报错：
ssh: connect to host github.com port 22: Connection timed out fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个时候需要检查一下SSH是否能够连接成功，输入以下命令
ssh -T git@github.com 稍等片刻如果继续报错，如下：
ssh: connect to host github.com port 22: Connection timed out则，可以使用一下解决办法
打开存放ssh的目录
cd ~/.ssh ls 查看是否存在 id_rsa id_rsa.pun known_hosts 三个文件，如果没有请查看Git设置SSH方式
，如果有请按照以下方式设置
1. vim config (进入到vim编辑界面，如果是windows系统powershell，可以设置使用vim,参考文章powershell设置vim
）
2. insert 编辑模式
Host github.com User YourEmail@163.com Hostname ssh.github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa Port 443 其中， YourEmail 为绑定的邮箱。
保存之后再次执行"ssh -T git@github.com
"时，会出现如下提示，回车"yes"即可</content></entry><entry><title>Java伪共享</title><url>/post/java%E4%BC%AA%E5%85%B1%E4%BA%AB/</url><categories><category>Java Base</category><category>计算机原理</category></categories><tags><tag>Java Base</tag><tag>计算机原理</tag></tags><content type="html"> Java伪共享
Java伪共享 维基百科对伪共享的定义如下：
In computer science, false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that will never be altered by another party, but those data shares a cache block with data that are altered, the caching protocol may force the first participant to reload the whole unit despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource
其大致意思是： CPU的缓存是以缓存行(cache line)为单位进行缓存的，当多个线程修改不同变量，而这些变量又处于同一个缓存行时就会影响彼此的性能。例如：线程1和线程2共享一个缓存行，线程1只读取缓存行中的变量1，线程2修改缓存行中的变量2，虽然线程1和线程2操作的是不同的变量，由于变量1和变量2同处于一个缓存行中，当变量2被修改后，缓存行失效，线程1要重新从主存(或者低级cache)中读取，因此导致缓存失效，从而产生性能问题。为了更深入一步理解伪共享，我们先看一下CPU缓存
1、Cpu三级缓存 CPU的速度要远远大于内存的速度，为了解决这个问题，CPU引入了三级缓存：L1，L2和L3三个级别，L1最靠近CPU，L2次之，L3离CPU最远，L3之后才是主存。速度是L1>L2>L3>主存。越靠近CPU的容量越小。CPU获取数据会依次从三级缓存中查找，如果找不到再从主存中加载。
当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。一般来说，每级缓存的命中率大概都在80%左右，也就是说全部数据量的80%都可以在一级缓存中找到，只剩下20%的总数据量才需要从二级缓存、三级缓存或内存中读取，由此可见一级缓存是整个CPU缓存架构中最为重要的部分
2、MESI协议（高速缓存一致性协议） 缓存行状态 CPU的缓存是以缓存行(cache line)为单位的，MESI协议描述了多核处理器中一个缓存行的状态。在MESI协议中，每个缓存行有4个状态，分别是：
M（修改，Modified）：本地处理器已经修改缓存行，即是脏行，它的内容与内存中的内容不一样，并且此 cache 只有本地一个拷贝(专有)； E（专有，Exclusive）：缓存行内容和内存中的一样，而且其它处理器都没有这行数据； S（共享，Shared）：缓存行内容和内存中的一样, 有可能其它处理器也存在此缓存行的拷贝； I（无效，Invalid）：缓存行失效, 不能使用 缓存行状态转换 在MESI协议中，每个Cache的Cache控制器不仅知道自己的读写操作，而且也监听(snoop)其它Cache的读写操作。每个Cache line所处的状态根据本核和其它核的读写操作在4个状态间进行迁移。MESI协议状态迁移图如下：
MESI协议-状态转换
初始：一开始时，缓存行没有加载任何数据，所以它处于 I 状态。 本地写（Local Write）：如果本地处理器写数据至处于 I 状态的缓存行，则缓存行的状态变成 M。 本地读（Local Read）：如果本地处理器读取处于 I 状态的缓存行，很明显此缓存没有数据给它。此时分两种情况：(1)其它处理器的缓存里也没有此行数据，则从内存加载数据到此缓存行后，再将它设成 E 状态，表示只有我一家有这条数据，其它处理器都没有；(2)其它处理器的缓存有此行数据，则将此缓存行的状态设为 S 状态。（备注：如果处于M状态的缓存行，再由本地处理器写入/读出，状态是不会改变的） 远程读（Remote Read）：假设我们有两个处理器 c1 和 c2，如果 c2 需要读另外一个处理器 c1 的缓存行内容，c1 需要把它缓存行的内容通过内存控制器 (Memory Controller) 发送给 c2，c2 接到后将相应的缓存行状态设为 S。在设置之前，内存也得从总线上得到这份数据并保存。 远程写（Remote Write）：其实确切地说不是远程写，而是 c2 得到 c1 的数据后，不是为了读，而是为了写。也算是本地写，只是 c1 也拥有这份数据的拷贝，这该怎么办呢？c2 将发出一个 RFO (Request For Owner) 请求，它需要拥有这行数据的权限，其它处理器的相应缓存行设为 I，除了它自已，谁不能动这行数据。这保证了数据的安全，同时处理 RFO 请求以及设置I的过程将给写操作带来很大的性能消耗。 缓存行 CPU缓存是以缓存行（cache line）为单位存储的。缓存行通常是 64 字节，并且它有效地引用主内存中的一块地址。一个 Java 的 long 类型是 8 字节，因此在一个缓存行中可以存 8 个 long 类型的变量。所以，如果你访问一个 long 数组，当数组中的一个值被加载到缓存中，它会额外加载另外 7 个，以致你能非常快地遍历这个数组。事实上，你可以非常快速的遍历在连续的内存块中分配的任意数据结构。而如果你在数据结构中的项在内存中不是彼此相邻的（如链表），你将得不到免费缓存加载所带来的优势，并且在这些数据结构中的每一个项都可能会出现缓存未命中。下图是一个CPU缓存行的示意图：
上图中，一个运行在处理器 core1上的线程想要更新变量 X 的值，同时另外一个运行在处理器 core2 上的线程想要更新变量 Y 的值。但是，这两个频繁改动的变量都处于同一条缓存行。两个线程就会轮番发送 RFO 消息，占得此缓存行的拥有权。当 core1 取得了拥有权开始更新 X，则 core2 对应的缓存行需要设为 I 状态。当 core2 取得了拥有权开始更新 Y，则 core1 对应的缓存行需要设为 I 状态(失效态)。轮番夺取拥有权不但带来大量的 RFO 消息，而且如果某个线程需要读此行数据时，L1 和 L2 缓存上都是失效数据，只有 L3 缓存上是同步好的数据。从前一篇我们知道，读 L3 的数据非常影响性能。更坏的情况是跨槽读取，L3 都要 miss，只能从内存上加载。
表面上 X 和 Y 都是被独立线程操作的，而且两操作之间也没有任何关系。只不过它们共享了一个缓存行，但所有竞争冲突都是来源于共享。
3、java中的伪共享 解决伪共享最直接的方法就是填充（padding），例如下面的VolatileLong，一个long占8个字节，Java的对象头占用8个字节（32位系统）或者12字节（64位系统，默认开启对象头压缩，不开启占16字节）。一个缓存行64字节，那么我们可以填充6个long（6 * 8 = 48 个字节）。这样就能避免多个VolatileLong共享缓存行。
public class VolatileLong { private volatile long v; // private long v0, v1, v2, v3, v4, v5 // 去掉注释，开启填充，避免缓存行共享 } 这是最简单直接的方法，Java 8中引入了一个更加简单的解决方案：@Contended注解：
@Retention(RetentionPolicy.RUNTIME) @Target({ElementType.FIELD, ElementType.TYPE}) public @interface Contended { String value() default ""; } Contended注解可以用于类型上和属性上，加上这个注解之后虚拟机会自动进行填充，从而避免伪共享。这个注解在Java8 ConcurrentHashMap、ForkJoinPool和Thread等类中都有应用。我们来看一下Java8中ConcurrentHashMap中如何运用Contended这个注解来解决伪共享问题。以下说的ConcurrentHashMap都是Java8版本。
ConcurrentHashMap中伪共享解决方案 ConcurrentHashMap的size操作通过CounterCell来计算，哈希表中的每个节点都对用了一个CounterCell，每个CounterCell记录了对应Node的键值对数目。这样每次计算size时累加各个CounterCell就可以了。ConcurrentHashMap中CounterCell以数组形式保存，而数组在内存中是连续存储的，CounterCell中只有一个long类型的value属性，这样CPU会缓存CounterCell临近的CounterCell，于是就形成了伪共享。
ConcurrentHashMap中用Contended注解自动对CounterCell来进行填充：
/** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; // CounterCell数组，CounterCell在内存中连续 public int size() { long n = sumCount(); return ((n &lt; 0L) ? 0 : (n > (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } // 计算size时直接对各个CounterCell的value进行累加 final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } // 使用Contended注解自动进行填充避免伪共享 @sun.misc.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } } 需要注意的是@sun.misc.Contended注解在user classpath中是不起作用的，需要通过一个虚拟机参数来开启：-XX:-RestrictContended
4、总结 CPU缓存是以缓存行为单位进行操作的。产生伪共享问题的根源在于不同的核同时操作同一个缓存行。 可以通过填充来解决伪共享问题，Java8 中引入了@sun.misc.Contended注解来自动填充。 并不是所有的场景都需要解决伪共享问题，因为CPU缓存是有限的，填充会牺牲掉一部分缓存</content></entry><entry><title>PowerShell设置使用vim</title><url>/post/powershell%E8%AE%BE%E7%BD%AE%E4%BD%BF%E7%94%A8vim/</url><categories><category>PowerShell</category><category>Vim</category><category>Git</category><category>problem-solving</category></categories><tags><tag>PowerShell</tag><tag>Vim</tag><tag>Git</tag><tag>problem-solving</tag></tags><content type="html"> PowerShell设置使用vim
PowerShell设置使用vim 如果系统安装了Git，Git本身有自带的vim，位置为%GITHOME%\usr\bin\vim.exe，如果没有安装Git,可以去github下载vim，Github vim下载
。
打开C:\Windows\System32\WindowsPowerShell\v1.0文件夹新建profile.ps1文件，内容如下，重点是$VIMPATH变量，值为第一步中vim.exe路径
# There's usually much more than this in my profile! $SCRIPTPATH = "C:\Program Files\Git\usr\share\vim" # 此行根据$VIMPATH寻找相应vim路径即可,此行不用更改 $VIMPATH = "D:\DevelopmentSoftware\Git\usr\bin\vim.exe" # 此行为1中vim.exe路径 Set-Alias vi $VIMPATH Set-Alias vim $VIMPATH # for editing your PowerShell profile Function Edit-Profile { vim $profile } # for editing your Vim settings Function Edit-Vimrc { vim $home\_vimrc } 以管理员身份运行PowerShell，运行Set-ExecutionPolicy RemoteSigned，然后输入Y即可
重新打开PowerShell即可使用vim命令</content></entry><entry><title>ThreadLocal与FastThreadLocal</title><url>/post/threadlocal%E4%B8%8Efastthreadlocal/</url><categories><category>ThreadLocal</category><category>Java Base</category><category>netty</category></categories><tags><tag>ThreadLocal</tag><tag>Java Base</tag><tag>netty</tag></tags><content type="html"> ThreadLocal与FastThreadLocal
ThreadLocal与FastThreadLocal FastThreadLocal是Netty中常用的一个工具类，他的基本功能与JDK自带的ThreadLocal
一样，但是性能优于ThreadLocal。在讲解FastThreadLocal之前，先大致讲一下ThreadLocal的原理。
1、ThreadLocal 如果想要在线程中保存一个变量，这个变量是该线程所独有的，其他线程不能对该变量进行访问和修改，那么我们可以使用ThreadLocal实现这一功能.
ThreadLocal的功能主要是通过ThreadLocalMap来实现的，每个线程都会有一个ThreadLocalMap类型的成员变量，ThreadLocalMap本质上就是一个哈希表，key为ThreadLocal，value为该ThreadLocal为该线程保存的变量，如下图所示： ThreadLocalMap中使用一个Entry类型的数组来作为哈希表，并用线性探测法解决哈希冲突，数组的大小永远是2的n次方。Entry是一个静态内部类，且继承了WeakReference&lt;ThreadLocal>，表示其保存的ThreadLocal是一个弱引用，这是为了防止内存泄漏，因为当所有指向ThreadLocal的强引用都为null时，该Entry也就没有必要再指向这个ThreadLocal了，这样下一次gc时该ThreadLoca就会被回收掉。Entry的成员变量Object value就是T和read Local为线程保存的独有的变量。
set方法将ThreadLocal和相应的value保存为一个Entry，再根据ThreadLocal的哈希值将Entry放入哈希表中的相应的位置，并用线性探测法解决哈希冲突。setEntry方法中，如果在查找哈希表的过程中发现某个Entry所保存的ThreadLocal被垃圾回收了，那么会将该Entry清除掉
private ThreadLocal.ThreadLocalMap.Entry getEntry(ThreadLocal&lt;?> key) { int i = key.threadLocalHashCode &amp; (table.length - 1); ThreadLocal.ThreadLocalMap.Entry e = table[i]; //找到了相应的entry，直接返回 if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); } private ThreadLocal.ThreadLocalMap.Entry getEntryAfterMiss(ThreadLocal&lt;?> key, int i, ThreadLocal.ThreadLocalMap.Entry e) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal&lt;?> k = e.get(); if (k == key) return e; //该threadLocal已经为null，那么entry也就没有存在的必要了，因此需要被清除 if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null; } private void set(ThreadLocal&lt;?> key, Object value) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; //将threadlocal的哈希值与哈希表的长度取余，放入哈希表中相应的位置 int i = key.threadLocalHashCode &amp; (len-1); for (ThreadLocal.ThreadLocalMap.Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal&lt;?> k = e.get(); //哈希表中保存了这个threadLocal，因此直接修改value值 if (k == key) { e.value = value; return; } //entry不为null，但entry保存threadLocal为null，说明该threadLocal已经被垃圾回收了，需要替换掉这个entry if (k == null) { replaceStaleEntry(key, value, i); return; } } tab[i] = new ThreadLocal.ThreadLocalMap.Entry(key, value); int sz = ++size; //如果哈希表的装载因子超过阈值，则需要对哈希表扩容，并对所有的entry重新做哈希 if (!cleanSomeSlots(i, sz) &amp;&amp; sz >= threshold) rehash(); } 2、FastThreadLocal 从 InternalThreadLocalMap 内部实现来看，与 ThreadLocalMap 一样都是采用数组的存储方式，但是 InternalThreadLocalMap 并没有使用线性探测法来解决 Hash 冲突，而是在 FastThreadLocal 初始化的时候分配一个数组索引 index，index 的值采用原子类 AtomicInteger 保证顺序递增，通过调用 InternalThreadLocalMap.nextVariableIndex() 方法获得。然后在读写数据的时候通过数组下标 index 直接定位到 FastThreadLocal 的位置，时间复杂度为 O(1)。如果数组下标递增到非常大，那么数组也会比较大，所以 FastThreadLocal 是通过空间换时间的思想提升读写性能
public FastThreadLocal() { index = InternalThreadLocalMap.nextVariableIndex(); } public static int nextVariableIndex() { //nextIndex是一个静态变量，每次调用nextVariableIndex()都会自增1，让后赋给FastThreadLocal的index属性 int index = nextIndex.getAndIncrement(); if (index &lt; 0) { nextIndex.decrementAndGet(); throw new IllegalStateException("too many thread-local indexed variables"); } return index; } static final AtomicInteger nextIndex = new AtomicInteger(); 通过上面 FastThreadLocal 的内部结构图可知，FastThreadLocal 使用 Object 数组替代了 Entry 数组，Object[0] 存储的是一个Set&lt;FastThreadLocal> 集合，从数组下标 1 开始都是直接存储的 value 数据，不再采用 ThreadLocal 的键值对形式进行存储。
假设现在我们有一批数据需要添加到数组中，分别为 value1、value2、value3、value4，对应的 FastThreadLocal 在初始化的时候生成的数组索引分别为 1、2、3、4。如下图所示。
使用代码示例 public class FastThreadLocalTest { private static final FastThreadLocal&lt;;String> THREAD_NAME_LOCAL = new FastThreadLocal&lt;;>(); private static final FastThreadLocal&lt;;TradeOrder> TRADE_THREAD_LOCAL = new FastThreadLocal&lt;;>(); public static void main(String[] args) { for (int i = 0; i &lt;; 2; i++) { int tradeId = i; String threadName = "thread-" + i; new FastThreadLocalThread(() -> { THREAD_NAME_LOCAL.set(threadName); TradeOrder tradeOrder = new TradeOrder(tradeId, tradeId % 2 == 0 ? "已支付" : "未支付"); TRADE_THREAD_LOCAL.set(tradeOrder); System.out.println("threadName: " + THREAD_NAME_LOCAL.get()); System.out.println("tradeOrder info：" + TRADE_THREAD_LOCAL.get()); }, threadName).start(); } } } FastThreadLocal 的使用方法几乎和 ThreadLocal 保持一致，只需要把代码中 Thread、ThreadLocal 替换为 FastThreadLocalThread 和 FastThreadLocal 即可。
FastThreadLocal源码分析 FastThreadLocal.set()
public final void set(V value) { if (value != InternalThreadLocalMap.UNSET) { // 1. value 是否为缺省值 InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.get(); // 2. 获取当前线程的 InternalThreadLocalMap setKnownNotUnset(threadLocalMap, value); // 3. 将 InternalThreadLocalMap 中数据替换为新的 value } else { remove(); } } set() 的过程主要分为三步：
判断 value 是否为缺省值(UNSET)，如果等于缺省值，那么直接调用 remove() 方法。这里我们还不知道缺省值和 remove() 之间的联系是什么，我们暂且把 remove() 放在最后分析。 如果 value 不等于缺省值，接下来会获取当前线程的 InternalThreadLocalMap。 然后将 InternalThreadLocalMap 中对应数据替换为新的 value。 FastThreadLocal使用字节填充解决伪共享 FastThreadLocal也是用在多线程场景，所以FastThreadLocal需要解决伪共享问题，FastThreadLocal使用字节填充解决伪共享，详情请移步java伪共享
缓存行 Cache是由很多个cache line组成的。每个cache line通常是64字节，并且它有效地引用主内存中的一块儿地址。一个Java的long类型变量是8字节，因此在一个缓存行中可以存8个long类型的变量。 CPU每次从主存中拉取数据时，会把相邻的数据也存入同一个cache line。 在访问一个long数组的时候，如果数组中的一个值被加载到缓存中，它会自动加载另外7个。因此你能非常快的遍历这个数组。事实上，你可以非常快速的遍历在连续内存块中分配的任意数据结构。
伪共享 由于多个线程同时操作同一缓存行的不同变量，但是这些变量之间却没有啥关联，但是每次修改，都会导致缓存的数据变成无效，从而明明没有任何修改的内容，还是需要去主存中读（CPU读取主存中的数据会比从L1中读取慢了近2个数量级）但是其实这块内容并没有任何变化，由于缓存的最小单位是一个缓存行，这就是伪共享。
如果让多线程频繁操作的并且没有关系的变量在不同的缓存行中，那么就不会因为缓存行的问题导致没有关系的变量的修改去影响另外没有修改的变量去读主存了（那么从L1中取是从主存取快2个数量级的）那么性能就会好很多很多。
FastThreadLocal 的一些思考 FastThreadLocal 真的一定比 ThreadLocal 快吗？答案是不一定的，只有使用FastThreadLocalThread 类型的线程才会更快，如果是普通线程反而会更慢。
FastThreadLocal 会浪费很大的空间吗？虽然 FastThreadLocal 采用的空间换时间的思路，但是在 FastThreadLocal 设计之初就认为不会存在特别多的 FastThreadLocal 对象，而且在数据中没有使用的元素只是存放了同一个缺省对象的引用，并不会占用太多内存空间。
FastThreadLocal vs ThreadLocal 高效查找。FastThreadLocal 在定位数据的时候可以直接根据数组下标 index 获取，时间复杂度 O(1)。而 JDK 原生的 ThreadLocal 在数据较多时哈希表很容易发生 Hash 冲突，线性探测法在解决 Hash 冲突时需要不停地向下寻找，效率较低。此外，FastThreadLocal 相比 ThreadLocal 数据扩容更加简单高效，FastThreadLocal 以 index 为基准向上取整到 2 的次幂作为扩容后容量，然后把原数据拷贝到新数组。而 ThreadLocal 由于采用的哈希表，所以在扩容后需要再做一轮 rehash。 安全性更高。JDK 原生的 ThreadLocal 使用不当可能造成内存泄漏，只能等待线程销毁。在使用线程池的场景下，ThreadLocal 只能通过主动检测的方式防止内存泄漏，从而造成了一定的开销。然而 FastThreadLocal 不仅提供了 remove() 主动清除对象的方法，而且在线程池场景中 Netty 还封装了 FastThreadLocalRunnable，FastThreadLocalRunnable 最后会执行 FastThreadLocal.removeAll() 将 Set 集合中所有 FastThreadLocal 对象都清理掉</content></entry><entry><title>ThreadLocal父子线程传递问题</title><url>/post/threadlocal%E7%88%B6%E5%AD%90%E7%BA%BF%E7%A8%8B%E4%BC%A0%E9%80%92%E9%97%AE%E9%A2%98/</url><categories><category>ThreadLocal</category><category>Java Base</category></categories><tags><tag>ThreadLocal</tag><tag>Java Base</tag></tags><content type="html"> ThreadLocal父子线程传递问题
ThreadLocal父子线程传递问题 1、父子线程传递问题 ThreadLocal是线程上下文，如果主线程开启子线程，还可以顺利获得本地变量吗？答案是否定的，以下是实验过程
public class TTLTest{ public static void main(String[] args) { ThreadLocal&lt;String> threadLocal = new ThreadLocal&lt;>(); InheritableThreadLocal&lt;String> inheritableThreadLocal = new InheritableThreadLocal&lt;>(); threadLocal.set("threadLocal-value"); inheritableThreadLocal.set("inheritableThreadLocal-value"); // 验证父子线程传递 new Thread(() -> { System.out.println(threadLocal.get()); // null System.out.println(inheritableThreadLocal.get()); //inheritableThreadLocal-value }).start(); threadLocal.remove(); inheritableThreadLocal.remove(); } } 可以看到，开启的子线程是无法获得父线程的本地变量的，所以java引入了InheritableThreadLocal,在子线程初始化时，会将父线程的本地变量传递到子线程
2、线程池环境的本地变量传递问题 在大多数实际项目中，为了节省线程开启关闭的开销，常常使用线程池来提高线程的复用，线程复用的同时，对本地变量的传递带来了新的影响，上文提到InheritableThreadLocal实现父子线程变量传递是在子线程初始化过程中，而池化的线程是不会重新初始化的，所以InheritableThreadLocal不能在线程池开辟的子线程中传递，或者说，只会在子线程初始化时传递一次，而后在线程池中未被销毁之前，无法再次接受父线程的变量传递。
public class TTLTest{ public static void main(String[] args) { InheritableThreadLocal&lt;String> inheritableThreadLocal = new InheritableThreadLocal&lt;>(); // 验证线程池传递 ExecutorService executorService = Executors.newFixedThreadPool(1); for (int i = 0; i &lt; 10; i++) { inheritableThreadLocal.set("inheritableThreadLocal-value->" + i); executorService.submit(() -> { System.out.println(inheritableThreadLocal.get()); }); } executorService.shutdown(); inheritableThreadLocal.remove(); } } 结果： inheritableThreadLocal-value->0 inheritableThreadLocal-value->0 inheritableThreadLocal-value->0 inheritableThreadLocal-value->0 ... 可以看出子线程只保留了最初获得的本地变量 解决方案是阿里巴巴开源框架TTL: transmittable-thread-local
&lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>transmittable-thread-local&lt;/artifactId> &lt;/dependency> public class TTLTest { public static void main(String[] args) { InheritableThreadLocal&lt;String> inheritableThreadLocal = new InheritableThreadLocal&lt;>(); TransmittableThreadLocal&lt;String> transmittableThreadLocal = new TransmittableThreadLocal&lt;>(); // 验证线程池传递2 ExecutorService executorService2 = TtlExecutors.getTtlExecutorService(Executors.newFixedThreadPool(1)); for (int i = 0; i &lt; 10; i++) { inheritableThreadLocal.set("inheritableThreadLocal-value->" + i); transmittableThreadLocal.set("transmittableThreadLocal-value->" + i); executorService2.submit(() -> { System.out.println(inheritableThreadLocal.get()); System.out.println(transmittableThreadLocal.get()); }); } executorService.shutdown(); inheritableThreadLocal.remove(); transmittableThreadLocal.remove(); } } 结果： inheritableThreadLocal-value->0 transmittableThreadLocal-value->1 inheritableThreadLocal-value->0 transmittableThreadLocal-value->2 inheritableThreadLocal-value->0 transmittableThreadLocal-value->3 inheritableThreadLocal-value->0 transmittableThreadLocal-value->4 ... 注意这句代码TtlExecutors.getTtlExecutorService 通过对线程池进行包装，从而实现TTL所具备的功能，使用阿里开源的TransmittableThreadLocal 优雅的实现父子线程的数据传递。</content></entry><entry><title>ThreadLocal详解</title><url>/post/threadlocal/</url><categories><category>ThreadLocal</category><category>Java Base</category></categories><tags><tag>ThreadLocal</tag><tag>Java Base</tag></tags><content type="html"> ThreadLocal详解
ThreadLocal详解 1 ThreadLocal有什么缺陷？为什么会导致内存泄漏？ 2 Entry的key为什么要用弱引用? 四种引用有什么区别说一下？为什么使用弱引用就可以解决内存泄漏问题? 3 ThreadLocalMap的Key是什么？ 4 ThreadLocalMap如何解决value冲突问题?跟HashMap有什么区别？ 5 进阶：ThreadLocal能否做到子线程中共享父线程中的数据?InheritableThreadLocal了解过吗？有什么局限性？ 6 进阶：FastThreadLocal有了解过吗？ 1、ThreadLocal简介 该类提供线程局部变量。这些变量与普通变量的不同之处在于，每个访问一个变量(通过其get或set方法)的线程都有自己的、独立初始化的变量副本。ThreadLocal实例通常是类中的私有静态字段，希望将状态与线程关联(例如，用户ID或事务ID)。
​ ThreadLocal叫做线程变量，意思是ThreadLocal中填充的变量属于当前线程，该变量对其他线程而言是隔离的，也就是说该变量是当前线程独有的变量。ThreadLocal为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量。
通俗的描述一下threadlocal的存取set()/get()方法大体逻辑如下：
set方法：threadlocal在set数据时，首先调用Thread.currentThread()获取当前操作线程，拿到线程之后获取线程的ThreadLocalMap变量(细节还包括首次获取时为null，为当前线程初始化ThreadLocalMap并返回)，然后向ThreadLocalMap中put数据，key是this(threadlocal)，value即为当前线程要保存的值。
get方法：逻辑与set类似，先获取当前线程，拿到线程的ThreadLocalMap，调用ThreadLocalMap的get方法，key即为this(当前threadlocal实例对象)，返回当前线程保存的值。
总的来说，ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景
图示：
2、ThreadLocal简单使用 public class ThreadLocaDemo { private static ThreadLocal&lt;String> localVar = new ThreadLocal&lt;String>(); static void print(String str) { //打印当前线程中本地内存中本地变量的值 System.out.println(str + " :" + localVar.get()); //清除本地内存中的本地变量 localVar.remove(); } public static void main(String[] args) throws InterruptedException { new Thread(new Runnable() { public void run() { ThreadLocaDemo.localVar.set("local_A"); print("A"); //打印本地变量 System.out.println("after remove : " + localVar.get()); } },"A").start(); Thread.sleep(1000); new Thread(new Runnable() { public void run() { ThreadLocaDemo.localVar.set("local_B"); print("B"); System.out.println("after remove : " + localVar.get()); } },"B").start(); } } A :local_A after remove : null B :local_B after remove : null 3、ThreadLocal源码原理 ThreadLocal的set()方法： public void set(T value) { //1、获取当前线程 Thread t = Thread.currentThread(); //2、获取线程中的属性 threadLocalMap ,如果threadLocalMap 不为空， //则直接更新要保存的变量值，否则创建threadLocalMap，并赋值 ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else // 初始化thradLocalMap 并赋值 createMap(t, value); } 从上面的代码可以看出，ThreadLocal set赋值的时候首先会获取当前线程thread,并获取thread线程中的ThreadLocalMap属性。如果map属性不为空，则直接更新value值，如果map为空，则实例化threadLocalMap,并将value值初始化。
那么ThreadLocalMap又是什么呢，还有createMap又是怎么做的
static class ThreadLocalMap { /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as "stale entries" in the code that follows. */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?>> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?> k, Object v) { super(k); value = v; } } } 可看出ThreadLocalMap是ThreadLocal的内部静态类，而它的构成主要是用Entry来保存数据 ，而且还是继承的弱引用。在Entry内部使用ThreadLocal作为key，使用我们设置的value作为value。
//这个是threadlocal 的内部方法 void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } //ThreadLocalMap 构造方法 ThreadLocalMap(ThreadLocal&lt;?> firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } ThreadLocal的get方法 public T get() { //1、获取当前线程 Thread t = Thread.currentThread(); //2、获取当前线程的ThreadLocalMap ThreadLocalMap map = getMap(t); //3、如果map数据不为空， if (map != null) { //3.1、获取threalLocalMap中存储的值 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings("unchecked") T result = (T)e.value; return result; } } //如果是数据为null，则初始化，初始化的结果，TheralLocalMap中存放key值为threadLocal，值为null return setInitialValue(); } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } ThreadLocal的remove方法
public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); } remove方法，直接将ThrealLocal 对应的值从当前相差Thread中的ThreadLocalMap中删除。为什么要删除，这涉及到内存泄露的问题。
实际上 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，弱引用的特点是，如果这个对象只存在弱引用，那么在下一次垃圾回收的时候必然会被清理掉。
所以如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会被清理掉的，这样一来 ThreadLocalMap中使用这个 ThreadLocal 的 key 也会被清理掉。但是，value 是强引用，不会被清理，这样一来就会出现 key 为 null 的 value。
ThreadLocal其实是与线程绑定的一个变量，如此就会出现一个问题：如果没有将ThreadLocal内的变量删除（remove）或替换，它的生命周期将会与线程共存。通常线程池中对线程管理都是采用线程复用的方法，在线程池中线程很难结束甚至于永远不会结束，这将意味着线程持续的时间将不可预测，甚至与JVM的生命周期一致。举个例字，如果ThreadLocal中直接或间接包装了集合类或复杂对象，每次在同一个ThreadLocal中取出对象后，再对内容做操作，那么内部的集合类和复杂对象所占用的空间可能会开始持续膨胀。
ThreadLocal与Thread，ThreadLocalMap之间的关系
4、ThreadLocal 常见使用场景 ThreadLocal 适用于如下两种场景：
每个线程需要有自己单独的实例
实例需要在多个方法中共享，但不希望被多线程共享
对于第一点，每个线程拥有自己实例，实现它的方式很多。例如可以在线程内部构建一个单独的实例。ThreadLoca 可以以非常方便的形式满足该需求。
对于第二点，可以在满足第一点（每个线程有自己的实例）的条件下，通过方法间引用传递的形式实现。ThreadLocal 使得代码耦合度更低，且实现更优雅。
场景
1）存储用户Session
一个简单的用ThreadLocal来存储Session的例子：
private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException { Session s = (Session) threadSession.get(); try { if (s == null) { s = getSessionFactory().openSession(); threadSession.set(s); } } catch (HibernateException ex) { throw new InfrastructureException(ex); } return s; } 场景二、数据库连接，处理数据库事务
场景三、数据跨层传递（controller,service, dao）
每个线程内需要保存类似于全局变量的信息（例如在拦截器中获取的用户信息），可以让不同方法直接使用，避免参数传递的麻烦却不想被多线程共享（因为不同线程获取到的用户信息不一样）。
例如，用 ThreadLocal 保存一些业务内容（用户权限信息、从用户系统获取到的用户名、用户ID 等），这些信息在同一个线程内相同，但是不同的线程使用的业务内容是不相同的。
在线程生命周期内，都通过这个静态 ThreadLocal 实例的 get() 方法取得自己 set 过的那个对象，避免了将这个对象（如 user 对象）作为参数传递的麻烦。
比如说我们是一个用户系统，那么当一个请求进来的时候，一个线程会负责执行这个请求，然后这个请求就会依次调用service-1()、service-2()、service-3()、service-4()，这4个方法可能是分布在不同的类中的。这个例子和存储session有些像。
package com.kong.threadlocal; public class ThreadLocalDemo05 { public static void main(String[] args) { User user = new User("jack"); new Service1().service1(user); } } class Service1 { public void service1(User user){ //给ThreadLocal赋值，后续的服务直接通过ThreadLocal获取就行了。 UserContextHolder.holder.set(user); new Service2().service2(); } } class Service2 { public void service2(){ User user = UserContextHolder.holder.get(); System.out.println("service2拿到的用户:"+user.name); new Service3().service3(); } } class Service3 { public void service3(){ User user = UserContextHolder.holder.get(); System.out.println("service3拿到的用户:"+user.name); //在整个流程执行完毕后，一定要执行remove UserContextHolder.holder.remove(); } } class UserContextHolder { //创建ThreadLocal保存User对象 public static ThreadLocal&lt;User> holder = new ThreadLocal&lt;>(); } class User { String name; public User(String name){ this.name = name; } } 执行的结果： service2拿到的用户:jack service3拿到的用户:jack 场景四、Spring使用ThreadLocal解决线程安全问题
我们知道在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全的“状态性对象”采用ThreadLocal进行封装，让它们也成为线程安全的“状态性对象”，因此有状态的Bean就能够以singleton的方式在多线程中正常工作了。
一般的Web应用划分为展现层、服务层和持久层三个层次，在不同的层中编写对应的逻辑，下层通过接口向上层开放功能调用。在一般情况下，从接收请求到返回响应所经过的所有程序调用都同属于一个线程，如图9-2所示。 这样用户就可以根据需要，将一些非线程安全的变量以ThreadLocal存放，在同一次请求响应的调用线程中，所有对象所访问的同一ThreadLocal变量都是当前线程所绑定的。
5、ThreadLocal内存泄漏 ThreadLocalMap实际上是以Entry作为数据存储载体，Entry将ThreadLocal作为Key，值作为value保存，它继承自WeakReference，注意构造函数里的第一行代码super(k)，这意味着它的Key(ThreadLocal对象)是一个「弱引用」
static class Entry extends WeakReference&lt;ThreadLocal&lt;?>> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?> k, Object v) { super(k); value = v; } } 主要两个原因 1 . 没有手动删除这个 Entry 2 . CurrentThread 当前线程依然运行
第一点很好理解，只要在使用完下 ThreadLocal ，调用其 remove 方法删除对应的 Entry(找到对应的entry并将entry=null) ，就能避免内存泄漏。 第二点稍微复杂一点，由于ThreadLocalMap 是 Thread 的一个属性，被当前线程所引用，所以ThreadLocalMap的生命周期跟 Thread 一样长。如果threadlocal变量被回收，那么当前线程的threadlocal 变量副本指向的就是key=null, 也即entry(null,value),那这个entry对应的value永远无法访问到。实际私用ThreadLocal场景都是采用线程池，而线程池中的线程都是复用的，这样就可能导致非常多的entry(null,value)出现，从而导致内存泄露。 综上， ThreadLocal 内存泄漏的根源是： 由于ThreadLocalMap 的生命周期跟 Thread 一样长，对于重复利用的线程来说，如果没有手动删除（remove()方法）对应 key 就会导致entry(null，value)的对象越来越多，从而导致内存泄漏。
**key为弱引用的好处**：**ThreadLocal 没有被外部强引用的情况下，key就会被回收变成null，在 ThreadLocalMap 中的set/getEntry 方法中，会对 key 为 null（也即是 ThreadLocal 为 null ）进行判断，如果为 null 的话，那么会把 value 置为 null 的．这就意味着使用threadLocal , CurrentThread 依然运行的前提下．就算忘记调用 remove 方法，弱引用比强引用可以多一层保障：弱引用的 ThreadLocal 会被回收．对应value在下一次 ThreadLocaI 调用 get()/set()/remove() 中的任一方法的时候会被清除，从而避免内存泄漏．**
6、ThreadLocalMap 和 HashMap 区别 ThreadLocalMap 和 HashMap 最大的区别在于解决 hash 冲突。
HashMap 使用的链地址法，而 ThreadLocalMap 使用的线性探测法/开放地址法。
当我们通过 int i = key.threadLocalHashCode &amp; (len-1) 计算出 hash 值，如果出现冲突，顺序查看表中下一单元，直到找出一个空单元或查遍全表。</content></entry><entry><title>GitHub提交设置代理</title><url>/post/github%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/</url><categories><category>problem-solving</category><category>Git</category></categories><tags><tag>Git</tag><tag>problem-solving</tag></tags><content type="html"> GitHub提交443错误可以设置代理（科学上网）解决
GitHub提交443错误设置代理 https方式提交可以通过设置代理解决，但是更好的办法是设置SSH方式来提交代码，详情请查看笔记《Git设置SSH方式》
错误描述 OpenSSL SSL_connect: Connection was reset in connection to github.com:443 看错误描述就标识ssl连接不到443端口。 可以设置代理解决。先检查git的全局配置
查看全局配置 git config --global -l 检查是否有https.proxy及http.proxy项
设置全局代理设置 示例10808端口是代理软件端口，按个人情况修改。
git config --global http.proxy 127.0.0.1:10808 git config --global https.proxy 127.0.0.1:10808 git config --global http.proxy 127.0.0.1:10818 git config --global https.proxy 127.0.0.1:10818 已有设置情况修改代理项 git config --global --unset http.proxy git config --global --unset https.proxy 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</content></entry><entry><title>B树与B+树</title><url>/post/b%E6%A0%91%E4%B8%8Eb+%E6%A0%91/</url><categories><category>Mysql</category></categories><tags><tag>数据结构</tag><tag>Mysql</tag></tags><content type="html"> B树、B+树 mysql存储引擎
一，b树 b树（balance tree）和b+树应用在数据库索引，可以认为是m叉的多路平衡查找树，但是从理论上讲，二叉树查找速度和比较次数都是最小的，为什么不用二叉树呢？
​ 平衡二叉树的查找效率是非常高的，并可以通过降低树的深度来提高查找的效率。但是当数据量非常大，树的存储的元素数量是有限的，这样会导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下。另外数据量过大会导致内存空间不够容纳平衡二叉树所有结点的情况。B树是解决这个问题的很好的结构。数据库索引是存储在磁盘上的，当数据量大时，就不能把整个索引全部加载到内存了，只能逐一加载每一个磁盘页（对应索引树的节点）。所以我们要减少IO次数，对于树来说，IO次数就是树的高度，而“矮胖”就是b树的特征之一，它的每个节点最多包含m个孩子，m称为b树的阶，m的大小取决于磁盘页的大小。
​ b树在查询时的比较次数并不比二叉树少，尤其是节点中的数非常多时，但是内存的比较速度非常快，耗时可以忽略，所以只要树的高度低，IO少，就可以提高查询性能，这是b树的优势之一。
二，b+树 b+树，是b树的一种变体，查询性能更好。m阶的b+树的特征：
有n棵子树的非叶子结点中含有n个关键字（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据）。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非叶子结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 通常在b+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。 同一个数字会在不同节点中重复出现，根节点的最大元素就是b+树的最大元素。
三、b+树相比于b树的查询优势： b+树的中间节点不保存数据，所以磁盘页能容纳更多节点元素，更“矮胖”； b+树查询必须查找到叶子节点，b树只要匹配到即可不用管元素位置，因此b+树查找更稳定（并不慢）； 对于范围查找来说，b+树只需遍历叶子节点链表即可，b树却需要重复地中序遍历。 四、图示 B+树 InnoDB
MyISAM</content></entry><entry><title>Git设置SSH方式</title><url>/post/git%E8%AE%BE%E7%BD%AEssh%E6%96%B9%E5%BC%8F/</url><categories><category>problem-solving</category><category>Git</category></categories><tags><tag>Git</tag><tag>linux</tag><tag>problem-solving</tag></tags><content type="html"> Linux环境Git上传出现：The requested URL returned error: 403解决办法小记
Git设置SSH方式 一、Gitee场景 ​ 阿里云环境push代码失败，代码托管为Gitee。切换https方式到ssh后提交成功，方式如下：
1、在linux控制台(powershell也是如此)生成sshkey $ ssh-keygen -t rsa -C &ldquo;xxxxx@xxxxx.com
&rdquo; 按照提示完成三次回车，即可生成 ssh key cat ~/.ssh/id_rsa.pub 2、将ssh key添加到gitee仓库 通过仓库主页 「管理」->「部署公钥管理」 点击黄色方框中添加个人公钥超链接 3、在linux终端输入ssh -T git@gitee.com
首次使用需要确认并添加主机到本机SSH可信列表。若返回 Hi XXX! You've successfully authenticated, but Gitee.com does not provide shell access. 内容，则证明添加成功。
4、修改原先https的下载方式 git config &ndash;list 查看配置得到 remote.origin.url=https://gitee.com/wk_acme/git_study.git 修改 git config remote.origin.url git@gitee.com
:wk_acme/git_study.git 5、push代码成功 二、Github场景 1、首先需要检查你电脑是否已经有 SSH key 运行 git Bash 客户端，输入如下代码：
$ cd ~/.ssh $ ls 这两个命令就是检查是否已经存在 id_rsa.pub 或 id_dsa.pub 文件，如果文件已经存在，那么你可以跳过步骤2，直接进入步骤3。
2、创建一个 SSH key $ ssh-keygen -t rsa -C "your_email@example.com" 代码参数含义：
-t 指定密钥类型，默认是 rsa ，可以省略。 -C 设置注释文字，比如邮箱。 -f 指定密钥文件存储文件名。
以上代码省略了 -f 参数，因此，运行上面那条命令后会让你输入一个文件名，用于保存刚才生成的 SSH key 代码，如：
Generating public/private rsa key pair. # Enter file in which to save the key (/c/Users/you/.ssh/id_rsa): [Press enter] 当然，你也可以不输入文件名，使用默认文件名（推荐），那么就会生成 id_rsa 和 id_rsa.pub 两个秘钥文件。
接着又会提示你输入两次密码（该密码是你push文件的时候要输入的密码，而不是github管理者的密码），
当然，你也可以不输入密码，直接按回车。那么push的时候就不需要输入密码，直接提交到github上了，如：
Enter passphrase (empty for no passphrase): # Enter same passphrase again: 接下来，就会显示如下代码提示，如：
Your identification has been saved in /c/Users/you/.ssh/id_rsa. # Your public key has been saved in /c/Users/you/.ssh/id_rsa.pub. # The key fingerprint is: # 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com 当你看到上面这段代码的收，那就说明，你的 SSH key 已经创建成功，你只需要添加到github的SSH key上就可以了。
3、添加你的 SSH key 到 github上面去 首先你需要拷贝 id_rsa.pub 文件的内容，你可以用编辑器打开文件复制，也可以用git命令复制该文件的内容，如：
$ clip &lt; ~/.ssh/id_rsa.pub 　Window 使用 clip 命令复制，Mac 则使用 pbcopy 命令
登录你的github账号，从又上角的设置（ Account Settings
）进入，然后点击菜单栏的 SSH key 进入页面添加 SSH key。
点击 Add SSH key 按钮添加一个 SSH key 。把你复制的 SSH key 代码粘贴到 key 所对应的输入框中，记得 SSH key 代码的前后不要留有空格或者回车。当然，上面的 Title 所对应的输入框你也可以输入一个该 SSH key 显示在 github 上的一个别名。默认的会使用你的邮件名称。
4、测试一下该SSH key 在git Bash 中输入以下代码
$ ssh -T git@github.com 当你输入以上代码时，会有一段警告代码，如：
The authenticity of host 'github.com (207.97.227.239)' can't be established. # RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48. # Are you sure you want to continue connecting (yes/no)? 这是正常的，你输入 yes 回车既可。如果你创建 SSH key 的时候设置了密码，接下来就会提示你输入密码，如：
Enter passphrase for key '/c/Users/Administrator/.ssh/id_rsa': 当然如果你密码输错了，会再要求你输入，知道对了为止。
注意：输入密码时如果输错一个字就会不正确，使用删除键是无法更正的。
密码正确后你会看到下面这段话，如：
Hi username! You've successfully authenticated, but GitHub does not # provide shell access. 成功！</content></entry><entry><title>Spring基础汇总</title><url>/post/spring%E5%9F%BA%E7%A1%80%E6%B1%87%E6%80%BB/</url><categories><category>Spring</category></categories><tags><tag>Spring</tag><tag>java</tag></tags><content type="html"> Spring基础汇总
Spring基础汇总 1.简介 1.1.简介 简介
Spring : 春天 —>给软件行业带来了春天
2002年，Rod Jahnson首次推出了Spring框架
雏形interface21框架。
2004年3月24日，Spring框架以interface21框架为基础，经过重新设计，发布了1.0正式版。
很难想象Rod Johnson的学历 , 他是悉尼大学的博士，然而他的专业不是计算机，而是音乐学。
Spring理念 : 使现有技术更加实用 . 本身就是一个大杂烩 , 整合现有的框架技术
官网 : http://spring.io/
官方下载地址 : https://repo.spring.io/libs-release-local/org/springframework/spring/
GitHub : https://github.com/spring-projects
&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-webmvc --> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>5.2.0.RELEASE&lt;/version> &lt;/dependency> &lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-jdbc --> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-jdbc&lt;/artifactId> &lt;version>5.2.0.RELEASE&lt;/version> &lt;/dependency> 1.2.优点 优点
1、Spring是一个开源免费的框架 , 容器 .
2、Spring是一个轻量级的框架 , 非侵入式的 .
3、控制反转 IoC , 面向切面 Aop
4、对事物的支持 , 对框架的支持
…
一句话概括：
Spring是一个轻量级的控制反转(IoC)和面向切面(AOP)的容器（框架）。
1.3.组成 组成
Spring 框架是一个分层架构，由 7 个定义良好的模块组成。Spring 模块构建在核心容器之上，核心容器定义了创建、配置和管理 bean 的方式 .
组成 Spring 框架的每个模块（或组件）都可以单独存在，或者与其他一个或多个模块联合实现。每个模块的功能如下：
核心容器：核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转（IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。 Spring 上下文：Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、校验和调度功能。 Spring AOP：通过配置管理特性，Spring AOP 模块直接将面向切面的编程功能 , 集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理任何支持 AOP的对象。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖组件，就可以将声明性事务管理集成到应用程序中。 Spring DAO：JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构。 Spring ORM：Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。 Spring Web 模块：Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Struts 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。 Spring MVC 框架：MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。 1.4.扩展 拓展
Spring Boot与Spring Cloud
Spring Boot 是 Spring 的一套快速配置脚手架，可以基于Spring Boot 快速开发单个微服务; Spring Cloud是基于Spring Boot实现的； Spring Boot专注于快速、方便集成的单个微服务个体，Spring Cloud关注全局的服务治理框架； Spring Boot使用了约束优于配置的理念，很多集成方案已经帮你选择好了，能不配置就不配置 , Spring Cloud很大的一部分是基于Spring Boot来实现，Spring Boot可以离开Spring Cloud独立使用开发项目，但是Spring Cloud离不开Spring Boot，属于依赖的关系。 SpringBoot在SpringClound中起到了承上启下的作用，如果你要学习SpringCloud必须要学习SpringBoot。 2.IOC理论推导 IoC基础
新建一个空白的maven项目
2.1.分析实现 分析实现
我们先用我们原来的方式写一段代码 .
1、先写一个UserDao接口
public interface UserDao { public void getUser(); } 123 2、再去写Dao的实现类
public class UserDaoImpl implements UserDao { @Override public void getUser() { System.out.println("获取用户数据"); } } 3、然后去写UserService的接口
public interface UserService { public void getUser(); } 4、最后写Service的实现类
public class UserServiceImpl implements UserService { private UserDao userDao = new UserDaoImpl(); @Override public void getUser() { userDao.getUser(); } } 5、测试一下
@Test public void test(){ UserService service = new UserServiceImpl(); service.getUser(); } 这是我们原来的方式 , 开始大家也都是这么去写的对吧 . 那我们现在修改一下 .
把Userdao的实现类增加一个 .
public class UserDaoMySqlImpl implements UserDao { @Override public void getUser() { System.out.println("MySql获取用户数据"); } } 123456 紧接着我们要去使用MySql的话 , 我们就需要去service实现类里面修改对应的实现
public class UserServiceImpl implements UserService { private UserDao userDao = new UserDaoMySqlImpl(); @Override public void getUser() { userDao.getUser(); } } 在假设, 我们再增加一个Userdao的实现类 .
public class UserDaoOracleImpl implements UserDao { @Override public void getUser() { System.out.println("Oracle获取用户数据"); } } 那么我们要使用Oracle , 又需要去service实现类里面修改对应的实现 . 假设我们的这种需求非常大 , 这种方式就根本不适用了, 甚至反人类对吧 , 每次变动 , 都需要修改大量代码 . 这种设计的耦合性太高了, 牵一发而动全身 .
那我们如何去解决呢 ?
我们可以在需要用到他的地方 , 不去实现它 , 而是留出一个接口 , 利用set , 我们去代码里修改下 .
public class UserServiceImpl implements UserService { private UserDao userDao; // 利用set实现 public void setUserDao(UserDao userDao) { this.userDao = userDao; } @Override public void getUser() { userDao.getUser(); } } 现在去我们的测试类里 , 进行测试 ;
@Test public void test(){ UserServiceImpl service = new UserServiceImpl(); service.setUserDao( new UserDaoMySqlImpl() ); service.getUser(); //那我们现在又想用Oracle去实现呢 service.setUserDao( new UserDaoOracleImpl() ); service.getUser(); } 大家发现了区别没有 ? 可能很多人说没啥区别 . 但是同学们 , 他们已经发生了根本性的变化 , 很多地方都不一样了 . 仔细去思考一下 , 以前所有东西都是由程序去进行控制创建 , 而现在是由我们自行控制创建对象 , 把主动权交给了调用者 . 程序不用去管怎么创建,怎么实现了 . 它只负责提供一个接口 .
这种思想 , 从本质上解决了问题 , 我们程序员不再去管理对象的创建了 , 更多的去关注业务的实现 . 耦合性大大降低 . 这也就是IOC的原型 !
2.2.IOC本质 IOC本质
控制反转IoC(Inversion of Control)，是一种设计思想，DI(依赖注入)是实现IoC的一种方法，也有人认为DI只是IoC的另一种说法。没有IoC的程序中 , 我们使用面向对象编程 , 对象的创建与对象间的依赖关系完全硬编码在程序中，对象的创建由程序自己控制，控制反转后将对象的创建转移给第三方，个人认为所谓控制反转就是：获得依赖对象的方式反转了。
IoC是Spring框架的核心内容，使用多种方式完美的实现了IoC，可以使用XML配置，也可以使用注解，新版本的Spring也可以零配置实现IoC。
Spring容器在初始化时先读取配置文件，根据配置文件或元数据创建与组织对象存入容器中，程序使用时再从Ioc容器中取出需要的对象。
采用XML方式配置Bean的时候，Bean的定义信息是和实现分离的，而采用注解的方式可以把两者合为一体，Bean的定义信息直接以注解的形式定义在实现类中，从而达到了零配置的目的。
控制反转是一种通过描述（XML或注解）并通过第三方去生产或获取特定对象的方式。在Spring中实现控制反转的是IoC容器，其实现方法是依赖注入（Dependency Injection,DI）。
3.HelloSpring 导入Jar包
注 : spring 需要导入commons-logging进行日志记录 . 我们利用maven , 他会自动下载对应的依赖项 .
&lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>5.1.10.RELEASE&lt;/version> &lt;/dependency> 编写代码
1、编写一个Hello实体类
public class Hello { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } public void show(){ System.out.println("Hello,"+ name ); } } 2、编写我们的spring文件 , 这里我们命名为beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;!--bean就是java对象 , 由Spring创建和管理--> &lt;bean id="hello" class="com.kuang.pojo.Hello"> &lt;property name="name" value="Spring"/> &lt;/bean> &lt;/beans> 3、我们可以去进行测试了 .
@Test public void test(){ //解析beans.xml文件 , 生成管理相应的Bean对象 ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); //getBean : 参数即为spring配置文件中bean的id . Hello hello = (Hello) context.getBean("hello"); hello.show(); } 思考
Hello 对象是谁创建的 ? hello 对象是由Spring创建的 Hello 对象的属性是怎么设置的 ? hello 对象的属性是由Spring容器设置的 这个过程就叫控制反转 :
控制 : 谁来控制对象的创建 , 传统应用程序的对象是由程序本身控制创建的 , 使用Spring后 , 对象是由Spring来创建的 反转 : 程序本身不创建对象 , 而变成被动的接收对象 . 依赖注入 : 就是利用set方法来进行注入的.
IOC是一种编程思想，由主动的编程变成被动的接收
可以通过newClassPathXmlApplicationContext去浏览一下底层源码 .
修改案例一
我们在案例一中， 新增一个Spring配置文件beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;bean id="MysqlImpl" class="com.kuang.dao.impl.UserDaoMySqlImpl"/> &lt;bean id="OracleImpl" class="com.kuang.dao.impl.UserDaoOracleImpl"/> &lt;bean id="ServiceImpl" class="com.kuang.service.impl.UserServiceImpl"> &lt;!--注意: 这里的name并不是属性 , 而是set方法后面的那部分 , 首字母小写--> &lt;!--引用另外一个bean , 不是用value 而是用 ref--> &lt;property name="userDao" ref="OracleImpl"/> &lt;/bean> &lt;/beans> 测试！
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserServiceImpl serviceImpl = (UserServiceImpl) context.getBean("ServiceImpl"); serviceImpl.getUser(); } 123456 OK , 到了现在 , 我们彻底不用再程序中去改动了 , 要实现不同的操作 , 只需要在xml配置文件中进行修改 , 所谓的IoC,一句话搞定 : 对象由Spring 来创建 , 管理 , 装配 !
4.IOC创建对象方式 4.1.通过无参构造方法来创建 通过无参构造方法来创建
1、User.java
public class User { private String name; public User() { System.out.println("user无参构造方法"); } public void setName(String name) { this.name = name; } public void show(){ System.out.println("name="+ name ); } } 1234567891011121314151617 2、beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;bean id="user" class="com.kuang.pojo.User"> &lt;property name="name" value="kuangshen"/> &lt;/bean> &lt;/beans> 3、测试类
@Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); //在执行getBean的时候, user已经创建好了 , 通过无参构造 User user = (User) context.getBean("user"); //调用对象的方法 . user.show(); } 结果可以发现，在调用show方法之前，User对象已经通过无参构造初始化了！
4.2.通过有参构造方法来创建 通过有参构造方法来创建
1、UserT . java
public class UserT { private String name; public UserT(String name) { this.name = name; } public void setName(String name) { this.name = name; } public void show(){ System.out.println("name="+ name ); } } 2、beans.xml 有三种方式编写
&lt;!-- 第一种根据index参数下标设置 --> &lt;bean id="userT" class="com.kuang.pojo.UserT"> &lt;!-- index指构造方法 , 下标从0开始 --> &lt;constructor-arg index="0" value="kuangshen2"/> &lt;/bean> &lt;!-- 第二种根据参数名字设置 --> &lt;bean id="userT" class="com.kuang.pojo.UserT"> &lt;!-- name指参数名 --> &lt;constructor-arg name="name" value="kuangshen2"/> &lt;/bean> &lt;!-- 第三种根据参数类型设置 --> &lt;bean id="userT" class="com.kuang.pojo.UserT"> &lt;constructor-arg type="java.lang.String" value="kuangshen2"/> &lt;/bean> 3、测试
@Test public void testT(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserT user = (UserT) context.getBean("userT"); user.show(); } 123456 结论：在配置文件加载的时候。其中管理的对象都已经初始化了！
5.Spring配置 5.1.别名 别名
alias 设置别名 , 为bean设置别名 , 可以设置多个别名
&lt;!--设置别名：在获取Bean的时候可以使用别名获取--> &lt;alias name="userT" alias="userNew"/> 5.2.Bean的配置 Bean的配置
&lt;!--bean就是java对象,由Spring创建和管理--> &lt;!-- id 是bean的标识符,要唯一,如果没有配置id,name就是默认标识符 如果配置id,又配置了name,那么name是别名 name可以设置多个别名,可以用逗号,分号,空格隔开 如果不配置id和name,可以根据applicationContext.getBean(.class)获取对象; class是bean的全限定名=包名+类名 --> &lt;bean id="hello" name="hello2 h2,h3;h4" class="com.kuang.pojo.Hello"> &lt;property name="name" value="Spring"/> &lt;/bean> 5.3.import import
团队的合作通过import来实现 .
&lt;import resource="{path}/beans.xml"/> 1 6.依赖注入（DI） Dependency Injection
概念
依赖注入（Dependency Injection,DI）。 依赖 : 指Bean对象的创建依赖于容器 . Bean对象的依赖资源 . 注入 : 指Bean对象所依赖的资源 , 由容器来设置和装配 . 6.1.构造器注入 构造器注入
我们在之前的案例已经讲过了
6.2.Set 注入 （重点） Set 注入 （重点）
要求被注入的属性 , 必须有set方法 , set方法的方法名由set + 属性首字母大写 , 如果属性是boolean类型 , 没有set方法 , 是 is .
测试pojo类 :
Address.java
public class Address { private String address; public String getAddress() { return address; } public void setAddress(String address) { this.address = address; } } Student.java
package com.kuang.pojo; import java.util.List; import java.util.Map; import java.util.Properties; import java.util.Set; public class Student { private String name; private Address address; private String[] books; private List&lt;String> hobbys; private Map&lt;String,String> card; private Set&lt;String> games; private String wife; private Properties info; public void setName(String name) { this.name = name; } public void setAddress(Address address) { this.address = address; } public void setBooks(String[] books) { this.books = books; } public void setHobbys(List&lt;String> hobbys) { this.hobbys = hobbys; } public void setCard(Map&lt;String, String> card) { this.card = card; } public void setGames(Set&lt;String> games) { this.games = games; } public void setWife(String wife) { this.wife = wife; } public void setInfo(Properties info) { this.info = info; } public void show(){ System.out.println("name="+ name + ",address="+ address.getAddress() + ",books=" ); for (String book:books){ System.out.print("&lt;&lt;"+book+">>\t"); } System.out.println("\n爱好:"+hobbys); System.out.println("card:"+card); System.out.println("games:"+games); System.out.println("wife:"+wife); System.out.println("info:"+info); } } 6.3.扩展的注入 1、常量注入
&lt;bean id="student" class="com.kuang.pojo.Student"> &lt;property name="name" value="小明"/> &lt;/bean> 测试：
@Test public void test01(){ ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); Student student = (Student) context.getBean("student"); System.out.println(student.getName()); } 2、Bean注入
注意点：这里的值是一个引用，ref
&lt;bean id="addr" class="com.kuang.pojo.Address"> &lt;property name="address" value="重庆"/> &lt;/bean> &lt;bean id="student" class="com.kuang.pojo.Student"> &lt;property name="name" value="小明"/> &lt;property name="address" ref="addr"/> &lt;/bean> 3、数组注入
&lt;bean id="student" class="com.kuang.pojo.Student"> &lt;property name="name" value="小明"/> &lt;property name="address" ref="addr"/> &lt;property name="books"> &lt;array> &lt;value>西游记&lt;/value> &lt;value>红楼梦&lt;/value> &lt;value>水浒传&lt;/value> &lt;/array> &lt;/property> &lt;/bean> 4、List注入
&lt;property name="hobbys"> &lt;list> &lt;value>听歌&lt;/value> &lt;value>看电影&lt;/value> &lt;value>爬山&lt;/value> &lt;/list> &lt;/property> 5、Map注入
&lt;property name="card"> &lt;map> &lt;entry key="中国邮政" value="456456456465456"/> &lt;entry key="建设" value="1456682255511"/> &lt;/map> &lt;/property> 6、set注入
&lt;property name="games"> &lt;set> &lt;value>LOL&lt;/value> &lt;value>BOB&lt;/value> &lt;value>COC&lt;/value> &lt;/set> &lt;/property> 7、Null注入
&lt;property name="wife">&lt;null/>&lt;/property> 8、Properties注入
&lt;property name="info"> &lt;props> &lt;prop key="学号">20190604&lt;/prop> &lt;prop key="性别">男&lt;/prop> &lt;prop key="姓名">小明&lt;/prop> &lt;/props> &lt;/property> 测试结果： 9、p命名和c命名注入
p命名和c命名注入
User.java ：【注意：这里没有有参构造器！】
public class User { private String name; private int age; public void setName(String name) { this.name = name; } public void setAge(int age) { this.age = age; } @Override public String toString() { return "User{" + "name='" + name + '\'' + ", age=" + age + '}'; } } 1、P命名空间注入 : 需要在头文件中加入约束文件
导入约束 : xmlns:p="http://www.springframework.org/schema/p" &lt;!--P(属性: properties)命名空间 , 直接注入属性--> &lt;bean id="user" class="com.kuang.pojo.User" p:name="狂神" p:age="18"/> 2、c 命名空间注入 : 需要在头文件中加入约束文件
导入约束 : xmlns:c="http://www.springframework.org/schema/c" &lt;!--C(构造: Constructor)命名空间 , 使用构造器注入--> &lt;bean id="user" class="com.kuang.pojo.User" c:name="狂神" c:age="18"/> 发现问题：爆红了，刚才我们没有写有参构造！
解决：把有参构造器加上，这里也能知道，c 就是所谓的构造器注入！
测试代码：
@Test public void test02(){ ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); User user = (User) context.getBean("user"); System.out.println(user); } 6.4.Bean的作用域 Bean的作用域
在Spring中，那些组成应用程序的主体及由Spring IoC容器所管理的对象，被称之为bean。简单地讲，bean就是由IoC容器初始化、装配及管理的对象 . 几种作用域中，request、session作用域仅在基于web的应用中使用（不必关心你所采用的是什么web应用框架），只能用在基于web的Spring ApplicationContext环境。
Singleton(单例模式) 当一个bean的作用域为Singleton，那么Spring IoC容器中只会存在一个共享的bean实例，并且所有对bean的请求，只要id与该bean定义相匹配，则只会返回bean的同一实例。Singleton是单例类型，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，他都存在了，每次获取到的对象都是同一个对象。注意，Singleton作用域是Spring中的缺省作用域。要在XML中将bean定义成singleton，可以这样配置：
&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" scope="singleton"> 测试：
@Test public void test03(){ ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); User user = (User) context.getBean("user"); User user2 = (User) context.getBean("user"); System.out.println(user==user2); } Prototype(原型模式) 当一个bean的作用域为Prototype，表示一个bean定义对应多个对象实例。Prototype作用域的bean会导致在每次对该bean请求（将其注入到另一个bean中，或者以程序的方式调用容器的getBean()方法）时都会创建一个新的bean实例。Prototype是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。根据经验，对有状态的bean应该使用prototype作用域，而对无状态的bean则应该使用singleton作用域。在XML中将bean定义成prototype，可以这样配置：
&lt;bean id="account" class="com.foo.DefaultAccount" scope="prototype"/> 或者 &lt;bean id="account" class="com.foo.DefaultAccount" singleton="false"/> Request 当一个bean的作用域为Request，表示在一次HTTP请求中，一个bean定义对应一个实例；即每个HTTP请求都会有各自的bean实例，它们依据某个bean定义创建而成。该作用域仅在基于web的Spring ApplicationContext情形下有效。考虑下面bean定义：
&lt;bean id="loginAction" class=cn.csdn.LoginAction" scope="request"/> 1 针对每次HTTP请求，Spring容器会根据loginAction bean的定义创建一个全新的LoginAction bean实例，且该loginAction bean实例仅在当前HTTP request内有效，因此可以根据需要放心的更改所建实例的内部状态，而其他请求中根据loginAction bean定义创建的实例，将不会看到这些特定于某个请求的状态变化。当处理请求结束，request作用域的bean实例将被销毁。
Session 当一个bean的作用域为Session，表示在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。考虑下面bean定义：
&lt;bean id="userPreferences" class="com.foo.UserPreferences" scope="session"/> 1 针对某个HTTP Session，Spring容器会根据userPreferences bean定义创建一个全新的userPreferences bean实例，且该userPreferences bean仅在当前HTTP Session内有效。与request作用域一样，可以根据需要放心的更改所创建实例的内部状态，而别的HTTP Session中根据userPreferences创建的实例，将不会看到这些特定于某个HTTP Session的状态变化。当HTTP Session最终被废弃的时候，在该HTTP Session作用域内的bean也会被废弃掉。
7.Bean的自动装配 自动装配说明
自动装配是使用spring满足bean依赖的一种方法 spring会在应用上下文中为某个bean寻找其依赖的bean。 Spring中bean有三种装配机制，分别是：
在xml中显式配置； 在java中显式配置； 隐式的bean发现机制和自动装配。 这里我们主要讲第三种：自动化的装配bean。
Spring的自动装配需要从两个角度来实现，或者说是两个操作：
组件扫描(component scanning)：spring会自动发现应用上下文中所创建的bean； 自动装配(autowiring)：spring自动满足bean之间的依赖，也就是我们说的IoC/DI； 组件扫描和自动装配组合发挥巨大威力，使得显示的配置降低到最少。
推荐不使用自动装配xml配置 , 而使用注解 .
** **
测试环境搭建
1、新建一个项目
2、新建两个实体类，Cat Dog 都有一个叫的方法
public class Cat { public void shout() { System.out.println("miao~"); } } public class Dog { public void shout() { System.out.println("wang~"); } } 3、新建一个用户类 User
public class User { private Cat cat; private Dog dog; private String str; } 4、编写Spring配置文件
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> &lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User"> &lt;property name="cat" ref="cat"/> &lt;property name="dog" ref="dog"/> &lt;property name="str" value="qinjiang"/> &lt;/bean> &lt;/beans> 5、测试
public class MyTest { @Test public void testMethodAutowire() { ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); User user = (User) context.getBean("user"); user.getCat().shout(); user.getDog().shout(); } } 结果正常输出，环境OK
7.1.byName byName
autowire byName (按名称自动装配)
由于在手动配置xml过程中，常常发生字母缺漏和大小写等错误，而无法对其进行检查，使得开发效率降低。
采用自动装配将避免这些错误，并且使配置简单化。
测试：
1、修改bean配置，增加一个属性 autowire=“byName”
&lt;bean id="user" class="com.kuang.pojo.User" autowire="byName"> &lt;property name="str" value="qinjiang"/> &lt;/bean> 2、再次测试，结果依旧成功输出！
3、我们将 cat 的bean id修改为 catXXX
4、再次测试， 执行时报空指针java.lang.NullPointerException。因为按byName规则找不对应set方法，真正的setCat就没执行，对象就没有初始化，所以调用时就会报空指针错误。
小结：
当一个bean节点带有 autowire byName的属性时。
将查找其类中所有的set方法名，例如setCat，获得将set去掉并且首字母小写的字符串，即cat。 去spring容器中寻找是否有此字符串名称id的对象。 如果有，就取出注入；如果没有，就报空指针异常。 7.2.byType byType
autowire byType (按类型自动装配)
使用autowire byType首先需要保证：同一类型的对象，在spring容器中唯一。如果不唯一，会报不唯一的异常。
NoUniqueBeanDefinitionException 测试：
1、将user的bean配置修改一下 ： autowire=“byType”
2、测试，正常输出
3、在注册一个cat 的bean对象！
&lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat" class="com.kuang.pojo.Cat"/> &lt;bean id="cat2" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User" autowire="byType"> &lt;property name="str" value="qinjiang"/> &lt;/bean> 1234567 4、测试，报错：NoUniqueBeanDefinitionException
5、删掉cat2，将cat的bean名称改掉！测试！因为是按类型装配，所以并不会报异常，也不影响最后的结果。甚至将id属性去掉，也不影响结果。
这就是按照类型自动装配！
7.3.使用注解 使用注解
jdk1.5开始支持注解，spring2.5开始全面支持注解。
准备工作：利用注解的方式注入属性。
1、在spring配置文件中引入context文件头
xmlns:context="http://www.springframework.org/schema/context" http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd 2、开启属性注解支持！
&lt;context:annotation-config/> @Autowired @Autowired是按类型自动转配的，不支持id匹配。 需要导入 spring-aop的包！ 测试：
1、将User类中的set方法去掉，使用@Autowired注解
public class User { @Autowired private Cat cat; @Autowired private Dog dog; private String str; public Cat getCat() { return cat; } public Dog getDog() { return dog; } public String getStr() { return str; } } 2、此时配置文件内容
&lt;context:annotation-config/> &lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User"/> 3、测试，成功输出结果！
【小狂神科普时间】
@Autowired(required=false) 说明：false，对象可以为null；true，对象必须存对象，不能为null。
//如果允许对象为null，设置required = false,默认为true @Autowired(required = false) private Cat cat; @Qualifier @Autowired是根据类型自动装配的，加上@Qualifier则可以根据byName的方式自动装配 @Qualifier不能单独使用。 测试实验步骤：
1、配置文件修改内容，保证类型存在对象。且名字不为类的默认名字！
&lt;bean id="dog1" class="com.kuang.pojo.Dog"/> &lt;bean id="dog2" class="com.kuang.pojo.Dog"/> &lt;bean id="cat1" class="com.kuang.pojo.Cat"/> &lt;bean id="cat2" class="com.kuang.pojo.Cat"/> 2、没有加Qualifier测试，直接报错
3、在属性上添加Qualifier注解
@Autowired @Qualifier(value = "cat2") private Cat cat; @Autowired @Qualifier(value = "dog2") private Dog dog; 测试，成功输出！
@Resource @Resource如有指定的name属性，先按该属性进行byName方式查找装配； 其次再进行默认的byName方式进行装配； 如果以上都不成功，则按byType的方式自动装配。 都不成功，则报异常。 实体类：
public class User { //如果允许对象为null，设置required = false,默认为true @Resource(name = "cat2") private Cat cat; @Resource private Dog dog; private String str; } beans.xml
&lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat1" class="com.kuang.pojo.Cat"/> &lt;bean id="cat2" class="com.kuang.pojo.Cat"/> &lt;bean id="user" class="com.kuang.pojo.User"/> 测试：结果OK
配置文件2：beans.xml ， 删掉cat2
&lt;bean id="dog" class="com.kuang.pojo.Dog"/> &lt;bean id="cat1" class="com.kuang.pojo.Cat"/> 实体类上只保留注解
@Resource private Cat cat; @Resource private Dog dog; 结果：OK
结论：先进行byName查找，失败；再进行byType查找，成功。
小结
@Autowired与@Resource异同：
1、@Autowired与@Resource都可以用来装配bean。都可以写在字段上，或写在setter方法上。
2、@Autowired默认按类型装配（属于spring规范），默认情况下必须要求依赖对象必须存在，如果要允许null 值，可以设置它的required属性为false，如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用
3、@Resource（属于J2EE复返），默认按照名称进行装配，名称可以通过name属性进行指定。如果没有指定name属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。
它们的作用相同都是用注解方式注入对象，但执行顺序不同。@Autowired先byType，@Resource先byName。
8.使用注解开发 说明
在spring4之后，想要使用注解形式，必须得要引入aop的包
在配置文件当中，还得要引入一个context约束
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"> &lt;/beans> 8.1.Bean的实现 我们之前都是使用 bean 的标签进行bean注入，但是实际开发中，我们一般都会使用注解！
1、配置扫描哪些包下的注解 &lt;!--指定注解扫描包--> &lt;context:component-scan base-package="com.kuang.pojo"/> 2、在指定包下编写类，增加注解
@Component("user") // 相当于配置文件中 &lt;bean id="user" class="当前注解的类"/> public class User { public String name = "秦疆"; } 3、测试
@Test public void test(){ ApplicationContext applicationContext = new ClassPathXmlApplicationContext("beans.xml"); User user = (User) applicationContext.getBean("user"); System.out.println(user.name); } 8.2.属性注入 使用注解注入属性
1、可以不用提供set方法，直接在直接名上添加@value(“值”)
@Component("user") // 相当于配置文件中 &lt;bean id="user" class="当前注解的类"/> public class User { @Value("秦疆") // 相当于配置文件中 &lt;property name="name" value="秦疆"/> public String name; } 2、如果提供了set方法，在set方法上添加@value(“值”);
@Component("user") public class User { public String name; @Value("秦疆") public void setName(String name) { this.name = name; } } 8.3.衍生注解 我们这些注解，就是替代了在配置文件当中配置步骤而已！更加的方便快捷！
@Component三个衍生注解
为了更好的进行分层，Spring可以使用其它三个注解，功能一样，目前使用哪一个功能都一样。
@Controller：controller层 @Service：service层 @Repository：dao层 写上这些注解，就相当于将这个类交给Spring管理装配了！
8.4.自动装配注解 在Bean的自动装配已经讲过了，可以回顾！
8.5.作用域 @scope
singleton：默认的，Spring会采用单例模式创建这个对象。关闭工厂 ，所有的对象都会销毁。 prototype：多例模式。关闭工厂 ，所有的对象不会销毁。内部的垃圾回收机制会回收 @Controller("user") @Scope("prototype") public class User { @Value("秦疆") public String name; } 8.6.小结 XML与注解比较
XML可以适用任何场景 ，结构清晰，维护方便 注解不是自己提供的类使用不了，开发简单方便 xml与注解整合开发 ：推荐最佳实践
xml管理Bean 注解完成属性注入 使用过程中， 可以不用扫描，扫描是为了类上的注解 &lt;context:annotation-config/> 作用：
进行注解驱动注册，从而使注解生效 用于激活那些已经在spring容器里注册过的bean上面的注解，也就是显示的向Spring注册 如果不扫描包，就需要手动配置bean 如果不加注解驱动，则注入的值为null！ 9.基于Java类进行配置 JavaConfig 原来是 Spring 的一个子项目，它通过 Java 类的方式提供 Bean 的定义信息，在 Spring4 的版本， JavaConfig 已正式成为 Spring4 的核心功能 。
测试：
1、编写一个实体类，Dog
@Component //将这个类标注为Spring的一个组件，放到容器中！ public class Dog { public String name = "dog"; } 2、新建一个config配置包，编写一个MyConfig配置类
@Configuration //代表这是一个配置类 public class MyConfig { @Bean //通过方法注册一个bean，这里的返回值就Bean的类型，方法名就是bean的id！ public Dog dog(){ return new Dog(); } } 3、测试
@Test public void test2(){ ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MyConfig.class); Dog dog = (Dog) applicationContext.getBean("dog"); System.out.println(dog.name); } 4、成功输出结果！
导入其他配置如何做呢？
1、我们再编写一个配置类！
@Configuration //代表这是一个配置类 public class MyConfig2 { } 2、在之前的配置类中我们来选择导入这个配置类
@Configuration @Import(MyConfig2.class) //导入合并其他配置类，类似于配置文件中的 inculde 标签 public class MyConfig { @Bean public Dog dog(){ return new Dog(); } } 关于这种Java类的配置方式，我们在之后的SpringBoot 和 SpringCloud中还会大量看到，我们需要知道这些注解的作用即可！
10.代理模式 为什么要学习代理模式，因为AOP的底层机制就是动态代理！
代理模式：
静态代理 动态代理 学习aop之前 , 我们要先了解一下代理模式！
10.1静态代理 静态代理角色分析
抽象角色 : 一般使用接口或者抽象类来实现 真实角色 : 被代理的角色 代理角色 : 代理真实角色 ; 代理真实角色后 , 一般会做一些附属的操作 . 客户 : 使用代理角色来进行一些操作 . 代码实现
Rent . java 即抽象角色
//抽象角色：租房 public interface Rent { public void rent(); } Host . java 即真实角色
//真实角色: 房东，房东要出租房子 public class Host implements Rent{ public void rent() { System.out.println("房屋出租"); } } Proxy . java 即代理角色
//代理角色：中介 public class Proxy implements Rent { private Host host; public Proxy() { } public Proxy(Host host) { this.host = host; } //租房 public void rent(){ seeHouse(); host.rent(); fare(); } //看房 public void seeHouse(){ System.out.println("带房客看房"); } //收中介费 public void fare(){ System.out.println("收中介费"); } Client . java 即客户
//客户类，一般客户都会去找代理！ public class Client { public static void main(String[] args) { //房东要租房 Host host = new Host(); //中介帮助房东 Proxy proxy = new Proxy(host); //你去找中介！ proxy.rent(); } } 分析：在这个过程中，你直接接触的就是中介，就如同现实生活中的样子，你看不到房东，但是你依旧租到了房东的房子通过代理，这就是所谓的代理模式，程序源自于生活，所以学编程的人，一般能够更加抽象的看待生活中发生的事情。
静态代理的好处:
可以使得我们的真实角色更加纯粹 . 不再去关注一些公共的事情 . 公共的业务由代理来完成 . 实现了业务的分工 , 公共业务发生扩展时变得更加集中和方便 . 缺点 :
类多了 , 多了代理类 , 工作量变大了 . 开发效率降低 . 我们想要静态代理的好处，又不想要静态代理的缺点，所以 , 就有了动态代理 !
10.2.静态代理再理解 同学们练习完毕后，我们再来举一个例子，巩固大家的学习！
练习步骤：
1、创建一个抽象角色，比如咋们平时做的用户业务，抽象起来就是增删改查！
//抽象角色：增删改查业务 public interface UserService { void add(); void delete(); void update(); void query(); } 2、我们需要一个真实对象来完成这些增删改查操作
//真实对象，完成增删改查操作的人 public class UserServiceImpl implements UserService { public void add() { System.out.println("增加了一个用户"); } public void delete() { System.out.println("删除了一个用户"); } public void update() { System.out.println("更新了一个用户"); } public void query() { System.out.println("查询了一个用户"); } } 3、需求来了，现在我们需要增加一个日志功能，怎么实现！
思路1 ：在实现类上增加代码 【麻烦！】 思路2：使用代理来做，能够不改变原来的业务情况下，实现此功能就是最好的了！ 4、设置一个代理类来处理日志！代理角色
//代理角色，在这里面增加日志的实现 public class UserServiceProxy implements UserService { private UserServiceImpl userService; public void setUserService(UserServiceImpl userService) { this.userService = userService; } public void add() { log("add"); userService.add(); } public void delete() { log("delete"); userService.delete(); } public void update() { log("update"); userService.update(); } public void query() { log("query"); userService.query(); } public void log(String msg){ System.out.println("执行了"+msg+"方法"); } } 5、测试访问类：
public class Client { public static void main(String[] args) { //真实业务 UserServiceImpl userService = new UserServiceImpl(); //代理类 UserServiceProxy proxy = new UserServiceProxy(); //使用代理类实现日志功能！ proxy.setUserService(userService); proxy.add(); } } OK，到了现在代理模式大家应该都没有什么问题了，重点大家需要理解其中的思想；
我们在不改变原来的代码的情况下，实现了对原有功能的增强，这是AOP中最核心的思想
聊聊AOP：纵向开发，横向开发
10.3动态代理 动态代理的角色和静态代理的一样 . 动态代理的代理类是动态生成的 . 静态代理的代理类是我们提前写好的 动态代理分为两类 : 一类是基于接口动态代理 , 一类是基于类的动态代理 基于接口的动态代理&mdash;-JDK动态代理 基于类的动态代理–cglib 现在用的比较多的是 javasist 来生成动态代理 . 百度一下javasist 我们这里使用JDK的原生代码来实现，其余的道理都是一样的！、 JDK的动态代理需要了解两个类
核心 : InvocationHandler 和 Proxy ， 打开JDK帮助文档看看
【InvocationHandler：调用处理程序】
Object invoke(Object proxy, 方法 method, Object[] args)； //参数 //proxy - 调用该方法的代理实例 //method -所述方法对应于调用代理实例上的接口方法的实例。方法对象的声明类将是该方法声明的接口，它可以是代理类继承该方法的代理接口的超级接口。 //args -包含的方法调用传递代理实例的参数值的对象的阵列，或null如果接口方法没有参数。原始类型的参数包含在适当的原始包装器类的实例中，例如java.lang.Integer或java.lang.Boolean 。 【Proxy : 代理】
//生成代理类 public Object getProxy(){ return Proxy.newProxyInstance(this.getClass().getClassLoader(), rent.getClass().getInterfaces(),this); } 代码实现
抽象角色和真实角色和之前的一样！
Rent . java 即抽象角色
//抽象角色：租房 public interface Rent { public void rent(); } 1234 Host . java 即真实角色
//真实角色: 房东，房东要出租房子 public class Host implements Rent{ public void rent() { System.out.println("房屋出租"); } } 123456 ProxyInvocationHandler. java 即代理角色
public class ProxyInvocationHandler implements InvocationHandler { private Rent rent; public void setRent(Rent rent) { this.rent = rent; } //生成代理类，重点是第二个参数，获取要代理的抽象角色！之前都是一个角色，现在可以代理一类角色 public Object getProxy(){ return Proxy.newProxyInstance(this.getClass().getClassLoader(), rent.getClass().getInterfaces(),this); } // proxy : 代理类 method : 代理类的调用处理程序的方法对象. // 处理代理实例上的方法调用并返回结果 @Override public Object invoke(Object proxy, Method method, Object[] args) throwsThrowable { seeHouse(); //核心：本质利用反射实现！ Object result = method.invoke(rent, args); fare(); return result; } //看房 public void seeHouse(){ System.out.println("带房客看房"); } //收中介费 public void fare(){ System.out.println("收中介费"); } } 12345678910111213141516171819202122232425262728293031323334 Client . java
//租客 public class Client { public static void main(String[] args) { //真实角色 Host host = new Host(); //代理实例的调用处理程序 ProxyInvocationHandler pih = new ProxyInvocationHandler(); pih.setRent(host); //将真实角色放置进去！ Rent proxy = (Rent)pih.getProxy(); //动态生成对应的代理类！ proxy.rent(); } } 核心：一个动态代理 , 一般代理某一类业务 , 一个动态代理可以代理多个类，代理的是接口！、
10.4深化理解 我们来使用动态代理实现代理我们后面写的UserService！
我们也可以编写一个通用的动态代理实现的类！所有的代理对象设置为Object即可！
public class ProxyInvocationHandler implements InvocationHandler { private Object target; public void setTarget(Object target) { this.target = target; } //生成代理类 public Object getProxy(){ return Proxy.newProxyInstance(this.getClass().getClassLoader(), target.getClass().getInterfaces(),this); } // proxy : 代理类 // method : 代理类的调用处理程序的方法对象. public Object invoke(Object proxy, Method method, Object[] args) throwsThrowable { log(method.getName()); Object result = method.invoke(target, args); return result; } public void log(String methodName){ System.out.println("执行了"+methodName+"方法"); } } 测试！
public class Test { public static void main(String[] args) { //真实对象 UserServiceImpl userService = new UserServiceImpl(); //代理对象的调用处理程序 ProxyInvocationHandler pih = new ProxyInvocationHandler(); pih.setTarget(userService); //设置要代理的对象 UserService proxy = (UserService)pih.getProxy(); //动态生成代理类！ proxy.delete(); } } 测试，增删改查，查看结果！
10.5动态代理的好处 静态代理有的它都有，静态代理没有的，它也有！
可以使得我们的真实角色更加纯粹 . 不再去关注一些公共的事情 . 公共的业务由代理来完成 . 实现了业务的分工 , 公共业务发生扩展时变得更加集中和方便 . 一个动态代理 , 一般代理某一类业务 一个动态代理可以代理多个类，代理的是接口！ 11.AOP 11.1.什么是AOP AOP（Aspect Oriented Programming）意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生范型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。
11.2.Aop在Spring中的作用 提供声明式事务；允许用户自定义切面
以下名词需要了解下：
横切关注点：跨越应用程序多个模块的方法或功能。即是，与我们业务逻辑无关的，但是我们需要关注的部分，就是横切关注点。如日志 , 安全 , 缓存 , 事务等等 … 切面（ASPECT）：横切关注点 被模块化 的特殊对象。即，它是一个类。 通知（Advice）：切面必须要完成的工作。即，它是类中的一个方法。 目标（Target）：被通知对象。 代理（Proxy）：向目标对象应用通知之后创建的对象。 切入点（PointCut）：切面通知 执行的 “地点”的定义。 连接点（JointPoint）：与切入点匹配的执行点。 SpringAOP中，通过Advice定义横切逻辑，Spring中支持5种类型的Advice:
即 Aop 在 不改变原有代码的情况下 , 去增加新的功能 .
11.3.使用Spring实现Aop 【重点】使用AOP织入，需要导入一个依赖包！
&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjweaver --> &lt;dependency> &lt;groupId>org.aspectj&lt;/groupId> &lt;artifactId>aspectjweaver&lt;/artifactId> &lt;version>1.9.4&lt;/version> &lt;/dependency> 11.3.1.通过 Spring API 实现 第一种方式
首先编写我们的业务接口和实现类
public interface UserService { public void add(); public void delete(); public void update(); public void search(); } public class UserServiceImpl implements UserService{ @Override public void add() { System.out.println("增加用户"); } @Override public void delete() { System.out.println("删除用户"); } @Override public void update() { System.out.println("更新用户"); } @Override public void search() { System.out.println("查询用户"); } } 然后去写我们的增强类 , 我们编写两个 , 一个前置增强 一个后置增强
public class Log implements MethodBeforeAdvice { //method : 要执行的目标对象的方法 //objects : 被调用的方法的参数 //Object : 目标对象 @Override public void before(Method method, Object[] objects, Object o) throws Throwable { System.out.println( o.getClass().getName() + "的" + method.getName() + "方法被执行了"); } } public class AfterLog implements AfterReturningAdvice { //returnValue 返回值 //method被调用的方法 //args 被调用的方法的对象的参数 //target 被调用的目标对象 @Override public void afterReturning(Object returnValue, Method method, Object[] args,Object target) throws Throwable { System.out.println("执行了" + target.getClass().getName() +"的"+method.getName()+"方法," +"返回值："+returnValue); } } 最后去spring的文件中注册 , 并实现aop切入实现 , 注意导入约束 .
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd"> &lt;!--注册bean--> &lt;bean id="userService" class="com.kuang.service.UserServiceImpl"/> &lt;bean id="log" class="com.kuang.log.Log"/> &lt;bean id="afterLog" class="com.kuang.log.AfterLog"/> &lt;!--aop的配置--> &lt;aop:config> &lt;!--切入点 expression:表达式匹配要执行的方法--> &lt;aop:pointcut id="pointcut" expression="execution(* com.kuang.service.UserServiceImpl.*(..))"/> &lt;!--执行环绕; advice-ref执行方法 . pointcut-ref切入点--> &lt;aop:advisor advice-ref="log" pointcut-ref="pointcut"/> &lt;aop:advisor advice-ref="afterLog" pointcut-ref="pointcut"/> &lt;/aop:config> &lt;/beans> 测试
public class MyTest { @Test public void test(){ ApplicationContext context = newClassPathXmlApplicationContext("beans.xml"); UserService userService = (UserService) context.getBean("userService"); userService.search(); } } Aop的重要性 : 很重要 . 一定要理解其中的思路 , 主要是思想的理解这一块 .
Spring的Aop就是将公共的业务 (日志 , 安全等) 和领域业务结合起来 , 当执行领域业务时 , 将会把公共业务加进来 . 实现公共业务的重复利用 . 领域业务更纯粹 , 程序猿专注领域业务 , 其本质还是动态代理 .
11.3.2.自定义类来实现Aop 第二种方式
目标业务类不变依旧是userServiceImpl
第一步 : 写我们自己的一个切入类
public class DiyPointcut { public void before(){ System.out.println("---------方法执行前---------"); } public void after(){ System.out.println("---------方法执行后---------"); } 去spring中配置
&lt;!--第二种方式自定义实现--> &lt;!--注册bean--> &lt;bean id="diy" class="com.kuang.config.DiyPointcut"/> &lt;!--aop的配置--> &lt;aop:config> &lt;!--第二种方式：使用AOP的标签实现--> &lt;aop:aspect ref="diy"> &lt;aop:pointcut id="diyPonitcut" expression="execution(* com.kuang.service.UserServiceImpl.*(..))"/> &lt;aop:before pointcut-ref="diyPonitcut" method="before"/> &lt;aop:after pointcut-ref="diyPonitcut" method="after"/> &lt;/aop:aspect> &lt;/aop:config> 测试：
public class MyTest { @Test public void test(){ ApplicationContext context = newClassPathXmlApplicationContext("beans.xml"); UserService userService = (UserService) context.getBean("userService"); userService.add(); } } 11.3.4.使用注解实现 第三种方式
第一步：编写一个注解实现的增强类
package com.kuang.config; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.After; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; @Aspect public class AnnotationPointcut { @Before("execution(* com.kuang.service.UserServiceImpl.*(..))") public void before(){ System.out.println("---------方法执行前---------"); } @After("execution(* com.kuang.service.UserServiceImpl.*(..))") public void after(){ System.out.println("---------方法执行后---------"); } @Around("execution(* com.kuang.service.UserServiceImpl.*(..))") public void around(ProceedingJoinPoint jp) throws Throwable { System.out.println("环绕前"); System.out.println("签名:"+jp.getSignature()); //执行目标方法proceed Object proceed = jp.proceed(); System.out.println("环绕后"); System.out.println(proceed); } } 第二步：在Spring配置文件中，注册bean，并增加支持注解的配置
&lt;!--第三种方式:注解实现--> &lt;bean id="annotationPointcut" class="com.kuang.config.AnnotationPointcut"/> &lt;aop:aspectj-autoproxy/> aop:aspectj-autoproxy：说明
通过aop命名空间的&lt;aop:aspectj-autoproxy />声明自动为spring容器中那些配置@aspectJ切面的bean创建代理，织入切面。当然，spring 在内部依旧采用AnnotationAwareAspectJAutoProxyCreator进行自动代理的创建工作，但具体实现的细节已经被&lt;aop:aspectj-autoproxy />隐藏起来了 &lt;aop:aspectj-autoproxy />有一个proxy-target-class属性，默认为false，表示使用jdk动态代理织入增强，当配为&lt;aop:aspectj-autoproxy poxy-target-class="true"/>时，表示使用CGLib动态代理技术织入增强。不过即使proxy-target-class设置为false，如果目标类没有声明接口，则spring将自动使用CGLib动态代理。 12.整合MyBatis 步骤 1、导入相关jar包
junit
&lt;dependency> &lt;groupId>junit&lt;/groupId> &lt;artifactId>junit&lt;/artifactId> &lt;version>4.12&lt;/version> &lt;/dependency> mybatis
&lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis&lt;/artifactId> &lt;version>3.5.2&lt;/version> &lt;/dependency> mysql-connector-java
&lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;version>5.1.47&lt;/version> &lt;/dependency> spring相关
&lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>5.1.10.RELEASE&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-jdbc&lt;/artifactId> &lt;version>5.1.10.RELEASE&lt;/version> &lt;/dependency> aspectJ AOP 织入器
&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjweaver --> &lt;dependency> &lt;groupId>org.aspectj&lt;/groupId> &lt;artifactId>aspectjweaver&lt;/artifactId> &lt;version>1.9.4&lt;/version> &lt;/dependency> mybatis-spring整合包 【重点】
&lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis-spring&lt;/artifactId> &lt;version>2.0.2&lt;/version> &lt;/dependency> 配置Maven静态资源过滤问题！
&lt;build> &lt;resources> &lt;resource> &lt;directory>src/main/java&lt;/directory> &lt;includes> &lt;include>**/*.properties&lt;/include> &lt;include>**/*.xml&lt;/include> &lt;/includes> &lt;filtering>true&lt;/filtering> &lt;/resource> &lt;/resources> &lt;/build> 2、编写配置文件
3、代码实现
12.1回忆MyBatis 编写pojo实体类
package com.kuang.pojo; public class User { private int id; //id private String name; //姓名 private String pwd; //密码 } 实现mybatis的配置文件
&lt;?xml version="1.0" encoding="UTF-8" ?> &lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"> &lt;configuration> &lt;typeAliases> &lt;package name="com.kuang.pojo"/> &lt;/typeAliases> &lt;environments default="development"> &lt;environment id="development"> &lt;transactionManager type="JDBC"/> &lt;dataSource type="POOLED"> &lt;property name="driver" value="com.mysql.jdbc.Driver"/> &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8"/> &lt;property name="username" value="root"/> &lt;property name="password" value="123456"/> &lt;/dataSource> &lt;/environment> &lt;/environments> &lt;mappers> &lt;package name="com.kuang.dao"/> &lt;/mappers> &lt;/configuration> UserDao接口编写
public interface UserMapper { public List&lt;User> selectUser(); } 接口对应的Mapper映射文件
&lt;?xml version="1.0" encoding="UTF-8" ?> &lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"> &lt;mapper namespace="com.kuang.dao.UserMapper"> &lt;select id="selectUser" resultType="User"> select * from user &lt;/select> &lt;/mapper> 测试类
@Test public void selectUser() throws IOException { String resource = "mybatis-config.xml"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = newSqlSessionFactoryBuilder().build(inputStream); SqlSession sqlSession = sqlSessionFactory.openSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); List&lt;User> userList = mapper.selectUser(); for (User user: userList){ System.out.println(user); } sqlSession.close(); } 12.2.MyBatis-Spring学习 引入Spring之前需要了解mybatis-spring包中的一些重要类；
http://www.mybatis.org/spring/zh/index.html
什么是 MyBatis-Spring？
MyBatis-Spring 会帮助你将 MyBatis 代码无缝地整合到 Spring 中。
知识基础
在开始使用 MyBatis-Spring 之前，你需要先熟悉 Spring 和 MyBatis 这两个框架和有关它们的术语。这很重要
MyBatis-Spring 需要以下版本：
MyBatis-Spring MyBatis Spring 框架 Spring Batch Java 2.0 3.5+ 5.0+ 4.0+ Java 8+ 1.3 3.4+ 3.2.2+ 2.1+ Java 6+ 如果使用 Maven 作为构建工具，仅需要在 pom.xml 中加入以下代码即可：
&lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis-spring&lt;/artifactId> &lt;version>2.0.2&lt;/version> &lt;/dependency> 要和 Spring 一起使用 MyBatis，需要在 Spring 应用上下文中定义至少两样东西：一个 SqlSessionFactory 和至少一个数据映射器类。
在 MyBatis-Spring 中，可使用SqlSessionFactoryBean来创建 SqlSessionFactory。要配置这个工厂 bean，只需要把下面代码放在 Spring 的 XML 配置文件中：
&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"> &lt;property name="dataSource" ref="dataSource" /> &lt;/bean> 注意：SqlSessionFactory需要一个 DataSource（数据源）。这可以是任意的 DataSource，只需要和配置其它 Spring 数据库连接一样配置它就可以了。
在基础的 MyBatis 用法中，是通过 SqlSessionFactoryBuilder 来创建 SqlSessionFactory 的。而在 MyBatis-Spring 中，则使用 SqlSessionFactoryBean 来创建。
在 MyBatis 中，你可以使用 SqlSessionFactory 来创建 SqlSession。一旦你获得一个 session 之后，你可以使用它来执行映射了的语句，提交或回滚连接，最后，当不再需要它的时候，你可以关闭 session。
SqlSessionFactory有一个唯一的必要属性：用于 JDBC 的 DataSource。这可以是任意的 DataSource 对象，它的配置方法和其它 Spring 数据库连接是一样的。
一个常用的属性是 configLocation，它用来指定 MyBatis 的 XML 配置文件路径。它在需要修改 MyBatis 的基础配置非常有用。通常，基础配置指的是 &lt; settings> 或 &lt; typeAliases>元素。
需要注意的是，这个配置文件并不需要是一个完整的 MyBatis 配置。确切地说，任何环境配置（），数据源（）和 MyBatis 的事务管理器（）都会被忽略。SqlSessionFactoryBean 会创建它自有的 MyBatis 环境配置（Environment），并按要求设置自定义环境的值。
SqlSessionTemplate 是 MyBatis-Spring 的核心。作为 SqlSession 的一个实现，这意味着可以使用它无缝代替你代码中已经在使用的 SqlSession。
模板可以参与到 Spring 的事务管理中，并且由于其是线程安全的，可以供多个映射器类使用，你应该总是用 SqlSessionTemplate 来替换 MyBatis 默认的 DefaultSqlSession 实现。在同一应用程序中的不同类之间混杂使用可能会引起数据一致性的问题。
可以使用 SqlSessionFactory 作为构造方法的参数来创建 SqlSessionTemplate 对象。
&lt;bean id="sqlSession" class="org.mybatis.spring.SqlSessionTemplate"> &lt;constructor-arg index="0" ref="sqlSessionFactory" /> &lt;/bean> 现在，这个 bean 就可以直接注入到你的 DAO bean 中了。你需要在你的 bean 中添加一个 SqlSession 属性，就像下面这样：
public class UserDaoImpl implements UserDao { private SqlSession sqlSession; public void setSqlSession(SqlSession sqlSession) { this.sqlSession = sqlSession; } public User getUser(String userId) { return sqlSession.getMapper...; } } 按下面这样，注入 SqlSessionTemplate：
&lt;bean id="userDao" class="org.mybatis.spring.sample.dao.UserDaoImpl"> &lt;property name="sqlSession" ref="sqlSession" /> &lt;/bean> 12.3.整合实现一 1、引入Spring配置文件beans.xml
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"> 2、配置数据源替换mybaits的数据源
&lt;!--配置数据源：数据源有非常多，可以使用第三方的，也可使使用Spring的--> &lt;bean id="dataSource"class="org.springframework.jdbc.datasource.DriverManagerDataSource"> &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"/> &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8"/> &lt;property name="username" value="root"/> &lt;property name="password" value="123456"/> &lt;/bean> 3、配置SqlSessionFactory，关联MyBatis
&lt;!--配置SqlSessionFactory--> &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"> &lt;property name="dataSource" ref="dataSource"/> &lt;!--关联Mybatis--> &lt;property name="configLocation" value="classpath:mybatis-config.xml"/> &lt;property name="mapperLocations" value="classpath:com/kuang/dao/*.xml"/> &lt;/bean> 4、注册sqlSessionTemplate，关联sqlSessionFactory；
&lt;!--注册sqlSessionTemplate , 关联sqlSessionFactory--> &lt;bean id="sqlSession" class="org.mybatis.spring.SqlSessionTemplate"> &lt;!--利用构造器注入--> &lt;constructor-arg index="0" ref="sqlSessionFactory"/> &lt;/bean> 5、增加Dao接口的实现类；私有化sqlSessionTemplate
public class UserDaoImpl implements UserMapper { //sqlSession不用我们自己创建了，Spring来管理 private SqlSessionTemplate sqlSession; public void setSqlSession(SqlSessionTemplate sqlSession) { this.sqlSession = sqlSession; } public List&lt;User> selectUser() { UserMapper mapper = sqlSession.getMapper(UserMapper.class); return mapper.selectUser(); } } 6、注册bean实现
&lt;bean id="userDao" class="com.kuang.dao.UserDaoImpl"> &lt;property name="sqlSession" ref="sqlSession"/> &lt;/bean> 7、测试
@Test public void test2(){ ApplicationContext context = newClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); } 结果成功输出！现在我们的Mybatis配置文件的状态！发现都可以被Spring整合！
&lt;?xml version="1.0" encoding="UTF-8" ?> &lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"> &lt;configuration> &lt;typeAliases> &lt;package name="com.kuang.pojo"/> &lt;/typeAliases> &lt;/configuration> 12.4.整合实现二 mybatis-spring1.2.3版以上的才有这个 .
官方文档截图 :
dao继承Support类 , 直接利用 getSqlSession() 获得 , 然后直接注入SqlSessionFactory . 比起方式1 , 不需要管理SqlSessionTemplate , 而且对事务的支持更加友好 . 可跟踪源码查看
测试：
1、将我们上面写的UserDaoImpl修改一下
public class UserDaoImpl extends SqlSessionDaoSupport implements UserMapper { public List&lt;User> selectUser() { UserMapper mapper = getSqlSession().getMapper(UserMapper.class); return mapper.selectUser(); } } 2、修改bean的配置
&lt;bean id="userDao" class="com.kuang.dao.UserDaoImpl"> &lt;property name="sqlSessionFactory" ref="sqlSessionFactory" /> &lt;/bean> 3、测试
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); } 总结 : 整合到spring以后可以完全不要mybatis的配置文件，除了这些方式可以实现整合之外，我们还可以使用注解来实现，这个等我们后面学习SpringBoot的时候还会测试整合！
13.声明式事务 13.1.回顾事务 事务在项目开发过程非常重要，涉及到数据的一致性的问题，不容马虎！ 事务管理是企业级应用程序开发中必备技术，用来确保数据的完整性和一致性。 事务就是把一系列的动作当成一个独立的工作单元，这些动作要么全部完成，要么全部不起作用。
事务四个属性ACID
原子性（atomicity） 事务是原子性操作，由一系列动作组成，事务的原子性确保动作要么全部完成，要么完全不起作用 一致性（consistency） 一旦所有事务动作完成，事务就要被提交。数据和资源处于一种满足业务规则的一致性状态中 隔离性（isolation） 可能多个事务会同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏 持久性（durability） 事务一旦完成，无论系统发生什么错误，结果都不会受到影响。通常情况下，事务的结果被写到持久化存储器中 测试 将上面的代码拷贝到一个新项目中
在之前的案例中，我们给userDao接口新增两个方法，删除和增加用户；
//添加一个用户 int addUser(User user); //根据id删除用户 int deleteUser(int id); mapper文件，我们故意把 deletes 写错，测试！
&lt;insert id="addUser" parameterType="com.kuang.pojo.User"> insert into user (id,name,pwd) values (#{id},#{name},#{pwd}) &lt;/insert> &lt;delete id="deleteUser" parameterType="int"> deletes from user where id = #{id} &lt;/delete> 编写接口的实现类，在实现类中，我们去操作一波
public class UserDaoImpl extends SqlSessionDaoSupport implements UserMapper { //增加一些操作 public List&lt;User> selectUser() { User user = new User(4,"小明","123456"); UserMapper mapper = getSqlSession().getMapper(UserMapper.class); mapper.addUser(user); mapper.deleteUser(4); return mapper.selectUser(); } //新增 public int addUser(User user) { UserMapper mapper = getSqlSession().getMapper(UserMapper.class); return mapper.addUser(user); } //删除 public int deleteUser(int id) { UserMapper mapper = getSqlSession().getMapper(UserMapper.class); return mapper.deleteUser(id); } } 测试
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); } 报错：sql异常，delete写错了
结果 ：插入成功！
没有进行事务的管理；我们想让他们都成功才成功，有一个失败，就都失败，我们就应该需要事务！
以前我们都需要自己手动管理事务，十分麻烦！
但是Spring给我们提供了事务管理，我们只需要配置即可；
13.2.Spring中的事务管理 Spring在不同的事务管理API之上定义了一个抽象层，使得开发人员不必了解底层的事务管理API就可以使用Spring的事务管理机制。Spring支持编程式事务管理和声明式的事务管理。
编程式事务管理
将事务管理代码嵌到业务方法中来控制事务的提交和回滚 缺点：必须在每个事务操作业务逻辑中包含额外的事务管理代码 声明式事务管理
一般情况下比编程式事务好用。 将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。 将事务管理作为横切关注点，通过aop方法模块化。Spring中通过Spring AOP框架支持声明式事务管理。 使用Spring管理事务，注意头文件的约束导入 : tx
xmlns:tx="http://www.springframework.org/schema/tx" http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"> 事务管理器
无论使用Spring的哪种事务管理策略（编程式或者声明式）事务管理器都是必须的。 就是 Spring的核心事务管理抽象，管理封装了一组独立于技术的方法。 JDBC事务
&lt;bean id="transactionManager"class="org.springframework.jdbc.datasource.DataSourceTransactionManager"> &lt;property name="dataSource" ref="dataSource" /> &lt;/bean> 配置好事务管理器后我们需要去配置事务的通知
&lt;!--配置事务通知--> &lt;tx:advice id="txAdvice" transaction-manager="transactionManager"> &lt;tx:attributes> &lt;!--配置哪些方法使用什么样的事务,配置事务的传播特性--> &lt;tx:method name="add" propagation="REQUIRED"/> &lt;tx:method name="delete" propagation="REQUIRED"/> &lt;tx:method name="update" propagation="REQUIRED"/> &lt;tx:method name="search*" propagation="REQUIRED"/> &lt;tx:method name="get" read-only="true"/> &lt;tx:method name="*" propagation="REQUIRED"/> &lt;/tx:attributes> &lt;/tx:advice> spring事务传播特性：
事务传播行为就是多个事务方法相互调用时，事务如何在这些方法间传播。spring支持7种事务传播行为：
propagation_requierd：如果当前没有事务，就新建一个事务，如果已存在一个事务中，加入到这个事务中，这是最常见的选择。 propagation_supports：支持当前事务，如果没有当前事务，就以非事务方法执行。 propagation_mandatory：使用当前事务，如果没有当前事务，就抛出异常。 propagation_required_new：新建事务，如果当前存在事务，把当前事务挂起。 propagation_not_supported：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 propagation_never：以非事务方式执行操作，如果当前事务存在则抛出异常。 propagation_nested：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与propagation_required类似的操作 Spring 默认的事务传播行为是 PROPAGATION_REQUIRED，它适合于绝大多数的情况。
假设 ServiveX#methodX() 都工作在事务环境下（即都被 Spring 事务增强了），假设程序中存在如下的调用链：Service1#method1()->Service2#method2()->Service3#method3()，那么这 3 个服务类的 3 个方法通过 Spring 的事务传播机制都工作在同一个事务中。
就好比，我们刚才的几个方法存在调用，所以会被放在一组事务当中！
配置AOP
导入aop的头文件！
&lt;!--配置aop织入事务--> &lt;aop:config> &lt;aop:pointcut id="txPointcut" expression="execution(* com.kuang.dao.*.*(..))"/> &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="txPointcut"/> &lt;/aop:config> 进行测试
删掉刚才插入的数据，再次测试！
@Test public void test2(){ ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); UserMapper mapper = (UserMapper) context.getBean("userDao"); List&lt;User> user = mapper.selectUser(); System.out.println(user); }</content></entry><entry><title>精通Git</title><url>/post/%E7%B2%BE%E9%80%9Agit/</url><categories><category>Git</category></categories><tags><tag>Git</tag></tags><content type="html"> 精通Git
精通Git 1、Git基础 （1）三种状态 Git 有三种状态，你的文件可能处于其中之一：已提交（committed）、已修改（modified）和已暂存（staged）。已提交表示数据已经安全的保存在本地数据库中。已修改表示修改了文件，但还没保存到数据库中。已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中
由此引入 Git 项目的三个工作区域的概念：Git 仓库、工作目录以及暂存区域。
Git 仓库目录是 Git 用来保存项目的元数据和对象数据库的地方。这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。
工作目录是对项目的某个版本独立提取出来的内容。这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。
暂存区域是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。有时候也被称作`‘索引’&rsquo;，不过一般说法还是叫暂存区域。
总结：如果 Git 目录中保存着的特定版本文件，就属于已提交状态。如果作了修改并已放入暂存区域，就属于已暂存状态。如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。
（2）记录数据方式 Git直接记录快照，而非差异比较
Git 和其它版本控制系统（包括 Subversion 和近似工具）的主要差别在于 Git 对待数据的方法。概念上来区分，其它大部分系统以文件变更列表的方式存储信息。这类系统（CVS、Subversion、Perforce、Bazaar 等等）将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。
Git 不按照以上方式对待或保存数据。反之，Git 更像是把数据看作是对小型文件系统的一组快照。每次你提交
更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。Git 对待数据更像是一个 快照流。
Git 更像是一个小型的文件系统，提供了许多以此为基础构建的超强工具，而不只是一个简单的 VCS
（3）近乎所有操作都是本地执行和保证文件完整性** 2、配置信息 ​ &ndash;global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 &ndash;global 选项的命令来配置
git config &ndash;global user.name &ldquo;kw&rdquo;
git config &ndash;global user.email 1121034507@qq.com
git config &ndash;list &lt;!--命令来列出所有 Git 当时能找到的配置-->
git config &ndash;globalcredential.helper cache/store 缓存用户名/密码，cache-15分钟 store-永久
配置Git可以用git config &ndash;list查看配置项，再使用命令git config 配置项 设置值
3、git基础操作 git init &lt;!--在当前目录初始化git-->
**$ git add *.c ** &lt;!--暂存命令，添加一个或多个文件到暂存区（开始跟踪一个新文件，或者把已跟踪的文件放到暂存区，概括起来就是添加内容到下一次提交中）-->
$ git add LICENSE
$ git commit -m &lsquo;initial project version&rsquo; &lt;!--提交命令,将暂存区文件提交到本地仓库-->
git clone https://github.com/libgit2/libgit2
[别名]&lt;!--克隆远程仓库-->
**git status **&lt;!--检查当前文件状态-->
$ git status -s
M README &lt;!--右边的 M 表示该文件被修改了但是还没放入暂存区-->
M lib/simplegit.rb &lt;!--左边的 M文件被修改了并将修改后的文件放入了暂存区-->
MM Rakefile &lt;!--在工作区被修改并提交到暂存区后又在工作区中被修改了，所以在暂存区和工作区都有该文件被修改了的记录-->
**A lib/git.rb ** &lt;!--新添加到暂存区中的文件前面有 A 标记，修改过的文件前面有 M 标记-->
?? LICENSE.txt &lt;!--新添加的未跟踪文件前面有 ?? 标记-->
git diff &lt;!--**查看已暂存和未暂存的修改**-->
要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff： 要查看已暂存的将要添加到下次提交里的内容，可以用 git diff &ndash;cached/&ndash;staged git commit -m &ldquo;message&rdquo; &lt;!--将暂存区文件提交到本地仓库-->
git commit -a -m &ldquo;message&rdquo; &lt;!--将已追踪文件提交到本地仓库，省略git add .操作-->
**git rm 文件名 **&lt;!--从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。该命令连带从工作目录中删除文件-->
git rm &ndash;cached README&lt;!--从本地仓库删除，当然也包括暂存区和工作目录-->
**git rm log/\*.log **&lt;!--删除 log/ 目录下扩展名为 .log 的所有文件。星号 * 之前的反斜杠 \，因为 Git 有它自己的文件模式扩展匹配方式-->
git rm \*~ &lt;!--删除以 ~ 结尾的所有文件-->
git mv file_from file_to &lt;!--改名操作-->
**git log **&lt;!--查看提交历史-->
git log -p -2 &lt;!--（-p）查看提交差异，2表示最近两次提交-->**
**git log &ndash;stat **&lt;!--简略信息-->**
**git log &ndash;pretty=format:"%h - %an, %ar : %s" ** &lt;!-- --pretty。这个选项可以指定使用不同于默认格式的方式展示提交历史-->
ca82a6d - Scott Chacon, 6 years ago : changed the version number 085bb3b - Scott Chacon, 6 years ago : removed unnecessary test a11bef0 - Scott Chacon, 6 years ago : first commit ​ 用 &ndash;author 选项显示指定作者的提交，用 &ndash;grep 选项搜索提交说明中的关键字。（请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用 &ndash;all-match 选项。否则，满足任意一个条件的提交都会被匹配出来）。另一个非常有用的筛选选项是 -S，可以列出那些添加或移除了某些字符串的提交。比如说，你想找出添加或移除了某一个特定函数的引用的提交，你可以这样使用： $ git log -Sfunction_name
​
选项 说明 -(n) 仅显示最近的 n 条提交 &ndash;since, &ndash;after 仅显示指定时间之后的提交。 &ndash;until, &ndash;before 仅显示指定时间之前的提交 &ndash;author 仅显示指定作者相关的提交。 &ndash;committer 仅显示指定提交者相关的提交。 &ndash;grep 仅显示含指定关键字的提交 -S 仅显示添加或移除了某个关键字的提交 git tag 查看标签
git tag -a v1.4 -m &lsquo;my version 1.4&rsquo; 增加标签V1.4
git tag tagname 轻量标签
$ git log --pretty=oneline 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge branch 'experiment' a6b4c97498bd301d84096da251c98a07c7723e65 beginning write support 0d52aaab4479697da7686c15f77a3d64d9165190 one more thing $ git tag -a v1.2 0d52aaa //对历史提交打标签，如果不加提交校验和会对在最近一次的提交打tag git push origin [tagname] 推送tag到远程仓库（git push不会推送tag）
git push origin &ndash;tags 推送所有标签
git checkout -b [branchname] [tagname] 在特定的标签上创建一个新分支
Switched to a new branch &lsquo;version2&rsquo;
git config 设置别名
$ git config --global alias.co checkout $ git config --global alias.br branch $ git config --global alias.ci commit $ git config --global alias.st status 4、Git分支 ​ Git 保存的不是文件的变化或者差异，而是一系列不同时刻的文件快照。Git 的分支，其实本质上仅仅是指向提交对象的可变指针。Git 的默认分支名字是 master。在多次提交操作之后，其实已经有一个指向最后那个提交对象的 master 分支。它会在每次的提交操作中自动向前移动。Git 又是怎么知道当前在哪一个分支上呢？也很简单，它有一个名为 HEAD 的特殊指针
git branch testing 创建分支testing
git checkout testing 切换分支
$ vim test.txt $ git commit -a -m 'made a change' 如图所示，testing 分支向前移动了，但是 master 分支却没有，它仍然指向运行 git checkout 时所指的对象。分支切换会改变你工作目录中的文件在切换分支时，一定要注意你工作目录里的文件会被改变。如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。如果 Git 不能干净利落地完成这个任务，它将禁止切换分支，要留意工作目录和暂存区里那些还没有被提交的修改，它可能会和你即将检出的分支产生冲突从而阻止 Git 切换到该分支（Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样）
git checkout -b iss53 新建并检出分支iss53
//等于以下两条命令 $ git branch iss53 $ git checkout iss53 ​ 从一个远程跟踪分支检出一个本地分支会自动创建一个叫做 “跟踪分支”（有时候也叫做 “上游分支”）。跟踪分支是与远程分支有直接关系的本地分支。如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。
​ 当克隆一个仓库时，它通常会自动地创建一个跟踪 origin/master 的 master 分支。然而，如果你愿意的话可以设置其他的跟踪分支 - 其他远程仓库上的跟踪分支，或者不跟踪 master 分支。最简单的就是之前看到的例子，运行 git checkout -b [branch] [remotename]/[branch]。这是一个十分常用的操作所以 Git 提供了 &ndash;track 快捷方式：
$ git checkout --track origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' 如果想要将本地分支与远程分支设置为不同名字，你可以轻松地增加一个不同名字的本地分支的上一个命令：
$ git checkout -b sf origin/serverfix Branch sf set up to track remote branch serverfix from origin. Switched to a new branch 'sf' 现在，本地分支 sf 会自动从 origin/serverfix 拉取。
设置已有的本地分支跟踪一个刚刚拉取下来的远程分支，或者想要修改正在跟踪的上游分支，你可以在任意时间使用 -u 或 &ndash;set-upstream-to 选项运行 git branch 来显式地设置。
$ git branch -u origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. git merge 合并分支
$ git checkout master //检出master分支 $ git merge hotfix //合并testing分支到master 遇到冲突时的分支合并：如果在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法干净的合并它们，这就是合并冲突
​
$ git merge iss53 Auto-merging index.html CONFLICT (content): Merge conflict in index.html Automatic merge failed; fix conflicts and then commit the result. //查看具体冲突信息 $ git status On branch master You have unmerged paths. (fix conflicts and run "git commit") Unmerged paths: (use "git add &lt;file>..." to mark resolution) both modified: index.html no changes added to commit (use "git add" and/or "git commit -a") //解决冲突 手动修改冲突文件后，对每个文件使用 git add 命令来将其标记为冲突已解决 $ git add index.html //查看状态，已无冲突 $ git status On branch master All conflicts fixed but you are still merging. (use "git commit" to conclude merge) Changes to be committed: modified: index.html //合并提交 $ git commit 使用git commit提交合并 git branch 分支管理，不加任务参数情况下显示所有分支列表
$ git branch  iss53 * master //当前检出分支  testing git branch -v 要查看每一个分支的最后一次提交
$ git branch -v  iss53 93b412c fix javascript issue * master 7a98805 Merge branch 'iss53'  testing 782fd34 add scott to the author list in the readmes git branch -vv 查看设置的所有跟踪分支
$ git branch -vv  iss53 7e424c3 [origin/iss53: ahead 2] forgot the brackets  master 1ae2a45 [origin/master] deploying index fix * serverfix f8674d9 [teamone/server-fix-good: ahead 3, behind 1] this should do it  testing 5ea463a trying something new ​ 这里可以看到 iss53 分支正在跟踪 origin/iss53 并且 “ahead” 是 2，意味着本地有两个提交还没有推送到服务器上。也能看到 master 分支正在跟踪 origin/master 分支并且是最新的。接下来可以看到serverfix 分支正在跟踪 teamone 服务器上的 server-fix-good 分支并且领先 2 落后 1，意味着服务器上有一次提交还没有合并入同时本地有三次提交还没有推送。最后看到 testing 分支并没有跟踪任何远程分支。
​ 需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。可以像这样做：$ git fetch &ndash;all; git branch -vv
git branch [&ndash;no-merged/&ndash;merged] 查看已经合并或尚未合并到当前分支的分支
git branch -d branchname 删除分支 -d无法删除未合并的分支，-D可以强制删除
远程分支 git ls-remote (remote)或者git remote show (remote)来显示远程分支列表或者详细信息
​ Git 的 clone 命令会为你自动将其命名为 origin，拉取它的所有数据，创建一个指向它的 master 分支的指针，并且在本地将命名为 origin/master。Git 也会给你一个与 origin 的 master 分支在指向同一个地方的本地 master 分支，这样你就有工作的基础
​ 远程仓库名字 “origin” 与分支名字 “master” 一样，在 Git 中并没有任何特别的含义一样。同时 “master” 是当你运行 git init 时默认的起始分支名字，原因仅仅是它的广泛使用，“origin” 是当你运行 git clone 时默认的远程仓库名字。如果你运行 git clone -o booyah，那么你默认的远程分支名字将会是 booyah/master
​ ​ 如果你在本地的 master 分支做了一些工作，然而在同一时间，其他人推送提交到 git.ourcompany.com 并更新了它的 master 分支，那么你的提交历史将向不同的方向前进。也许，只要你不与 origin 服务器连接，你的 origin/master 指针就不会移动。
​ git fetch origin 在进行同步工作工作时，从远程仓库抓取本地没有的数据，更新本地数据库并将origin/master向后移动
​
git remote add 添加远程仓库分支
git remote add 别名 远程仓库
git fetch 拉取远程仓库
​ 当 git fetch 命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。它只会获取数据然后让你自己合并。然而，有一个命令叫作 git pull 在大多数情况下它的含义是一个 git fetch 紧接着一个git merge 命令。如果有一个像之前章节中演示的设置好的跟踪分支，不管它是显式地设置还是通过 clone 或checkout 命令为你创建的，git pull 都会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。
​ 运行 git fetch teamone 来抓取远程仓库 teamone 有而本地没有的数据。因为那台服务器上现
有的数据是 origin 服务器上的一个子集，所以 Git 并不会抓取数据而是会设置远程跟踪分支
teamone/master 指向 teamone 的 master 分支
git push 推送
git push (remote) (branch)
git push &ndash;all 推送到所有远程仓库
git push origin &ndash;delete serverfix 删除远程分支serverfix
$ git push origin --delete serverfix To https://github.com/schacon/simplegit  - [deleted] serverfix 变基
在 Git 中整合来自不同分支的修改主要有两种方法：merge 以及 rebase
​ ​ 之前介绍过，整合分支最容易的方法是 merge 命令。它会把两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交）。
使用rebase也可以达到目的，提取在 C4 中引入的补丁和修改，然后在 C3 的基础上再应用一次。在 Git 中，这种操作就叫做 变基。你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样
$ git checkout experiment $ git rebase master First, rewinding head to replay your work on top of it... Applying: added staged command 它的原理是首先找到这两个分支（即当前分支 experiment、变基操作的目标基底分支 master）的最近共同祖先 C2，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底 C3, 最后以此将之前另存为临时文件的修改依序应用。（译注：写明了 commit id，以便理解，下同）
现在回到 master 分支，进行一次快进合并。
$ git checkout master $ git merge experiment ​ 此时，C4&rsquo; 指向的快照就和上面使用 merge 命令的例子中 C5 指向的快照一模一样了。这两种整合方法的最终结果没有任何区别，但是变基使得提交历史更加整洁。你在查看一个经过变基的分支的历史记录时会发现，尽管实际的开发工作是并行的，但它们看上去就像是先后串行的一样，提交历史是一条直线没有分叉。
​ 一般我们这样做的目的是为了确保在向远程分支推送时能保持提交历史的整洁——例如向某个别人维护的项目贡献代码时。在这种情况下，你首先在自己的分支里进行开发，当开发完成时你需要先将你的代码变基到origin/master 上，然后再向主项目提交修改。这样的话，该项目的维护者就不再需要进行整合工作，只需要快进合并便可。
​ 请注意，无论是通过变基，还是通过三方合并，整合的最终结果所指向的快照始终是一样的，只不过提交历史不同罢了。变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。
变基的风险
呃，奇妙的变基也并非完美无缺，要用它得遵守一条准则：不要对在你的仓库外有副本的分支执行变基。</content></entry><entry><title>Spring学习记录</title><url>/post/spring%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url><categories><category>Spring</category><category>源码</category></categories><tags><tag>Spring</tag><tag>源码</tag></tags><content type="html"> 根据雷神《spring注解驱动开发》整理记录知识点
Spring学习记录 本文是一篇spring(4.3.12.release版本)源码相关的文章。本文的源码解析就不按照传统的去贴代码的方式去讲解spring源码了，本文的源码解析以流程的方式来讲解spring在每一步都干了什么，所以谓之曰spring源码导读。。。。
在正式开始spring源码导读之前，读者总得知道spring里的各个标签是干啥的吧，因此文中前一部分罗列了spring常见的注解用法。并搞了点SpringAOP和spring事务源码的解析作为后面正式开始的导读的开胃菜
介绍完了，让我们开始吧！！！。
spring注解 @Configuration 用于标注配置类 @Bean 结合@Configuration（full mode）使用或结合@Component（light mode）使用。可以导入第三方组件,入方法有参数默认从IOC容器中获取，可以指定initMethod和destroyMethod 指定初始化和销毁方法,多实例对象不会调用销毁方法. 包扫描@ComponentScan (@ComponentScans可以配置多个扫描,@TypeFilter:指定过滤规则,自己实现TypeFilter类) 组件(@Service、@Controller、@Repository):包扫描+组件注解导入注解。 @Scope:设置组件作用域 1.prototype:多例的2.singleton:单例的（默认值） @Lazy 懒加载 @Conditional({Condition}):按照一定的条件进行判断,满足条件给容器中注册Bean,传入Condition数组,，使用时需自己创建类继承Condition然后重写match方法。 @Import[快速给容器中导入一个组件] Import(类名),容器中就会自动注册这个组件，id默认是组件的全名 ImportSelector：返回需要导入的组件的全类名的数组 ImportBeanDefinitionRegistrar：手动注册bean FactoryBean:工厂Bean,交给spring用来生产Bean到spring容器中.可以通过前缀&amp;来获取工厂Bean本身. @Value:给属性赋值,也可以使用SpEL和外部文件的值 @PropertySource:读取外部配置文件中的k/v保存到运行环境中,结合@value使用,或使用ConfigurableEnvironment获取 @Profile:结合@Bean使用,默认为default环境,可以通过命令行参数来切换环境 自定义组件使用Spring容器底层的组件:需要让自定义组件实现xxxAware，(例如:ApplicationContextAware),spring在创建对象的时候,会帮我们自动注入。spring通过BeanPostProcessor机制来实现XXXXAware的自动注入。 ApplicationContextProcessor.java private void invokeAwareInterfaces(Object bean) { if (bean instanceof Aware) { if (bean instanceof ResourceLoaderAware) { ((ResourceLoaderAware)bean).setResourceLoader(this.applicationContext); } if (bean instanceof ApplicationContextAware) { ((ApplicationContextAware)bean).setApplicationContext(this.applicationContext); } } } @Autowried 装配优先级如下: 使用按照类型去容器中找对应的组件 按照属性名称去作为组件id去找对应的组件 @Qualifier:指定默认的组件,结合@Autowried使用 &ndash;标注在构造器:spring创建对象调用构造器创建对象 &ndash;标注在方法上: @Primary:spring自动装配的时候,默认首先bean,配合@Bean使用 @Resource(JSR250):jsr规范:按照组件名称进行装配 @Inject(JSR330):jsr规范和@Autowired功能一致,不支持require=false; Bean生命周期: 初始化和销毁
通过@Bean 指定init-method和destroy-method 实现InitializingBean定义初始化逻辑,实现DisposableBean定义销毁方法 实现BeanPostProcessor接口的后置拦截器放入容器中，可以拦截bean初始化，并可以在被拦截的Bean的初始化前后进行一些处理工作。 spring底层常用的BeanPostProcessor：
* BeanValidationPostProcessor用来实现数据校验 * AutowireAnnotationBeanPostProcessor,@Autowire实现 * ApplicationContextProcessor实现XXXAware的自动注入。 执行时机
doCreateBean -populateBean（）：给bean的各种属性赋值 -initializeBean（）：初始化bean -处理Aware方法 -applyBeanPostProcessorsBeforeInitialization：后置处理器的实例化前拦截 -invokeInitMethods:执行@Bean指定的initMethod -applyBeanPostProcessorsAfterInitialization：后置处理器的实例化后拦截 SpringAOP实现原理 使用步骤
@EnableAspectJAutoProxy 开启基于注解的aop模式 @Aspect：定义切面类，切面类里定义通知 @PointCut 切入点，可以写切入点表达式，指定在哪个方法切入 通知方法 @Before(前置通知) @After(后置通知) @AfterReturning(返回通知) @AfterTrowing(异常通知)@Around(环绕通知) JoinPoint：连接点,是一个类，配合通知使用，用于获取切入的点的信息 SpringAop原理
@EnableAspectJAutoProxy @EnableAspectJAutoProxy 通过@Import(AspectJAutoProxyRegistrar.class)给spring容器中导入了一个AnnotationAwareAspectJAutoProxyCreator。 AnnotationAwareAspectJAutoProxyCreator实现了InstantiationAwareBeanPostProcessor,InstantiationAwareBeanPostProcessor是一个BeanPostProcessor。它可以拦截spring的Bean初始化(Initialization)前后和实例化(Initialization)前后。 AnnotationAwareAspectJAutoProxyCreator的postProcessBeforeInstantiation(bean实例化前)：会通过调用isInfrastructureClass(beanClass)来判断 被拦截的类是否是基础类型的Advice、PointCut、Advisor、AopInfrastructureBean，或者是否是切面（@Aspect），若是则放入adviseBean集合。这里主要是用来处理我们的切面类。 AnnotationAwareAspectJAutoProxyCreator的BeanPostProcessorsAfterInitialization（bean初始化后）： 首先找到被拦截的Bean的匹配的增强器（通知方法），这里有切入点表达式匹配的逻辑 将增强器保存到proxyFactory中， 根据被拦截的Bean是否实现了接口，spring自动决定使用JdkDynamicAopProxy还是ObjenesisCglibAopProxy 最后返回被拦截的Bean的代理对象，注册到spring容器中 代理Bean的目标方法执行过程：CglibAopProxy.intercept(); 保存所有的增强器，并处理转换为一个拦截器链 如果没有拦截器链，就直接执行目标方法 如果有拦截器链，就将目标方法，拦截器链等信息传入并创建CglibMethodInvocation对象，并调用proceed()方法获取返回值。proceed方法内部会依次执行拦截器链。 spring 声明式事务 基本步骤
配置数据源：DataSource 配置事务管理器来控制事务：PlatformTransactionManager @EnableTransactionManagement开启基于注解的事务管理功能 给方法上面标注@Transactional标识当前方法是一个事务方法 声明式事务实现原理
@EnableTransactionManagement利用TransactionManagementConfigurationSelector给spring容器中导入两个组件：AutoProxyRegistrar和ProxyTransactionManagementConfiguration AutoProxyRegistrar给spring容器中注册一个InfrastructureAdvisorAutoProxyCreator，InfrastructureAdvisorAutoProxyCreator实现了InstantiationAwareBeanPostProcessor,InstantiationAwareBeanPostProcessor是一个BeanPostProcessor。它可以拦截spring的Bean初始化(Initialization)前后和实例化(Initialization)前后。利用后置处理器机制在被拦截的bean创建以后包装该bean并返回一个代理对象代理对象执行方法利用拦截器链进行调用（同springAop的原理） ProxyTransactionManagementConfiguration：是一个spring的配置类,它为spring容器注册了一个BeanFactoryTransactionAttributeSourceAdvisor,是一个事务事务增强器。它有两个重要的字段：AnnotationTransactionAttributeSource和TransactionInterceptor。 AnnotationTransactionAttributeSource：用于解析事务注解的相关信息 TransactionInterceptor：事务拦截器，在事务方法执行时，都会调用TransactionInterceptor的invoke->invokeWithinTransaction方法，这里面通过配置的PlatformTransactionManager控制着事务的提交和回滚。 Spring 扩展(钩子) BeanFactoryPostProcessor：beanFactory后置处理器，的拦截时机：所有Bean的定义信息已经加载到容器，但还没有被实例化。可以对beanFactory进行一些操作。 BeanPostProcessor：bean后置处理器，拦截时机：bean创建对象初始化前后进行拦截工作。可以对每一个Bean进行一些操作。 BeanDefinitionRegistryPostProcessor：是BeanFactoryPostProcessor的子接口，拦截时机：所有Bean的定义信息已经加载到容器，但还没有被实例化，可以对每一个Bean的BeanDefinition进行一些操作。 ApplicationListener,自定义ApplicationListener实现类并加入到容器中,可以监听spring容器中发布的事件。spring在创建容器的时候（finishRefresh（）方法）会发布ContextRefreshedEvent事件，关闭的时候（doClose()）会发布ContextClosedEvent事件。也可以通过spring容器的publishEvent发布自己的事件。 事件发布流程：publishEvent方法 获取事件的多播器，getApplicationEventMulticaster()。 调用multicastEvent(applicationEvent, eventType)派发事件。获取到所有的ApplicationListener,即getApplicationListeners()，然后同步或者异步的方式执行监听器的onApplicationEvent。 事件的多播器的初始化中（initApplicationEventMulticaster（）），如果容器中没有配置applicationEventMulticaster，就使用SimpleApplicationEventMulticaster。然后获取所有的监听器，并把它们注册到SimpleApplicationEventMulticaster中。 @EventListener(class={})：在普通的业务逻辑的方法上监听事件特定的事件。原理：EventListenerMethodProcessor是一个SmartInitializingSingleton，当所有的单例bean都初始化完以后， 容器会回调该接口的方法afterSingletonsInstantiated(),该方法里会遍历容器中所有的bean，并判断每一个bean里是否带有@EventListener注解的Method，然后创建ApplicationListenerMethodAdapter存储并包装该Method，最后将ApplicationListenerMethodAdapter添加到spring容器中。 Spring源代码分析 spring核心逻辑AbstractApplicationContext的refresh()方法如下
public void refresh() { synchronized (this.startupShutdownMonitor) { // 刷新前的预准备工作 prepareRefresh(); // 提取bean的配置信息并封装成BeanDefinition实例，然后将其添加到注册中心。注册中心是一个ConcurrentHashMap&lt;String,BeanDefinition>类型，key为Bean的名字，value为BeanDefinition实例。 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //对beanFactory进行一些配置，注册一些BeanPostProcessor和一些特殊的Bean。 prepareBeanFactory(beanFactory);
//留给子类在BeanFactory准备工作完成后处理一些工作。
postProcessBeanFactory(beanFactory);
//调用 BeanFactory的后置处理器。
invokeBeanFactoryPostProcessors(beanFactory);
//注册Bean的后置处理器。
registerBeanPostProcessors(beanFactory);
//国际化相关功能
initMessageSource();
//初始化事件派发器；
initApplicationEventMulticaster();
// 提供给子容器类，供子容器去实例化其他的特殊的Bean
onRefresh();
// 处理容器中已有的ApplicationListener
registerListeners();
//初始化容器中剩余的单实例bean
finishBeanFactoryInitialization(beanFactory);
//最后一步
finishRefresh();
}
}
prepareRefresh() 记录启动时间，设置容器的active和close状态。 initPropertySources():提供给子容器类，子容器类可覆盖该方法进行一些自定义的属性设置。 getEnvironment().validateRequiredProperties()：检验属性的合法性 this.earlyApplicationEvents = new LinkedHashSet() ：保存容器中的一些早期的事件，待事件多播器创建后执行。 obtainFreshBeanFactory() 提取bean的配置信息并封装成BeanDefinition实例，然后将其添加到注册中心。注册中心是一个ConcurrentHashMap&lt;String,BeanDefinition>类型，key为Bean的名字，value为BeanDefinition实例。
refreshBeanFactory：如果当前容器已经有了BeanFactory就销毁原来的BeanFactory。然后创建一个DefaultListableBeanFactory(); 对BeanFactory并进行配置，主要配置是否允许BeanDefinition覆盖，是否允许Bean间的循环引用。 加载BeanDefinition，解析XML文件和配置文件，将其转换为BeanDefinition，然后保存到DefaultListableBeanFactory的beanDefinitionMap字段中。 getBeanFactory() 简单的返回beanFactory，即DefaultListableBeanFactory。 prepareBeanFactory（） 设置BeanFactory的类加载器、设置支持SPEL表达式的解析器。 添加ApplicationContextAwareProcessor用于处理XXXAware接口的回调。 设置忽略一些接口。并注册一些类，这些类可以在bean里直接进行自动装配。 添加ApplicationListenerDetector用于识别并保存ApplicationListener的子类。 postProcessBeanFactory（）： 提供给子容器类，子容器类可以覆盖该方法在BeanFactory准备工作完成后处理一些工作。
invokeBeanFactoryPostProcessors() 执行BeanFactoryPostProcessor类型的监听方法。
BeanFactoryPostProcessor是beanFactory后置处理器，在整个BeanFactory标准初始化完成后进行拦截调用，
BeanDefinitionRegistryPostProcessor继承了BeanFactoryPostProcessor，在beanFactory解析完所有的BeanDefinition后拦截调用。
BeanFactoryPostProcessor来源
通过ApplicationContent的addBeanFactoryPostProcessor()方法手动添加自己的拦截器 系统默认了一些BeanFactoryPostProcessor。例如：ConfigurationClassPostProcessor用来处理@Configuration标注的Spring配置类。 调用顺序
先调用BeanDefinitionRegistryPostProcessor类型的拦截器， 然后再依次调用实现了PriorityOrdered,Ordered接口的BeanFactoryPostProcessor 最后调用普通的BeanFactoryPostProcessor BeanFactoryPostProcessor是beanFactory后置处理器，在整个BeanFactory标准初始化完成后进行拦截调用，
BeanDefinitionRegistryPostProcessor继承了BeanFactoryPostProcessor，在beanFactory解析完所有的BeanDefinition后拦截调用。
BeanFactoryPostProcessor来源
通过ApplicationContent的addBeanFactoryPostProcessor()方法手动添加自己的拦截器 系统默认了一些BeanFactoryPostProcessor。例如：ConfigurationClassPostProcessor用来处理@Configuration标注的Spring配置类。 调用顺序
先调用BeanDefinitionRegistryPostProcessor类型的拦截器， 然后再依次调用实现了PriorityOrdered,Ordered接口的BeanFactoryPostProcessor 最后调用普通的BeanFactoryPostProcessor registerBeanPostProcessors() 注册Bean的后置处理器。
从beanFactory里获取所有BeanPostProcessor类型的Bean的名称。
调用beanFactory的getBean方法并传入每一个BeanPostProcesso类型的Bean名称，从容器中获取该Bean的实例。
第一步向beanFactory注册实现了PriorityOrdered的BeanPostProcessor类型的Bean实例。 第二步向beanFactory注册实现了Ordered的BeanPostProcessor类型的Bean实例。 第三步向beanFactory注册普通的BeanPostProcessor类型的Bean实例。 最后一步向beanFactory重新注册实现了MergedBeanDefinitionPostProcessor的BeanPostProcessor类型的Bean实例 向beanFactory注册BeanPostProcessor的过程就是简单的将实例保存到beanFactory的beanPostProcessors属性中。
initMessageSource() 国际化相关功能
看容器中是否有id为messageSource的，类型是MessageSource的Bean实例。如果有赋值给messageSource，如果没有自己创建一个DelegatingMessageSource。 把创建好的MessageSource注册在容器中，以后获取国际化配置文件的值的时候，可以自动注入MessageSource。 initApplicationEventMulticaster() 初始化事件派发器；
看容中是否有名称为applicationEventMulticaster的，类型是ApplicationEventMulticaster的Bean实例。如果没有就创建一个SimpleApplicationEventMulticaster。 把创建好的ApplicationEventMulticaster添加到BeanFactory中。 onRefresh()： 提供给子容器类，供子容器去实例化其他的特殊的Bean。
registerListeners()： 处理容器中已有的ApplicationListener。
1. 从容器中获得所有的ApplicationListener 2. 将每个监听器添加到事件派发器（ApplicationEventMulticaster）中； 3. 处理之前步骤产生的事件； finishBeanFactoryInitialization()： 初始化容器中剩余的单实例bean：拿到剩余的所有的BeanDefinition，依次调用getBean方法（详看beanFactory.getBean的执行流程）
finishRefresh()： 最后一步。
1. 初始化和生命周期有关的后置处理器；LifecycleProcessor，如果容器中没有指定处理就创建一个DefaultLifecycleProcessor加入到容器。 2. 获取容器中所有的LifecycleProcessor回调onRefresh()方法。 3. 发布容器刷新完成事件ContextRefreshedEvent。 ConfigurationClassPostProcessor处理@Configuration的过程： 先从主从中心取出所有的BeanDefinition。依次判断，若一个BeanDefinition是被@Configuration标注的，spring将其标记为FullMode，否则若一个BeanDefinition没有被@Configuration标注，但有被@Bean标注的方法，spring将其标记为LightMode。筛选出所有候选配置BeanDefinition（FullMode和LightMode） 创建一个ConfigurationClassParser，调用parse方法解析每一个配置类。 解析@PropertySources,将解析结果设置到Environment 利用ComponentScanAnnotationParser，将@ComponentScans标签解析成BeanDefinitionHolder。再迭代解析BeanDefinitionHolder 解析@Import，@ImportResource 将@Bean解析为MethodMetadata，将结果保存到ConfigurationClass中。最终ConfigurationClass会被保存到ConfigurationClassParser的configurationClasses中。 调用ConfigurationClassParser的loadBeanDefinitions方法，加载解析结果到注册中。 从利用ComponentScanAnnotationParser的configurationClasses获取所有的ConfigurationClass，依次调用loadBeanDefinitionsForConfigurationClass方法。 loadBeanDefinitionsForConfigurationClass会将每一个BeanMethod转为ConfigurationClassBeanDefinition，最后将其添加到spring的注册中心。 beanFactory.getBean方法执行的过程 首先将方法传入的beanName进行转换：先去除FactoryBean前缀（&amp;符）如果传递的beanName是别名，则通过别名找到bean的原始名称。 根据名称先从singletonObjects（一个Map类型的容）获取bean实例。如果能获取到就先判断该bean实例是否实现了FactoryBean，如果是FactoryBean类型的bean实例，就通过FactoryBean获取Bean。然后直接返回该bean实例。getBean方法结束。 如果从singletonObjects没有获取到bean实例就开始创建Bean的过程。 首先标记该Bean处于创建状态。 根据Bean的名称找到BeanDefinition。查看该Bean是否有前置依赖的Bean。若有则先创建该Bean前置依赖的Bean。 spring调用AbstractAutowireCapableBeanFactory的createBean方法并传入BeanDefinition开始创建对象。先调用resolveBeforeInstantiation给BeanPostProcessor一个机会去返回一个代理对象去替代目标Bean的实例。 如果BeanPostProcessor没有返回Bean的代理就通过doCreateBean方法创建对象。 首先确定Bean的构造函数，如果有有参构造器，先自动装配有参构造器，默认使用无参数构造器。 选择一个实例化策略去实例化bean。默认使用CglibSubclassingInstantiationStrategy。该策略模式中,首先判断bean是否有方法被覆盖,如果没有则直接通过反射的方式来创建,如果有的话则通过CGLIB来实例化bean对象. 把创建好的bean对象包裹在BeanWrapper里。 调用MergedBeanDefinitionPostProcessor的postProcessMergedBeanDefinition 判断容器是否允许循环依赖，如果允许循环依赖，就创建一个ObjectFactory类并实现ObjectFactory接口的唯一的一个方法getObject（）用于返回Bean。然后将该ObjectFactory添加到singletonFactories中。 调用populateBean为bean实例赋值。在赋值之前执行InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation和postProcessPropertyValues方法。 调用initializeBean初始化bean。如果Bean实现了XXXAware，就先处理对应的Aware方法。然后调用beanProcessor的postProcessBeforeInitialization方法。再以反射的方式调用指定的bean指定的init方法。最后调用beanProcessor的postProcessAfterInitialization方法。 调用registerDisposableBeanIfNecessary，将该bean保存在一个以beanName为key，以包装了bean引用的DisposableBeanAdapter，为value的map中，在spring容器关闭时，遍历这个map来获取需要调用bean来依次调用Bean的destroyMethod指定的方法。 将新创建出来的Bean保存到singletonObjects中 spring原理补充 spring解决循环依赖 以类A，B互相依赖注入为例
根据类A的名称先从singletonObjects获取Bean实例，发现获取不到，就通过doGetBean方法开始创建Bean的流程。 根据A的名称找到对应的BeanDefinition，通过doCreateBean（）方法创建对象，先确定类A的构造函数，然后选择一个实例化策略去实例化类A。 判断容器是否允许循环依赖，如果允许循环依赖，就创建一个ObjectFactory类并实现ObjectFactory接口的唯一的一个方法getObject（）用于返回类A。然后将该ObjectFactory添加到singletonFactories中。 调用populateBean（）为类A进行属性赋值，发现需要依赖类B，此时类B尚未创建，启动创建类B的流程。 根据类B的名称先从singletonObjects获取Bean实例，发现获取不到，就开始通过doGetBean方法开始创建Bean的流程 找到类B对应的BeanDefinition，确认B的构造函数，然后实例化B。 判断容器是否允许循环依赖，创建一个ObjectFactory并实现getObject（）方法，用于返回类B，并添加到singletonFactories中。 调用populateBean（）为类B进行属性赋值，发现需要依赖类A，调用getSingleton方法获取A：A现在已存在于singletonFactories中，getSingleton将A从singletonFactories方法中移除并放入earlySingletonObjects中。 调用getSingleton（）方法获取B：getSingleton将A从singletonFactories方法中移除并放入earlySingletonObjects中。 调用initializeBean初始化bean，最后将新创建出来的类B保存到singletonObjects中 调用getSingleton（）方法获取A，这时A已在earlySingletonObjects中了，就直接返回A 调用initializeBean初始化bean，最后将新创建出来的类B保存到singletonObjects中。 @Autowire 实现原理 上面介绍beanFactory.getBean方法执行的过程中提到：populateBean为bean实例赋值。在赋值之前执行InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation和postProcessPropertyValues方法。@Autowire由AutowiredAnnotationBeanPostProcessor完成，它实现了InstantiationAwareBeanPostProcessor。 AutowiredAnnotationBeanPostProcessor执行过程：
postProcessAfterInstantiation方法执行，直接return null。 postProcessPropertyValues方法执行，主要逻辑在此处理。待补充。。。。。</content></entry><entry><title>Linux-unix编程手册-文件</title><url>/post/linux-unix-file/</url><categories><category>Linux</category><category>IO</category></categories><tags><tag>Linux</tag><tag>File</tag><tag>IO</tag></tags><content type="html"> Linux-unix编程手册-文件
1. 概述 所有执行 I/O 操作的系统调用都以文件描述符，一个非负整数（通常是小整数），来指代打开的文件。文件描述符用以表示所有类型的已打开文件，包括管道（pipe）、FIFO、socket、终端、设备和普通文件。针对每个进程，文件描述符都自成一套</content></entry><entry><title>软件架构设计读书笔记</title><url>/post/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url><categories><category>架构原理</category><category>中间件</category></categories><tags><tag>软件架构</tag><tag>底层原理</tag><tag>中间件</tag></tags><content type="html"> 根据软件架构设计：大型网站技术架构与业务架构融合之道一书所作的读书笔记
1. 架构分层 第一层：基础架构
指云平台、操作系统、网络、存储、数据库等
第二层：中间件与大数据平台
中间件：如分布式服务中间件，消息中间件、数据库中间件、缓存中间件、监控中间件、工作流或者规则引擎等 大数据架构：Hadoop生态体系，hive,Spark,Storm,Flink等 第三层：业务系统架构
通用软件系统，常用的办公软件、浏览器、播放器等 离线业务系统，如各种基于大数据的BI分析，数据挖掘，报表与可视化。 大型在线业务系统，如搜索，推荐，即时通信，电商，游戏，广告，企业ERP等 架构的道与术：解决一系列问题的方法论即为道，具体到某种语言技术，某个中间件的使用即为术。道是知行合一中的知，是理论，套路，方法论。术是行，是实践操作，用于解决一个个实际问题
2. 计算机功底 WAL即 Write Ahead Log，WAL的主要意思是说在将元数据的变更操作写入磁盘之前，先预先写入到一个log文件中，增加IO速度。磁盘顺序读写速度可媲美内存速度。 Checksum：总和检验码，校验和。在数据处理和数据通信领域中，用于校验目的的一组数据项的和。这些数据项可以是数字或在计算检验总和过程中看作数字的其它字符串，可以保证数据的完整性和正确性 TCP如何把不可靠变成可靠，mvcc解决并发多版本一致性等 计算机思维举例，可以借鉴
3. 操作系统 缓冲IO与直接IO
缓冲IO是C语言提供的函数库，以f打头，如fopen,fclose,fseek,fread,fwrite，直接IO是Linux系统的API,如open,read，write等
对于缓冲IO,读写都是三次拷贝
读：磁盘→内核缓冲区→用户缓冲区→应用程序内存
写：应用程序内存→用户缓冲区→内核缓冲区→磁盘
对于直接IO,读写都是两次数据拷贝
读：磁盘→内核缓冲区→应用程序内存
写：应用程序内存→内核缓冲区→磁盘
所以，直接IO没有用户缓冲区
应用层序内存：通常是代码用malloc/free、new/delete等分配出来的内存
用户缓冲区：C语言的FILE结构体中的buffer
内核缓冲区：Linux操作系统的Page Cahce。为了加快磁盘IO，Linux会把磁盘上的数据以page为单位加载到内存中，page是一个逻辑概念，一般一个page为4K
缓冲IO与直接IO有几点需要贴别说明：
fflush和fsync的区别。fflush是缓冲IO的一个api，作用是把数据从用户缓存刷到内核缓存，fsync是把数据从内核缓存刷到磁盘，所以无论是直接IO还是缓冲IO在写数据之后不调用fsync,此时断电的话会造成数据丢失。 对于直接IO也有read/write和pread/pwrite两组api，后者在多线程读写同一个文件时效率更高 内存映射与零拷贝
相较于直接IO,内存映射文件更进一步，其实现是用应用程序的逻辑内存与Linux操作系统的内核缓存区映射，相当于与内核共用内存，数据拷贝只需一次即
磁盘$\Leftrightarrow$内核缓冲区</content></entry><entry><title>java设计模式</title><url>/post/java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url><categories><category>Design Pattern</category></categories><tags><tag>design pattern</tag><tag>java</tag></tags><content type="html"> java 设计模式学习
1 七大原则 单一职责原则：一个类只负责一个功能领域中的相应职责，或者可以定义为：就一个类而言，应该只有一个引起它变化的原因
开闭原则：一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。 为了满足开闭原则，需要对系统进行抽象化设计，抽象化是开闭原则的关键，可以为系统定义一个相对稳定的抽象层，而将不同的实现行为移至具体的实现层中完成。
里氏代换原则：所有引用基类（父类）的地方必须能透明地使用其子类的对象。里氏代换原则是实现开闭原则的重要方式之一，由于使用基类对象的地方都可以使用子类对象，因此在程序中尽量使用基类类型来对对象进行定义，而在运行时再确定其子类类型，用子类对象来替换父类对象
在使用里氏代换原则时需要注意如下几个问题：
子类的所有方法必须在父类中声明，或子类必须实现父类中声明的所有方法。根据里氏代换原 则，为了保证系统的扩展性，在程序中通常使用父类来进行定义，如果一个方法只存在子类中，在父类中不提供相应的声明，则无法在以父类定义的对象中使用该方法。 我们在运用里氏代换原则时，尽量把父类设计为抽象类或者接口，让子类继承父类或实现父接口，并实现在父类中声明的方法，运行时，子类实例替换父类实例，我们可以很方便地扩展系统的功能，同时无须修改原有子类的代码，增加新的功能可以通过增加一个新的子类来实现。里氏代换原则是开闭原则的具体实现手段之一。 依赖倒转原则：抽象不应该依赖于细节，细节应当 依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。依赖倒转原则要求我们在程序代码中传递参数时或在关联关系中，尽量引用层次高的抽象层类，即使
用接口和抽象类进行变量类型声明、参数类型声明、方法返回类型声明，以及数据类型的转换等，而不要用具体类来做这些事情。
在实现依赖倒转原则时，我们需要针对抽象层编程，而将具体类的对象通过依赖注入(DependencyInjection, DI)的方式注入到其他对象中，依赖注入是指当一个对象要与其他对象 发生依赖关系时，通过抽象来注入所依赖的对象。常用的注入方式有三种，分别是：构造注入，设值注入（Setter注入）和接口注入
接口隔离原则：使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。
合成复用原则：尽量使用对象组合，而不是继承来达到复用的目的。合成复用原则就是在一个新的对象里通过关联关系（包括组合关系和聚合关系）来使用一些已有的对象，使之成为新对象的一部分；新对象通过委派调用已有对象的方法达到复用功能的目的。
两个类之间是“Has-A”的关系应使用组合或聚合，如果是“Is-A”关系可使用继 承。”Is-A”是严格的分类学意义上的定义，意思是一个类是另一个类的”一种”；而”Has-A”则不同，它表示某一个角色具有某一项责任。
迪米特法则：一个软件实体应当尽可能少地与其他实体发生相互作用（最少知道原则）。
不要和“陌生人”说话、只与你的直接朋友通信等，在迪米特法 则中，对于一个对象，其朋友包括以下几类：
当前对象本身(this)；
以参数形式传入到当前对象方法中的对象；
当前对象的成员对象；
如果当前对象的成员对象是一个集合，那么集合中的元素也都是朋友；
当前对象所创建的对象。
任何一个对象，如果满足上面的条件之一，就是当前对象的“朋友”，否则就是“陌生人”。
迪米特法则要求我们在设计系统时，应该尽量减少对象之间的交互，如果两个对象之间不必彼此直接 通信，那么这两个对象就不应该发生任何直接的相互作用，如果其中的一个对象需要调用另一个对象 的某一个方法的话，可以通过第三者转发这个调用。简言之，就是通过引入一个合理的第三者来降低 现有对象之间的耦合度。</content></entry><entry><title>MySql原理学习记录</title><url>/post/mysql%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url><categories><category>MySql</category><category>MySql Basis</category></categories><tags><tag>MySql</tag><tag>原理知识</tag></tags><content type="html"> 根据《MySql是怎样运行的一书》整理记录知识点
1 mysql编码 mysql支持41种字符集包括常用的utf-8、ASCII、GB2312等
utf是unicode编码的一种实现方式，utf8使用1～4个字节编码一个字符，utf16使用2个或4个字节编码一个字符，utf32使用4个字节编码一个字符 mysql中的utf8(utf8mb3)是标准utf8的阉割版，采用1~3个字节存储字符，涵盖常用字符的表示范围 mysql中utf8mb4 才是正宗的 utf8 字符集，使用1～4个字节表示字符，可以表示不常用字符，如表情符号等 SHOW CHARACTER SET;查看字符集 比较规则，比较规则和字符集是绑定的
每种字符集都有几种对应的比较规则，如忽略大小写等，比较规则用于比较字符或者排序（order by）
SHOW COLLATION [LIKE 匹配的模式]; SHOW COLLATION LIKE &lsquo;utf8_%&rsquo;;查看utf8编码的比较规则 字符集和比较规则的应用
MySQL 有4个级别的字符集和比较规则，数据字符集采用就近原则，如果没有就从上级继承，从上到下级别分别是：①服务器级别②数据库③表级别④列级别
以服务器级别为例：
SHOW VARIABLES in(&lsquo;character_set_server&rsquo;,&lsquo;collation_server&rsquo;);查看服务器级别的字符集和比较规则
数据库乱码原因及原理
结论：编码和解码使用的字符集不一致的后果
mysql字符编系统变量
过程：客户端发往服务器的请求本质上就是一个字符串，服务器向客户端返回的结果本质上也是一个字符。过程分为三步：①客户端发送往服务器的字符串经过编码成二进制数据才能发送，比如客户端的默认编码是gbk，发送的数据就是以gbk字符集编码的二进制数据，数据到了mysql端，mysql就会使用character_set_client字符集来对数据进行解码，如果字符集不是gbk这一步就会乱码，导致sql语句无法正确解析②第一步成功编解码之后，服务器会把字符按照character_set_connection字符集进行编码，然后拿着编码后的数据去数据库表中对应列进行匹配（数据库中字段值是按照character_set_connection字符集编码后存储的）③最后将查出来的数据用character_set_results编码之后发给客户端，客户端再用客户端默认字符集解码得到最终展示数据
总结：服务器认为客户端发送过来的请求是用 character_set_client 编码的。假设你的客户端采用的字符集和 character_set_client 不一样的话，这就会出现意想不到的情况。比如我的客户端使用的是 utf8 字符集，如果把系统变量 character_set_client 的值设置为 ascii 的话，服务器可能无法理解我们发送的请求，更别谈处理这个请求了。服务器将把得到的结果集使用 character_set_results 编码后发送给客户端。假设你的客户端采用的字符集和 character_set_results 不一样的话，这就可能会出现客户端无法解码结果集的情况，结果就是在你的屏幕上出现乱码。客户端默认字符集==character_set_client==character_set_results才不会乱码
设置成统一字符集
SET NAMES 字符集名;
等价于：
SET character_set_client = 字符集名;
SET character_set_connection = 字符集名;
SET character_set_results = 字符集名;
等价于：
在my.cnf配置文件中
[client]
default-character-set=字符集名
2 存储引擎InnoDB 我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为 行格式 或者 记录格式 设计 InnoDB 存储引擎的大叔们到现在为止设计了4种不同类型的 行格式 ，分别是 Compact 、 Redundant Dynamic 和 Compressed 行格式
行格式用法：
CREATE TABLE 表名 (列的信息) ROW_FORMAT=Compact
ALTER TABLE 表名 ROW_FORMAT=Compact
compact行格式
变长字段长度列表
变长字段，使用变长数据类型（varchar(n)，blob,text等）或者变长字符集（如utf8是1~3个字符）存储的字段。在 Compact 行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表，各变长字段数据占用的字节数按照列的顺序逆序存放，
NULL值列表
我们知道表中的某些列可能存储 NULL 值，如果把这些 NULL 值都放到 记录的真实数据 中存储会很占地方，所以 Compact 行格式把这些值为 NULL 的列统一管理起来，存储到 NULL 值列表中，它的处理过程是这样的：
首先统计表中允许存储 NULL 的列有哪些。我们前边说过，主键列、被 NOT NULL 修饰的列都是不可以存储 NULL 值的，所以在统计的时候不会把这些列算进去。比方说表 record_format_demo 的3个列 c1 、 c3 、 c4 都是允许存储 NULL 值的，而 c2 列是被NOT NULL 修饰，不允许存储 NULL 值。
如果表中没有允许存储 NULL 的列，则 NULL值列表 也不存在了，否则将每个允许存储 NULL 的列对应一个二进制位，二进制位按照列的顺序逆序排列，二进制位表示的意义如下：二进制位的值为 1 时，代表该列的值为 NULL 。二进制位的值为 0 时，代表该列的值不为 NULL
MySQL 规定 NULL值列表 必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补 0 。 表 record_format_demo 只有3个值允许为 NULL 的列，对应3个二进制位，不足一个字节，所以在字节的高位补 0
记录头信息
除了 变长字段长度列表 、 NULL值列表 之外，还有一个用于描述记录的 记录头信息 ，它是由固定的 5 个字节组成。 5 个字节也就是 40 个二进制位，不同的位代表不同的意思
存储行
/*MySQL 对一条记录占用的最大存储空间是有限制的，除了 BLOB 或者 TEXT 类型的列之 外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节 存储一个 VARCHAR(M) 类型的列，其实需要占用3部分存储空间： ①真实数据 ②真实数据占用字节的长度 ③NULL 值标识，如果该列有 NOT NULL 属性则可以没有这部分存储空间 如果该 VARCHAR 类型的列没有 NOT NULL 属性，那最多只能存储 65532 个字节的数据，因为真实数据的长度可能 占用2个字节， NULL 值标识需要占用1个字节： */ CREATE TABLE varchar_size_demo( c1 VARCHAR(65532) )CHARSET=ASCII ROW_FORMAT=COMPACT; /*编码如果是gbk,最多只能32766 （也就是：65532/2），因为gbk一个字符占两个字节 utf8 字符集表示一个字符最多需要 3 个字节，那在该字符集下， M 的最大取值就是 21844 ，就是说最多能存 储 21844 （也就是：65532/3）个字符*/ /*这些都是表中只有一个字段的情况下，如果是多个字段，则所有字段（（不包括隐藏 列和记录头信息））加起来不能超过65535*/ CREATE TABLE varchar_size_demo( c VARCHAR(32766) ) CHARSET=gbk ROW_FORMAT=COMPACT; 3 访问方法 const: 通过主键或者唯一二级索引列与常数的等值比较来定位一条记录,如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较
ref：通过某个普通的二级索引列与常数进行等值比较（普通二级索引与唯一二级索引对应，普通二级索引等值比较可能查出多个列）
二级索引列值为 NULL 的情况
不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含 NULL 值的数量并不限制，所以我们采用key IS NULL 这种形式的搜索条件最多只能使用 ref 的访问方法，而不是 const 的访问方法。
对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用 ref的访问方法，比方说下边这几个查询：
SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo;;
SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo; AND key_part2 = &rsquo;legendary&rsquo;;
SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo; AND key_part2 = &rsquo;legendary&rsquo;
AND key_part3 = &lsquo;penta kill&rsquo;。但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为 ref 了，比方说这样： SELECT * FROM single_table WHERE key_part1 = &lsquo;god like&rsquo; AND key_part2 > &rsquo;legendary&rsquo;;
ref_or_null：有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为 NULL 的记录也找出来，就像下边这个查询：
SELECT * FROM single_demo WHERE key1 = &lsquo;abc&rsquo; OR key1 IS NULL;
range：利用索引进行范围匹配
index：遍历二级索引记录的执行方式
SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &lsquo;abc&rsquo;;
key_part1, key_part2, key_part3三个要查询的字段是联合索引，查询条件key_part2 不是最左索引，此时需要直接通过遍历 idx_key_part 索引的叶子节点的记录来比较 key_part2 = &lsquo;abc&rsquo; 这个条件是否成立，把匹配成功的二级索引记录的 key_part1 , key_part2 , key_part3 列的值直接加到结果集中就行了，不需要回表
当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是 index ，比如这样：
mysql> EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = &lsquo;a&rsquo;;
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;-
&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| id | select_type | table | partitions | type | possible_keys | key | key
_len | ref | rows | filtered | Extra |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;-
&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| 1 | SIMPLE | s1 | NULL | index | NULL | idx_key_part | 909
| NULL | 9688 | 10.00 | Using where; Using index |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;-
&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
1 row in set, 1 warning (0.00 sec)
上述查询中的搜索列表中只有 key_part2 一个列，而且搜索条件中也只有 key_part3 一个列，这两个列又恰
好包含在 idx_key_part 这个索引中，可是搜索条件 key_part3 不能直接使用该索引进行 ref 或者 range 方
式的访问，只能扫描整个 idx_key_part 索引的记录，所以查询计划的 type 列的值就是 index 。
小贴士：
再一次强调，对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，
而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，
也就是扫描聚簇索引的代价更低一些。
all：全表扫描
eq_ref：在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是
eq_ref
index_merge：一般情况下对于某个表的查询只能使用到一个索引，在某些场景下可以使用 Intersection 、 Union 、 Sort-Union 这三种索引合并的方式来执行查询，如
mysql> EXPLAIN SELECT * FROM s1 WHERE key1 = &lsquo;a&rsquo; OR key3 = &lsquo;a&rsquo;;
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;
&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
&mdash;&ndash;+
| id | select_type | table | partitions | type | possible_keys | key
| key_len | ref | rows | filtered | Extra |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;
&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
&mdash;&ndash;+
| 1 | SIMPLE | s1 | NULL | index_merge | idx_key1,idx_key3 | idx_key
1,idx_key3 | 303,303 | NULL | 14 | 100.00 | Using union(idx_key1,idx_key3); Using
where |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;
&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
&mdash;&ndash;+
1 row in set, 1 warning (0.01 sec)
unique_subquery：类似于两表连接中被驱动表的 eq_ref 访问方法， unique_subquery 是针对在一些包含 IN 子查询的查询语句中，如果查询优化器决定将 IN 子查询转换为 EXISTS 子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的 type 列的值就是 unique_subquery ，比如下边的这个查询语句：
mysql> EXPLAIN SELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 = s2.
key1) OR key3 = &lsquo;a&rsquo;;
+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;
+&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;-+
| id | select_type | table | partitions | type | possible_keys
| key | key_len | ref | rows | filtered | Extra |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;
+&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;-+
| 1 | PRIMARY | s1 | NULL | ALL | idx_key3
| NULL | NULL | NULL | 9688 | 100.00 | Using where |
| 2 | DEPENDENT SUBQUERY | s2 | NULL | unique_subquery | PRIMARY,idx_key1
| PRIMARY | 4 | func | 1 | 10.00 | Using where |
+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;
+&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;+&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;-+
system：当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是 system
4 redo log 概念：在系统奔溃重启时需要按照上述内容所记录的步骤重新更新数据页，所以上述内容也被称之为 重做日志 ，英文名为 redo log 。
描述：想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。所以我们其实没有必要在每次事务提交时就把该事务在内存中修改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好，这样我们在事务提交时，把上述内容刷新到磁盘中，即使之后系统崩溃了，重启之后只要按照上述内容所记录的步骤重新更新一下数据页，那么该事务对数据库中所做的修改又可以被恢复出来。
结构：
type ：该条 redo 日志的类型。
在 MySQL 5.7.21 这个版本中，设计 InnoDB 的大叔一共为 redo 日志设计了53种不同的类型，稍后会详细介
绍不同类型的 redo 日志。
space ID ：表空间ID。
page number ：页号。
data ：该条 redo 日志的具体内容。
刷盘时机：redo日志不会直接写入磁盘，而是写入内存的log buffer中，将log buffer刷入磁盘的时机有以下几种情况：①log buffer空间不足②事务提交③后台线程大概每秒会做一次刷盘④正常关闭服务器时⑤做checkpoint时
Mini-Transaction：底层页面中的一次原子访问的过程称之为一个 Mini-Transaction ，简称 mtr ，比如上边
所说的修改一次 Max Row ID 的值算是一个 Mini-Transaction ，向某个索引对应的 B+ 树中插入一条记录的过程也算是一个 Mini-Transaction 。通过上边的叙述我们也知道，一个所谓的 mtr 可以包含一组 redo 日志，在进行奔溃恢复时这一组 redo 日志作为一个不可分割的整体。一个事务可以包含若干条语句，每一条语句其实是由若干个 mtr 组成，每一个 mtr 又可以包含若干条 redo 日志
lsn和flushed_to_disk_lsn：日志序列号 ，简称 lsn有新的 redo 日志写入到 log buffer 时，首先 lsn 的值会增长。刷新到磁盘中的 redo 日志量的全局变量，称之为flushed_to_disk_lsn 。系统第一次启动时，该变量的值和初始的 lsn 值是相同的，都是 8704 。随着系统的运行， redo 日志被不断写入 log buffer ，但是并不会立即刷新到磁盘， lsn 的值就和 flushed_to_disk_lsn 的值拉开了差距，如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了
checkpoint：全局变量 checkpoint_lsn 来代表当前系统中可以被覆盖的 redo 日志总量是多少，比方说现在 页a 被刷新到了磁盘， mtr_1 生成的 redo 日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn 的操作，我们把这个过程称之为做一次 checkpoint 。计算一下当前系统中可以被覆盖的 redo 日志对应的 lsn 值最大是多少，redo 日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的 oldest_modification 值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的 oldest_modification 赋值给 checkpoint_lsn
崩溃恢复：系统崩溃后重启时根据 redo 日志中的记录就可以将页面恢复到系统奔溃前的状态。
崩溃恢复起点：checkpoint_lsn 之前的 redo 日志都可以被覆盖，也就是说这些 redo 日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于 checkpoint_lsn 之后的 redo 日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从 checkpoint_lsn 开始读取 redo 日志来恢复页面。 崩溃恢复的终点：写 redo 日志的时候都是顺序写的，写满了一个block之后会再往下一个block，普通block的 log block header 部分有一个称之为 LOG_BLOCK_HDR_DATA_LEN 的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为 512 。如果该属性的值不为 512，那该block就是最后一个block，也就是redo日志的终点。 怎么恢复：按照 redo 日志的顺序依次扫描checkpoint_lsn 之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来， 5 undo log 定义：在事务中为了回滚而记录的这些东东称之为撤销日志，英文名为 undo log 6 隔离级别 事务并发遇到的问题：
脏写（ Dirty Write ）：如果一个事务修改了另一个未提交事务修改过的数据，那就意味着发生了 脏写 脏读（ Dirty Read ）：如果一个事务读到了另一个未提交事务修改过的数据，那就意味着发生了 脏读 不可重复读（Non-Repeatable Read）：如果一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值，那就意味着发生了 不可重复读 幻读（Phantom）：如果一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来，那就意味着发生了 幻读 四种隔离级别：
READ UNCOMMITTED ：未提交读。
READ COMMITTED ：已提交读。
REPEATABLE READ ：可重复读。
SERIALIZABLE ：可串行化。
READ UNCOMMITTED 隔离级别下，可能发生 脏读 、 不可重复读 和 幻读 问题。
READ COMMITTED 隔离级别下，可能发生 不可重复读 和 幻读 问题，但是不可以发生 脏读 问题。
REPEATABLE READ 隔离级别下，可能发生 幻读 问题，但是不可以发生 脏读 和 不可重复读 的问题。
SERIALIZABLE 隔离级别下，各种问题都不可以发生。
MySQL在REPEATABLE READ隔离级别下，是可以禁止幻读问题的发生的
MVCC原理
版本链描述：每次对记录进行改动，都会记录一条 undo日志 ，每条 undo日志 也都有一个 roll_pointer 属性（ INSERT 操作对应的 undo日志 没有该属性，因为该记录并没有更早的版本），可以将这些 undo日志 都连来，串成一个链表，对该记录每次更新后，都会将旧值放到一条 undo日志 中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，我们把这个链表称之为 版本链 ，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的 事务id
ReadView：对于使用 READ UNCOMMITTED 隔离级别的事务来说，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了；对于使用 SERIALIZABLE 隔离级别的事务来说，设计 InnoDB 的大叔规定使用加锁的方式来访问记录（加锁是啥我们后续文章中说哈）；对于使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是：需要判断一下版本链中的哪个版本是当前事务可见的。为此，设计 InnoDB 的大叔提出了一个 ReadView 的概念，这个 ReadView 中主要包含4个比较重要的内容：
m_ids ：表示在生成 ReadView 时当前系统中活跃的读写事务的 事务id 列表。
min_trx_id ：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的 事务id ，也就是 m_ids 中的最
小值。
max_trx_id ：表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。
小贴士：
注意max_trx_id并不是m_ids中的最大值，事务id是递增分配的。比方说现在有id为1，2，3这三
个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，m_ids就包括1和2，mi
n_trx_id的值就是1，max_trx_id的值就是4。
creator_trx_id ：表示生成该 ReadView 的事务的 事务id 。
有了这个 ReadView ，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：
如果被访问版本的 trx_id 属性值与 ReadView 中的 creator_trx_id 值相同，意味着当前事务在访问它自己
修改过的记录，所以该版本可以被当前事务访问。如果被访问版本的 trx_id 属性值小于 ReadView 中的 min_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。
如果被访问版本的 trx_id 属性值大于 ReadView 中的 max_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。
如果被访问版本的 trx_id 属性值在 ReadView 的 min_trx_id 和 max_trx_id 之间，那就需要判断一下
trx_id 属性值是不是在 m_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该
版本不可以被访问；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。
**在 MySQL 中， READ COMMITTED 和 REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同，READ COMMITTED —— 每次读取数据前都生成一个****ReadView，REPEATABLE READ —— 在第一次读取数据时生成一个****ReadView
小结：从上边的描述中我们可以看出来，所谓的 MVCC （Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行普通的 SEELCT 操作时访问记录的版本链的过程，这样子可以使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。 READ COMMITTD 、REPEATABLE READ 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了。
7 锁 结构属性：
trx信息 ：代表这个锁结构是哪个事务生成的。
is_waiting ：代表当前事务是否在等待。
type：Next-Key Locks/插入意向锁/&hellip;
加解锁过程：
事务 T1 要改动某条记录时，就生成了一个 锁结构 与该记录关联，因为之前没有别的事务为这条记录加锁，所以 is_waiting 属性就是 false ，我们把这个场景就称之为获取锁成功，或者加锁成功，然后就可以继续执行操作了。
在事务 T1 提交之前，另一个事务 T2 也想对该记录做改动，那么先去看看有没有 锁结构 与这条记录关联，发现有一个 锁结构 与之关联后，然后也生成了一个 锁结构 与这条记录关联，不过 锁结构 的is_waiting 属性值为 true ，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，或者没有成功的获取到锁
在事务 T1 提交之后，就会把该事务生成的 锁结构 释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务 T2 还在等待获取锁，所以把事务 T2 对应的锁结构的 is_waiting 属性设置为 false ，然后把该事务对应的线程唤醒，让它继续执行，此时事务 T2 就算获取到锁了
读操作利用多版本并发控制（ MVCC ），写操作进行加锁，除非有特殊场景需要读写都加锁，比如一些业务场景不允许读取记录的旧版本，而是每次都必须去读取记录的最新版本，比方在银行存款的事务中，你需要先把账户的余额读出来，然后将其加上本次存款的数额，最后再写到数据库中。在将账户余额读取出来后，就不想让别的事务再访问该余额，直到本次存款事务执行完成，其他事务才可以访问账户的余额。这样在读取记录的时候也就需要对其进行 加锁 操作，这样也就意味着 读操作和 写 操作也像 写-写 操作那样排队执行
一致性读和锁定读
事务利用 MVCC 进行的读取操作称之为 一致性读 ，或者 一致性无锁读 ，有的地方也称之为 快照读 。所有普通的 SELECT 语句（ plain SELECT ）在 READ COMMITTED 、 REPEATABLE READ 隔离级别下都算是 一致性读，一致性读 并不会对表中的任何记录做 加锁 操作，其他事务可以自由的对表中的记录做改动。
锁定读要求在读-读 情况不受影响，又要使 写-写 、 读-写 或 写-读 情况中的操作相互阻塞，mysql中用两种锁独占锁共享锁和独占锁来实现：
共享锁 ，英文名： Shared Locks ，简称 S锁 。在事务要读取一条记录时，需要先获取该记录的 S锁 独占锁 ，也常称 排他锁 ，英文名： Exclusive Locks ，简称 X锁 。在事务要改动一条记录时，需要先获取该记录的 X锁 假如事务 T1 首先获取了一条记录的 S锁 之后，事务 T2 接着也要访问这条记录：如果事务 T2 想要再获取一个记录的 S锁 ，那么事务 T2 也会获得该锁，也就意味着事务 T1 和 T2 在该记录上同时持有 S锁 。如果事务 T2 想要再获取一个记录的 X锁 ，那么此操作会被阻塞，直到事务 T1 提交之后将 S锁 释放掉。如果事务 T1 首先获取了一条记录的 X锁 之后，那么不管事务 T2 接着想获取该记录的 S锁 还是 X锁 都会被阻塞，直到事务 T1 提交。 总结：S锁 和 S锁 是兼容的， S锁 和 X锁 是不兼容的， X锁 和 X锁 也是不兼容的 加锁语句：S锁 SELECT &hellip; LOCK IN SHARE MODE; X锁 SELECT &hellip; FOR UPDATE; 写操作 DELETE ：对一条记录做 DELETE 操作的过程其实是先在 B+ 树中定位到这条记录的位置，然后获取一下这条记录的 X 锁 ，然后再执行 delete mark 操作。我们也可以把这个定位待删除记录在 B+ 树中位置的过程看成是一个获取 X锁 的 锁定读 。
UPDATE ：在对一条记录做 UPDATE 操作时分为三种情况：如果未修改该记录的键值并且被更新的列占用的存储空间在修改前后未发生变化，则先在 B+ 树中定位到这条记录的位置，然后再获取一下记录的 X锁 ，最后在原记录的位置进行修改操作。其实我们也可以把这个定位待修改记录在 B+ 树中位置的过程看成是一个获取 X锁 的 锁定读 。如果未修改该记录的键值并且至少有一个被更新的列占用的存储空间在修改前后发生变化，则先在B+ 树中定位到这条记录的位置，然后获取一下记录的 X锁 ，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插入一条新记录。这个定位待修改记录在 B+ 树中位置的过程看成是一个获取 X 锁 的 锁定读 ，新插入的记录由 INSERT 操作提供的 隐式锁 进行保护。如果修改了该记录的键值，则相当于在原录上做 DELETE 操作之后再来一次 INSERT 操作，加锁操作就需要按照 DELETE 和 INSERT 的规则进行了。
INSERT ：一般情况下，新插入一条记录的操作并不加锁，设计 InnoDB 的大叔通过一种称之为 隐式锁 的东东来保护这条新插入的记录在本事务提交前不被别的事务访问，更多细节我们后边看哈～
多粒度锁
我们前边提到的 锁 都是针对记录的，也可以被称之为 行级锁 或者 行锁 ，对一条记录加锁影响的也只是这条记录而已，我们就说这个锁的粒度比较细；其实一个事务也可以在 表 级别进行加锁，自然就被称之为 表级锁 或 者 表锁 ，对一个表加锁影响整个表中的记录，我们就说这个锁的粒度比较粗。给表加的锁也可以分为 共享锁 S锁 ）和 独占锁 （ X锁 ）
给表加 S锁 ：如果一个事务给表加了 S锁 ，那么别的事务可以继续获得该表的 S锁，别的事务可以继续获得该表中的某些记录的 S锁，别的事务不可以继续获得该表的 X锁，别的事务不可以继续获得该表中的某些记录的 X锁
给表加 X锁 ：如果一个事务给表加了 X锁 （意味着该事务要独占这个表），那么：别的事务不可以继续获得该表的 S锁，别的事务不可以继续获得该表中的某些记录的 S锁别的事务不可以继续获得该表的 X锁，别的事务不可以继续获得该表中的某些记录的 X锁
我们在对教学楼整体上锁（ 表锁 ）时，怎么知道教学楼中有没有教室已经被上锁（ 行锁 ）了呢？依次检查每一间教室门口有没有上锁？那这效率也太慢了吧！遍历是不可能遍历的，这辈子也不可能遍历的，于是乎设计
InnoDB 的大叔们提出了一种称之为 意向锁
意向共享锁，英文名： Intention Shared Lock ，简称 IS锁 。当事务准备在某条记录上加 S锁 时，需要先
在表级别加一个 IS锁 。
意向独占锁，英文名： Intention Exclusive Lock ，简称 IX锁 。当事务准备在某条记录上加 X锁 时，需要先在表级别加一个 IX锁 。
当加表锁时，如果要加S锁，要查看表级有无IX锁，如果要加X锁，要看有无表级IS锁和IX锁
IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否
被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是兼容的，IX锁和IX锁是兼容的
InnoDB存储引擎中的锁
表级锁：在InnoDB 存储引擎提供的表级 S锁 或者 X锁 是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。不过我们还是可以手动获取一下的，比方说在系统变量autocommit=0，innodb_table_locks =1 时，手动获取 InnoDB 存储引擎提供的表 t 的 S锁 或者 X锁 可以这么写：LOCK TABLES t READ ： InnoDB 存储引擎会对表 t 加表级别的 S锁 。LOCK TABLES t WRITE ： InnoDB 存储引擎会对表 t 加表级别的 X锁 。
行级锁：行锁 ，也称为 记录锁 ，顾名思义就是在记录上加的锁。不过设计 InnoDB 的大叔很有才，一个 行锁 玩出了各种花样，也就是把 行锁 分成了各种类型。换句话说即使对同一条记录加 行锁 ，如果类型不同，起到的功效也是不同的
Record Locks：行记录锁，分为S锁和X锁
Gap Locks：间隙锁，MySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方案解决，也可以采用 加锁 方案解决。但是在使用 加锁 方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上Record Locks，于是就有了间隙锁，顾名思义间隙锁是记录的间隙加锁，使其无法在某个范围内插入数据，从而解决幻读。
原文：A gap lock is a lock on a gap between index records, or a lock on the gap before the first or after the last index record.
译文：间隙锁是索引记录之间间隙上的锁，或者是第一个索引记录之前或最后一个索引记录之后间隙上的锁
Next-Key Locks：其实就是Record Locks+Gap Locks，即给当前行加锁，又给当前行的前后间隙加锁。
比如在查询select * from test_gaplock where id>1 and id&lt;7 for update中一共有id=3和id=5两条记录，其中id为主键或者唯一列的情况下，会锁定范围id(1,3)，（3，5），（5，7）之间不允许插入数据，并且id=3和id=5两条记录也会被锁定。如果id不为主键或者唯一列，则有可能锁定整张表，在本次事务结束前别的事物无法插入数据。
另外在id列为主键或者唯一列的情况下，select * from test_gaplock where id=1 for update 不会产生间隙锁，因为已经明确查询的范围只会有唯一一条数据，不需要间隙锁。
Insert Intention Locks：插入意向锁，事务在对语句加锁之前要先判断语句上有没有锁，于是就产生了插入意向锁。在1锁结构中有tyep字段来表明当前事务的锁是什么锁，插入意向锁就是其中一种所结构，它表示当前记录范围被其他事务锁定，当前事务对被锁定的记录范围有插入意向，正在等待插入（被阻塞）。插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁
隐式锁：一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），当别的事务在对这条新加的记录加 S锁 或者 X锁时，程序会先判断新纪录上的trx_id是否与要加锁的事务一致，如果不一致，则会给插入记录的事务生成一个锁结构，然后再给此事务生成一个锁结构后进入等待状态，这种方式成为隐式加锁。</content></entry><entry><title>VMware虚拟机CentOS 7.5设置静态ip</title><url>/post/vmware%E8%99%9A%E6%8B%9F%E6%9C%BAcentos-7.5%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81ip/</url><categories><category>Linux</category><category>Linux Basis</category></categories><tags><tag>Linux</tag><tag>ip</tag></tags><content type="html"> CentOS 7.5设置静态ip,配置网关，DNS等
打开vmware虚拟机界面的编辑->虚拟网络编辑器如下图： 打开网配置文件
#vim /etc/sysconfig/network-scripts/ifcfg-ens33
DEFROUTE="yes" IPV4_FAILURE_FATAL="no" IPV6INIT="yes" IPV6_AUTOCONF="yes" IPV6_DEFROUTE="yes" IPV6_FAILURE_FATAL="no" IPV6_ADDR_GEN_MODE="stable-privacy" NAME="ens33" UUID="48aeeb81-96b5-4d08-880a-53e3f527469c" DEVICE="ens33" ONBOOT="yes" BOOTPROTO=static #静态ip获取方式 IPADDR="192.168.52.104" #静态ip地址 NETMASK="255.255.255.0" #子网掩码 GATEWAY="192.168.52.2" #网关 DNS1="114.114.114.114" #国内移动联通电信网络DNS</content></entry><entry><title>Linux常用设置、命令杂记</title><url>/post/linux%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%E5%91%BD%E4%BB%A4%E6%9D%82%E8%AE%B0/</url><categories><category>Linux</category><category>Linux Command</category></categories><tags><tag>Linux</tag><tag>Command</tag></tags><content type="html"> 记录Linux常用的便捷设置和常用命令
快速卸载已安装的rpm软件 rpm -qa | grep -i java | xargs -n1 rpm -e &ndash;nodeps
Ø rpm -qa：查询所安装的所有rpm软件包
Ø grep -i：忽略大小写
Ø xargs -n1：表示每次只传递一个参数
Ø rpm -e –nodeps：强制卸载软件，忽略依赖
安装java *1**）**卸载现有**JDK*
注意：安装JDK前，一定确保提前删除了虚拟机自带的JDK。详细步骤见问文档3.1节中卸载JDK步骤。
*2**）**用**XShell传输**工具将JDK导入到opt目录下面的software文件夹下面*
*3**）**在Linux系统下的opt目录中查看软件包是否导入成功*
[atguigu@hadoop102 ~]$ ls /opt/software/
看到如下结果：
jdk-8u212-linux-x64.tar.gz
*4**）**解压JDK到/opt/module目录下*
[atguigu@hadoop102 software]$ tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
*5**）**配置JDK环境变量*
​ （1）新建/etc/profile.d/my_env.sh文件
[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh
添加如下内容
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
​ （2）保存后退出
:wq
​ （3）source一下/etc/profile文件，让新的环境变量PATH生效
[atguigu@hadoop102 ~]$ source /etc/profile
*6**）**测试JDK**是否**安装成功*
[atguigu@hadoop102 ~]$ java -version
如果能看到以下结果，则代表Java安装成功。
java version &ldquo;1.8.0_212&rdquo;
编写集群分发脚本xsync *1）scp（**secure copy**）**安全**拷贝*
（1）scp定义
scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）
（2）基本语法
scp -r $pdir/$fname $user@$host:$pdir/$fname
命令 递归 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称
（3）案例实操
Ø 前提：在hadoop102、hadoop103、hadoop104都已经创建好的/opt/module、 /opt/software两个目录，并且已经把这两个目录修改为atguigu:atguigu
[atguigu@hadoop102 ~]$ sudo chown atguigu:atguigu -R /opt/module
（a）在hadoop102上，将hadoop102中/opt/module/jdk1.8.0_212目录拷贝到hadoop103上。
[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212 atguigu@hadoop103:/opt/module
（b）在hadoop103上，将hadoop102中/opt/module/hadoop-3.1.3目录拷贝到hadoop103上。
[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/
（c）在hadoop103上操作，将hadoop102中/opt/module目录下所有目录拷贝到hadoop104上。
[atguigu@hadoop103 opt]$ scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module
*2）rsync远程**同步**工具*
rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。
rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。
​ （1）基本语法
rsync -av $pdir/$fname $user@$host:$pdir/$fname
命令 选项参数 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称
​ 选项参数说明
选项 功能 -a 归档拷贝 -v 显示复制过程 （2）案例实操
​ （a）删除hadoop103中/opt/module/hadoop-3.1.3/wcinput
[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/
​ （b）同步hadoop102中的/opt/module/hadoop-3.1.3到hadoop103
[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/
*3）**xsync集群分发**脚本*
（1）需求：循环复制文件到所有节点的相同目录下
​ （2）需求分析：
（a）rsync命令原始拷贝：
rsync -av /opt/module atguigu@hadoop103:/opt/
（b）期望脚本：
xsync要同步的文件名称
（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）
[atguigu@hadoop102 ~]$ echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin:/opt/module/jdk1.8.0_212/bin
（3）脚本实现
（a）在/home/atguigu/bin目录下创建xsync文件
[atguigu@hadoop102 opt]$ cd /home/atguigu
[atguigu@hadoop102 ~]$ mkdir bin
[atguigu@hadoop102 ~]$ cd bin
[atguigu@hadoop102 bin]$ vim xsync
在该文件中编写如下代码
# #!/bin/bash #1. 判断参数个数 if [ $# -lt 1 ] then echo Not Enough Arguement! exit; fi #2. 遍历集群所有机器 for host in centos1 centos2 centos4 do echo ==================== $host ==================== #3. 遍历所有目录，挨个发送 for file in $@ do #4. 判断文件是否存在 if [ -e $file ] then #5. 获取父目录 pdir=$(cd -P $(dirname $file); pwd) #6. 获取当前文件的名称 fname=$(basename $file) ssh $host "mkdir -p $pdir" rsync -av $pdir/$fname $host:$pdir else echo $file does not exists! fi done done （b）修改脚本 xsync 具有执行权限
[atguigu@hadoop102 bin]$ chmod +x xsync
（c）测试脚本
[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin
（d）将脚本复制到/bin中，以便全局调用
[atguigu@hadoop102 bin]$ sudo cp xsync /bin/
（e）同步环境变量配置（root所有者）
[atguigu@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh
注意：如果用了sudo，那么xsync一定要给它的路径补全。
让环境变量生效
[atguigu@hadoop103 bin]$ source /etc/profile
[atguigu@hadoop104 opt]$ source /etc/profile
SSH无密登录配置 *1**）**配置ssh*
（1）基本语法
ssh另一台电脑的IP地址
（2）ssh连接时出现Host key verification failed的解决方法
[atguigu@hadoop102 ~]$ ssh hadoop103
Ø 如果出现如下内容
Are you sure you want to continue connecting (yes/no)?
Ø 输入yes，并回车
（3）退回到hadoop102
[atguigu@hadoop103 ~]$ exit
*2**）**无密钥配置*
（1）免密登录原理
（2）生成公钥和私钥
[atguigu@hadoop102 .ssh]$ pwd
/home/atguigu/.ssh
[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa
然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
（3）将公钥拷贝到要免密登录的目标机器上
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104
注意：
还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。
还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。
还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；
*3**）**.ssh文件夹下**（~/.ssh）**的文件功能解释*
known_hosts 记录ssh访问过计算机的公钥（public key） id_rsa 生成的私钥 id_rsa.pub 生成的公钥 authorized_keys 存放授权过的无密登录服务器公钥</content></entry><entry><title>Linux文件详细属性学习</title><url>/post/linux%E6%96%87%E4%BB%B6%E8%AF%A6%E7%BB%86%E5%B1%9E%E6%80%A7%E5%AD%A6%E4%B9%A0/</url><categories><category>Linux</category><category>Linux Basis</category></categories><tags><tag>Linux</tag><tag>文件属性</tag><tag>ls命令</tag></tags><content type="html"> 详细介绍如何查看并分辨Linux文件的详细属性，包括文件的类型，权限信息，所有者，用户等
Linux的基本思想有两点：第一，一切都是文件；第二，每个文件都有确定的用途。其中第一条详细来讲就是系统中的所有都归结为一个文件，包括命令、硬件和软件设备、操作系统、进程等等对于操作系统内核而言，都被视为拥有各自特性或类型的文件
ls -l 命令是以长格式的形式查看当前目录下所有可见文件的详细属性
文件的详细属性如下：
如果是一个符号链接，那么会有一个 “->" 箭头符号，后面根一个它指向的文件名;
灰白色表示普通文件；
亮绿色表示可执行文件；
亮红色表示压缩文件；
灰蓝色表示目录；
亮蓝色表示链接文件；
亮黄色表示设备文件；</content></entry><entry><title>Vim的四种模式及操作命令大全</title><url>/post/vim%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8F%E5%8F%8A%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url><categories><category>Linux</category><category>Linux Command</category></categories><tags><tag>Linux</tag><tag>Vim</tag><tag>Command</tag></tags><content type="html"> 介绍Vim的四种模式及命令操作大全
一、四种模式 1、正常模式（normal） 正常模式是使用vim打开文件时的默认模式。 无论在哪种模式下，按下Esc键就会进入正常模式。 在这个模式下： 可以移动光标 选中行，复制（ctrl+C） 可以增、删 x删除光标后的一个字符，nx（n是数字）删除光标后的n个字符，X删除光标前的一个字符， dd剪切光标所在的那一行，ndd剪切光标所在行后的n行 p光标所在行开始，向后粘贴已经复制的内容，P光标所在行开始，向前粘贴已经复制的内容 yy复制光标所在的行，nyy复制光标所在行后的n行 u还原上一次的操作 2、命令模式（command） 在正常模式下输入:或/进入命令行模式 在该模式下可以进行保存，搜索，替换，退出，显示行号等。 /word 光标之后查找字符串word，按n向后搜索，按N向前搜索 ?word光标之前查找字符串word，按n向前搜索，按N向前搜索 :n1,n2/word1/word2/g 将n1到n2行之间的word1替换为word2，不加g则只替换每行的第一个word1，加g则搜到的word1全部替换为word2； :1,$s/word1/word2/g将文章中的word1替换为word2，不加g则只替换每行的第一个word1 :w保存文本 ，:w!强制保存 :q退出vim ；:q!强制退出 :wq 保存并退出 :set nu 显示行号，:set nonu不显示行号 3、插入模式（insert） 在正常模式下按下i键，进入插入模式。在插入模式下按Esc键切换到普通模式。 插入模式里可以进行文字的输入 i 在光标所在字符前开始输入文字并进入插入模式。 I 在行首开始输入文字并进入插入模式。此行首指第一个非空白字符处。如果行首有空格，则在空格之后输入文字并进入插入模式 a 在光标所在字符后开始输入文字并进入插入模式 A 在行尾开始输入文字并进入插入模式。这个好用，您不必管光标在此行的什麽地方，只要按 A 就会在行尾等着您输入文字。 o 在光标所在行的下面单独开一新行，来输入文字并进入插入模式 O 在光标所在行的上面单独开一新行来输入文字并进入插入模式 s 删除光标所在的字符并进入插入模式 S 删除光标所在行并进入插入模式 4、可视模式（visual） 在正常模式下按v（小写）进入字符文本，按V（大写）进入行文本，然后使用上下左右键操作选中区域，对选中的部分使用 d进行删除 y进行复制 p进行粘贴 r进行文本替换 gu转换为小写，gU转换为大写，g~大小写互换 二、操作命令大全 [常用] 剪切和复制、粘贴
[n]x: 剪切光标右边n个字符，相当于d[n]l。 [n]X: 剪切光标左边n个字符，相当于d[n]h。 y: 复制在可视模式下选中的文本。 yy or Y: 复制整行文本。 y[n]w: 复制一(n)个词。 y[n]l: 复制光标右边1(n)个字符。 y[n]h: 复制光标左边1(n)个字符。 y$: 从光标当前位置复制到行尾。 y0: 从光标当前位置复制到行首。 :m,ny 复制m行到n行的内容。 y1G或ygg: 复制光标以上的所有行。 yG: 复制光标以下的所有行。 yaw和yas：复制一个词和复制一个句子，即使光标不在词首和句首也没关系。 d: 删除（剪切）在可视模式下选中的文本。 d$ or D: 删除（剪切）当前位置到行尾的内容。 d[n]w: 删除（剪切）1(n)个单词 d[n]l: 删除（剪切）光标右边1(n)个字符。 d[n]h: 删除（剪切）光标左边1(n)个字符。 d0: 删除（剪切）当前位置到行首的内容 [n] dd: 删除（剪切）1(n)行。 :m,nd 剪切m行到n行的内容。 d1G或dgg: 剪切光标以上的所有行。 dG: 剪切光标以下的所有行。 daw和das：剪切一个词和剪切一个句子，即使光标不在词首和句首也没关系。 d/f：这是一个比较高级的组合命令，它将删除当前位置 到下一个f之间的内容。 p: 在光标之后粘贴。 P: 在光标之前粘贴。 [常用]基本插入
i: 在光标前插入；一个小技巧：按8，再按i，进入插入模式，输入=， 按esc进入命令模式，就会出现8个=。 这在插入分割线时非常有用，如30i+就插入了36个+组成的分割线。 I: 在当前行第一个非空字符前插入； gI: 在当前行第一列插入； a: 在光标后插入； A: 在当前行最后插入； o: 在下面新建一行插入； O: 在上面新建一行插入； :r filename在当前位置插入另一个文件的内容。 :[n]r filename在第n行插入另一个文件的内容。 :r !date 在光标处插入当前日期与时间。同理，:r !command可以将其它shell命令的输出插入当前文档。 [常用] 基本移动
以下移动都是在normal模式下。
h或退格: 左移一个字符； l或空格: 右移一个字符； j: 下移一行； k: 上移一行； gj: 移动到一段内的下一行； gk: 移动到一段内的上一行； +或Enter: 把光标移至下一行第一个非空白字符。 -: 把光标移至上一行第一个非空白字符。 w: 前移一个单词，光标停在下一个单词开头； W: 移动下一个单词开头，但忽略一些标点； e: 前移一个单词，光标停在下一个单词末尾； E: 移动到下一个单词末尾，如果词尾有标点，则移动到标点； b: 后移一个单词，光标停在上一个单词开头； B: 移动到上一个单词开头，忽略一些标点； ge: 后移一个单词，光标停在上一个单词末尾； gE: 同 ge ，不过‘单词’包含单词相邻的标点。 (: 前移1句。 ): 后移1句。 {: 前移1段。 }: 后移1段。 fc: 把光标移到同一行的下一个c字符处 Fc: 把光标移到同一行的上一个c字符处 tc: 把光标移到同一行的下一个c字符前 Tc: 把光标移到同一行的上一个c字符后 ;: 配合f &amp; t使用，重复一次 ,: 配合f &amp; t使用，反向重复一次 上面的操作都可以配合n使用，比如在正常模式(下面会讲到)下输入3h， 则光标向左移动3个字符。
0: 移动到行首。 g0: 移到光标所在屏幕行行首。 ^: 移动到本行第一个非空白字符。 g^: 同 ^ ，但是移动到当前屏幕行第一个非空字符处。 $: 移动到行尾。 g$: 移动光标所在屏幕行行尾。 n|: 把光标移到递n列上。 nG: 到文件第n行。 :n 移动到第n行。 :$ 移动到最后一行。 H: 把光标移到屏幕最顶端一行。 M: 把光标移到屏幕中间一行。 L: 把光标移到屏幕最底端一行。 gg: 到文件头部。 G: 到文件尾部。 2. 启动Vim
vim -c cmd file: 在打开文件前，先执行指定的命令； vim -r file: 恢复上次异常退出的文件； vim -R file: 以只读的方式打开文件，但可以强制保存； vim -M file: 以只读的方式打开文件，不可以强制保存； vim -y num file: 将编辑窗口的大小设为num行； vim + file: 从文件的末尾开始； vim +num file: 从第num行开始； vim +/string file: 打开file，并将光标停留在第一个找到的string上。 vim &ndash;remote file: 用已有的vim进程打开指定的文件。 如果你不想启用多个vim会话，这个很有用。但要注意， 如果你用vim，会寻找名叫VIM的服务器；如果你已经有一个gvim在运行了， 你可以用gvim &ndash;remote file在已有的gvim中打开文件。 3. 文档操作
:e file &ndash;关闭当前编辑的文件，并开启新的文件。 如果对当前文件的修改未保存，vi会警告。 :e! file &ndash;放弃对当前文件的修改，编辑新的文件。 :e+file &ndash; 开始新的文件，并从文件尾开始编辑。 :e+n file &ndash; 开始新的文件，并从第n行开始编辑。 :enew &ndash;编译一个未命名的新文档。(CTRL-W n) :e &ndash; 重新加载当前文档。 :e! &ndash; 重新加载当前文档，并丢弃已做的改动。 :e#或ctrl+^ &ndash; 回到刚才编辑的文件，很实用。 :f或ctrl+g &ndash; 显示文档名，是否修改，和光标位置。 :f filename &ndash; 改变编辑的文件名，这时再保存相当于另存为。 gf &ndash; 打开以光标所在字符串为文件名的文件。 :w &ndash; 保存修改。 :n1,n2w filename &ndash; 选择性保存从某n1行到另n2行的内容。 :wq &ndash; 保存并退出。 ZZ &ndash; 保存并退出。 :x &ndash; 保存并退出。 :q[uit] ——退出当前窗口。(CTRL-W q或CTRL-W CTRL-Q) :saveas newfilename &ndash; 另存为 :browse e &ndash; 会打开一个文件浏览器让你选择要编辑的文件。 如果是终端中，则会打开netrw的文件浏览窗口； 如果是gvim，则会打开一个图形界面的浏览窗口。 实际上:browse后可以跟任何编辑文档的命令，如sp等。 用browse打开的起始目录可以由browsedir来设置： :set browsedir=last &ndash; 用上次访问过的目录（默认）； :set browsedir=buffer &ndash; 用当前文件所在目录； :set browsedir=current &ndash; 用当前工作目录； :Sex &ndash; 水平分割一个窗口，浏览文件系统； :Vex &ndash; 垂直分割一个窗口，浏览文件系统； 4. 光标的移动
4.1 基本移动
以下移动都是在normal模式下。
h或退格: 左移一个字符； l或空格: 右移一个字符； j: 下移一行； k: 上移一行； gj: 移动到一段内的下一行； gk: 移动到一段内的上一行； +或Enter: 把光标移至下一行第一个非空白字符。 -: 把光标移至上一行第一个非空白字符。 w: 前移一个单词，光标停在下一个单词开头； W: 移动下一个单词开头，但忽略一些标点； e: 前移一个单词，光标停在下一个单词末尾； E: 移动到下一个单词末尾，如果词尾有标点，则移动到标点； b: 后移一个单词，光标停在上一个单词开头； B: 移动到上一个单词开头，忽略一些标点； ge: 后移一个单词，光标停在上一个单词末尾； gE: 同 ge ，不过‘单词’包含单词相邻的标点。 (: 前移1句。 ): 后移1句。 {: 前移1段。 }: 后移1段。 fc: 把光标移到同一行的下一个c字符处 Fc: 把光标移到同一行的上一个c字符处 tc: 把光标移到同一行的下一个c字符前 Tc: 把光标移到同一行的上一个c字符后 ;: 配合f &amp; t使用，重复一次 ,: 配合f &amp; t使用，反向重复一次 上面的操作都可以配合n使用，比如在正常模式(下面会讲到)下输入3h， 则光标向左移动3个字符。
0: 移动到行首。 g0: 移到光标所在屏幕行行首。 ^: 移动到本行第一个非空白字符。 g^: 同 ^ ，但是移动到当前屏幕行第一个非空字符处。 $: 移动到行尾。 g$: 移动光标所在屏幕行行尾。 n|: 把光标移到递n列上。 nG: 到文件第n行。 :n 移动到第n行。 :$ 移动到最后一行。 H: 把光标移到屏幕最顶端一行。 M: 把光标移到屏幕中间一行。 L: 把光标移到屏幕最底端一行。 gg: 到文件头部。 G: 到文件尾部。 4.2 翻屏
ctrl+f: 下翻一屏。 ctrl+b: 上翻一屏。 ctrl+d: 下翻半屏。 ctrl+u: 上翻半屏。 ctrl+e: 向下滚动一行。 ctrl+y: 向上滚动一行。 n%: 到文件n%的位置。 zz: 将当前行移动到屏幕中央。 zt: 将当前行移动到屏幕顶端。 zb: 将当前行移动到屏幕底端。 4.3 标记
使用标记可以快速移动。到达标记后，可以用Ctrl+o返回原来的位置。 Ctrl+o和Ctrl+i 很像浏览器上的 后退 和 前进 。
m{a-z}: 标记光标所在位置，局部标记，只用于当前文件。 m{A-Z}: 标记光标所在位置，全局标记。标记之后，退出Vim， 重新启动，标记仍然有效。 `{a-z}: 移动到标记位置。 &lsquo;{a-z}: 移动到标记行的行首。 `{0-9}：回到上[2-10]次关闭vim时最后离开的位置。 : 移动到上次编辑的位置。''也可以，不过精确到列，而&rsquo;&lsquo;精确到行 。如果想跳转到更老的位置，可以按C-o，跳转到更新的位置用C-i。 `": 移动到上次离开的地方。 `.: 移动到最后改动的地方。 :marks 显示所有标记。 :delmarks a b &ndash; 删除标记a和b。 :delmarks a-c &ndash; 删除标记a、b和c。 :delmarks a c-f &ndash; 删除标记a、c、d、e、f。 :delmarks! &ndash; 删除当前缓冲区的所有标记。 :help mark-motions 查看更多关于mark的知识。 5. 插入文本
[ 5.2 改写插入
c[n]w: 改写光标后1(n)个词。 c[n]l: 改写光标后n个字母。 c[n]h: 改写光标前n个字母。 [n]cc: 修改当前[n]行。 [n]s: 以输入的文本替代光标之后1(n)个字符，相当于c[n]l。 [n]S: 删除指定数目的行，并以所输入文本代替之。 注意，类似cnw,dnw,ynw的形式同样可以写为ncw,ndw,nyw。
6. 剪切复制和寄存器
6.2 文本对象
aw：一个词 as：一句。 ap：一段。 ab：一块（包含在圆括号中的）。 y, d, c, v都可以跟文本对象。
6.3 寄存器
a-z：都可以用作寄存器名。&ldquo;ayy把当前行的内容放入a寄存器。 A-Z：用大写字母索引寄存器，可以在寄存器中追加内容。 如"Ayy把当前行的内容追加到a寄存器中。 :reg 显示所有寄存器的内容。 &ldquo;"：不加寄存器索引时，默认使用的寄存器。 &ldquo;*：当前选择缓冲区，"*yy把当前行的内容放入当前选择缓冲区。 &ldquo;+：系统剪贴板。"+yy把当前行的内容放入系统剪贴板。 7. 查找与替换
7.1 查找
/something: 在后面的文本中查找something。 ?something: 在前面的文本中查找something。 /pattern/+number: 将光标停在包含pattern的行后面第number行上。 /pattern/-number: 将光标停在包含pattern的行前面第number行上。 n: 向后查找下一个。 N: 向前查找下一个。 可以用grep或vimgrep查找一个模式都在哪些地方出现过，
其中:grep是调用外部的grep程序，而:vimgrep是vim自己的查找算法。
用法为： :vim[grep]/pattern/[g] [j] files
g的含义是如果一个模式在一行中多次出现，则这一行也在结果中多次出现。
j的含义是grep结束后，结果停在第j项，默认是停在第一项。
vimgrep前面可以加数字限定搜索结果的上限，如
:1vim/pattern/ % 只查找那个模式在本文件中的第一个出现。
其实vimgrep在读纯文本电子书时特别有用，可以生成导航的目录。
比如电子书中每一节的标题形式为：n. xxxx。你就可以这样：
:vim/^d{1,}./ %
然后用:cw或:copen查看结果，可以用C-w H把quickfix窗口移到左侧，
就更像个目录了。
7.2 替换
:s/old/new - 用new替换当前行第一个old。 :s/old/new/g - 用new替换当前行所有的old。 :n1,n2s/old/new/g - 用new替换文件n1行到n2行所有的old。 :%s/old/new/g - 用new替换文件中所有的old。 :%s/^/xxx/g - 在每一行的行首插入xxx，^表示行首。 :%s/$/xxx/g - 在每一行的行尾插入xxx，$表示行尾。 所有替换命令末尾加上c，每个替换都将需要用户确认。 如：%s/old/new/gc，加上i则忽略大小写(ignore)。 还有一种比替换更灵活的方式，它是匹配到某个模式后执行某种命令，
语法为 :[range]g/pattern/command
例如 :%g/^ xyz/normal dd。
表示对于以一个空格和xyz开头的行执行normal模式下的dd命令。
关于range的规定为：
如果不指定range，则表示当前行。 m,n: 从m行到n行。 0: 最开始一行（可能是这样）。 $: 最后一行 .: 当前行 %: 所有行 7.3 正则表达式
高级的查找替换就要用到正则表达式。
d: 表示十进制数（我猜的） s: 表示空格 S: 非空字符 a: 英文字母 |: 表示 或 .: 表示. {m,n}: 表示m到n个字符。这要和 s与a等连用，如 a{m,n} 表示m 到n个英文字母。 {m,}: 表示m到无限多个字符。 **: 当前目录下的所有子目录。 :help pattern得到更多帮助。
8. 排版
8.1 基本排版
&laquo; 向左缩进一个shiftwidth >> 向右缩进一个shiftwidth :ce(nter) 本行文字居中 :le(ft) 本行文字靠左 :ri(ght) 本行文字靠右 gq 对选中的文字重排，即对过长的文字进行断行 gqq 重排当前行 gqnq 重排n行 gqap 重排当前段 gqnap 重排n段 gqnj 重排当前行和下面n行 gqQ 重排当前段对文章末尾 J 拼接当前行和下一行 gJ 同 J ，不过合并后不留空格。 8.2 拼写检查
:set spell－开启拼写检查功能 :set nospell－关闭拼写检查功能 ]s－移到下一个拼写错误的单词 [s－作用与上一命令类似，但它是从相反方向进行搜索 z=－显示一个有关拼写错误单词的列表，可从中选择 zg－告诉拼写检查器该单词是拼写正确的 zw－与上一命令相反，告诉拼写检查器该单词是拼写错误的 8.3 统计字数
g ^g可以统计文档字符数，行数。 将光标放在最后一个字符上，用字符数减去行数可以粗略统计中文文档的字数。 以上对 Mac 或 Unix 的文件格式适用。 如果是 Windows 文件格式（即换行符有两个字节），字数的统计方法为： 字符数 - 行数 * 2。
9. 编辑多个文件
9.1 一次编辑多个文件
我们可以一次打开多个文件，如
&lt;span style="font-size:14px;">vi a.txt b.txt c.txt &lt;/span> 使用:next(:n)编辑下一个文件。 :2n 编辑下2个文件。 使用:previous或:N编辑上一个文件。 使用:wnext，保存当前文件，并编辑下一个文件。 使用:wprevious，保存当前文件，并编辑上一个文件。 使用:args 显示文件列表。 :n filenames或:args filenames 指定新的文件列表。 vi -o filenames 在水平分割的多个窗口中编辑多个文件。 vi -O filenames 在垂直分割的多个窗口中编辑多个文件。 9.2 多标签编辑
vim -p files: 打开多个文件，每个文件占用一个标签页。 :tabe, tabnew &ndash; 如果加文件名，就在新的标签中打开这个文件， 否则打开一个空缓冲区。 ^w gf &ndash; 在新的标签页里打开光标下路径指定的文件。 :tabn &ndash; 切换到下一个标签。Control + PageDown，也可以。 :tabp &ndash; 切换到上一个标签。Control + PageUp，也可以。 [n] gt &ndash; 切换到下一个标签。如果前面加了 n ， 就切换到第n个标签。第一个标签的序号就是1。 :tab split &ndash; 将当前缓冲区的内容在新页签中打开。 :tabc[lose] &ndash; 关闭当前的标签页。 :tabo[nly] &ndash; 关闭其它的标签页。 :tabs &ndash; 列出所有的标签页和它们包含的窗口。 :tabm[ove] [N] &ndash; 移动标签页，移动到第N个标签页之后。 如 tabm 0 当前标签页，就会变成第一个标签页。 9.3 缓冲区
:buffers或:ls或:files 显示缓冲区列表。 ctrl+^：在最近两个缓冲区间切换。 :bn &ndash; 下一个缓冲区。 :bp &ndash; 上一个缓冲区。 :bl &ndash; 最后一个缓冲区。 :b[n]或:[n]b &ndash; 切换到第n个缓冲区。 :nbw(ipeout) &ndash; 彻底删除第n个缓冲区。 :nbd(elete) &ndash; 删除第n个缓冲区，并未真正删除，还在unlisted列表中。 :ba[ll] &ndash; 把所有的缓冲区在当前页中打开，每个缓冲区占一个窗口。 10. 分屏编辑
vim -o file1 file2:水平分割窗口，同时打开file1和file2 vim -O file1 file2:垂直分割窗口，同时打开file1和file2 10.1 水平分割
:split(:sp) &ndash; 把当前窗水平分割成两个窗口。(CTRL-W s 或 CTRL-W CTRL-S) 注意如果在终端下，CTRL-S可能会冻结终端，请按CTRL-Q继续。 :split filename &ndash; 水平分割窗口，并在新窗口中显示另一个文件。 :nsplit(:nsp) &ndash; 水平分割出一个n行高的窗口。 :[N]new &ndash; 水平分割出一个N行高的窗口，并编辑一个新文件。 (CTRL-W n或 CTRL-W CTRL-N) ctrl+w f &ndash;水平分割出一个窗口，并在新窗口打开名称为光标所在词的文件 。 C-w C-^ &ndash; 水平分割一个窗口，打开刚才编辑的文件。 10.2 垂直分割
:vsplit(:vsp) &ndash; 把当前窗口分割成水平分布的两个窗口。 (CTRL-W v或CTRL CTRL-V) :[N]vne[w] &ndash; 垂直分割出一个新窗口。 :vertical 水平分割的命令： 相应的垂直分割。 10.3 关闭子窗口
:qall &ndash; 关闭所有窗口，退出vim。 :wall &ndash; 保存所有修改过的窗口。 :only &ndash; 只保留当前窗口，关闭其它窗口。(CTRL-W o) :close &ndash; 关闭当前窗口，CTRL-W c能实现同样的功能。 (象 :q :x同样工作 ) 10.4 调整窗口大小
ctrl+w + &ndash;当前窗口增高一行。也可以用n增高n行。 ctrl+w - &ndash;当前窗口减小一行。也可以用n减小n行。 ctrl+w _ &ndash;当前窗口扩展到尽可能的大。也可以用n设定行数。 :resize n &ndash; 当前窗口n行高。 ctrl+w = &ndash; 所有窗口同样高度。 n ctrl+w _ &ndash; 当前窗口的高度设定为n行。 ctrl+w &lt; &ndash;当前窗口减少一列。也可以用n减少n列。 ctrl+w > &ndash;当前窗口增宽一列。也可以用n增宽n列。 ctrl+w | &ndash;当前窗口尽可能的宽。也可以用n设定列数。 10.5 切换和移动窗口
如果支持鼠标，切换和调整子窗口的大小就简单了。
ctrl+w ctrl+w: 切换到下一个窗口。或者是ctrl+w w。 ctrl+w p: 切换到前一个窗口。 ctrl+w h(l,j,k):切换到左（右，下，上）的窗口。 ctrl+w t(b):切换到最上（下）面的窗口。 ctrl+w H(L,K,J): 将当前窗口移动到最左（右、上、下）面。 ctrl+w r：旋转窗口的位置。 ctrl+w T: 将当前的窗口移动到新的标签页上。 11. 快速编辑
11.1 改变大小写
~: 反转光标所在字符的大小写。 可视模式下的U或u：把选中的文本变为大写或小写。 gu(U)接范围（如$，或G），可以把从光标当前位置到指定位置之间字母全部 转换成小写或大写。如ggguG，就是把开头到最后一行之间的字母全部变为小 写。再如gu5j，把当前行和下面四行全部变成小写。 11.2 替换（normal模式）
r: 替换光标处的字符，同样支持汉字。 R: 进入替换模式，按esc回到正常模式。 11.3 撤消与重做（normal模式）
[n] u: 取消一(n)个改动。 :undo 5 &ndash; 撤销5个改变。 :undolist &ndash; 你的撤销历史。 ctrl + r: 重做最后的改动。 U: 取消当前行中所有的改动。 :earlier 4m &ndash; 回到4分钟前 :later 55s &ndash; 前进55秒 11.4 宏
. &ndash;重复上一个编辑动作 qa：开始录制宏a（键盘操作记录） q：停止录制 @a：播放宏a 12. 编辑特殊文件
12.1 文件加解密
vim -x file: 开始编辑一个加密的文件。 :X &ndash; 为当前文件设置密码。 :set key= &ndash; 去除文件的密码。 这里是
滇狐总结的比较高级的vi技巧。
12.2 文件的编码
:e ++enc=utf8 filename, 让vim用utf-8的编码打开这个文件。 :w ++enc=gbk，不管当前文件什么编码，把它转存成gbk编码。 :set fenc或:set fileencoding，查看当前文件的编码。 在vimrc中添加set fileencoding=ucs-bom,utf-8,cp936，vim会根据要打开的文件选择合适的编码。 注意：编码之间不要留空格。 cp936对应于gbk编码。 ucs-bom对应于windows下的文件格式。 让vim 正确处理文件格式和文件编码，有赖于 ~/.vimrc的正确配置
12.3 文件格式
大致有三种文件格式：unix, dos, mac. 三种格式的区别主要在于回车键的编码：dos 下是回车加换行，unix 下只有 换行符，mac 下只有回车符。
:e ++ff=dos filename, 让vim用dos格式打开这个文件。 :w ++ff=mac filename, 以mac格式存储这个文件。 :set ff，显示当前文件的格式。 在vimrc中添加set fileformats=unix,dos,mac，让vim自动识别文件格式。 13. 编程辅助
13.1 一些按键
gd: 跳转到局部变量的定义处； gD: 跳转到全局变量的定义处，从当前文件开头开始搜索； g;: 上一个修改过的地方； g,: 下一个修改过的地方； [[: 跳转到上一个函数块开始，需要有单独一行的{。 ]]: 跳转到下一个函数块开始，需要有单独一行的{。 []: 跳转到上一个函数块结束，需要有单独一行的}。 ][: 跳转到下一个函数块结束，需要有单独一行的}。 [{: 跳转到当前块开始处； ]}: 跳转到当前块结束处； [/: 跳转到当前注释块开始处； ]/: 跳转到当前注释块结束处； %: 不仅能移动到匹配的(),{}或[]上，而且能在#if，#else， #endif之间跳跃。 下面的括号匹配对编程很实用的。
ci&rsquo;, di&rsquo;, yi&rsquo;：修改、剪切或复制&rsquo;之间的内容。 ca&rsquo;, da&rsquo;, ya&rsquo;：修改、剪切或复制&rsquo;之间的内容，包含&rsquo;。 ci&rdquo;, di&rdquo;, yi&rdquo;：修改、剪切或复制"之间的内容。 ca&rdquo;, da", ya"：修改、剪切或复制"之间的内容，包含"。 ci(, di(, yi(：修改、剪切或复制()之间的内容。 ca(, da(, ya(：修改、剪切或复制()之间的内容，包含()。 ci[, di[, yi[：修改、剪切或复制[]之间的内容。 ca[, da[, ya[：修改、剪切或复制[]之间的内容，包含[]。 ci{, di{, yi{：修改、剪切或复制{}之间的内容。 ca{, da{, ya{：修改、剪切或复制{}之间的内容，包含{}。 ci&lt;, di&lt;, yi&lt;：修改、剪切或复制&lt;>之间的内容。 ca&lt;, da&lt;, ya&lt;：修改、剪切或复制&lt;>之间的内容，包含&lt;>。 13.2 ctags
ctags -R: 生成tag文件，-R表示也为子目录中的文件生成tags :set tags=path/tags &ndash; 告诉ctags使用哪个tag文件 :tag xyz &ndash; 跳到xyz的定义处，或者将光标放在xyz上按C-]，返回用C-t :stag xyz &ndash; 用分割的窗口显示xyz的定义，或者C-w ]， 如果用C-w n ]，就会打开一个n行高的窗口 :ptag xyz &ndash; 在预览窗口中打开xyz的定义，热键是C-w }。 :pclose &ndash; 关闭预览窗口。热键是C-w z。 :pedit abc.h &ndash; 在预览窗口中编辑abc.h :psearch abc &ndash; 搜索当前文件和当前文件include的文件，显示包含abc的行。 有时一个tag可能有多个匹配，如函数重载，一个函数名就会有多个匹配。 这种情况会先跳转到第一个匹配处。
:[n]tnext &ndash; 下一[n]个匹配。 :[n]tprev &ndash; 上一[n]个匹配。 :tfirst &ndash; 第一个匹配 :tlast &ndash; 最后一个匹配 :tselect tagname &ndash; 打开选择列表 tab键补齐
:tag xyz &ndash; 补齐以xyz开头的tag名，继续按tab键，会显示其他的。 :tag /xyz &ndash; 会用名字中含有xyz的tag名补全。 13.3 cscope
cscope -Rbq: 生成cscope.out文件 :cs add /path/to/cscope.out /your/work/dir :cs find c func &ndash; 查找func在哪些地方被调用 :cw &ndash; 打开quickfix窗口查看结果 13.4 gtags
Gtags综合了ctags和cscope的功能。 使用Gtags之前，你需要安装GNU Gtags。 然后在工程目录运行 gtags 。
:Gtags funcname 定位到 funcname 的定义处。 :Gtags -r funcname 查询 funcname被引用的地方。 :Gtags -s symbol 定位 symbol 出现的地方。 :Gtags -g string Goto string 出现的地方。 :Gtags -gi string 忽略大小写。 :Gtags -f filename 显示 filename 中的函数列表。 你可以用 :Gtags -f % 显示当前文件。 :Gtags -P pattern 显示路径中包含特定模式的文件。 如 :Gtags -P .h$ 显示所有头文件， :Gtags -P /vm/ 显示vm目录下的文件。 13.5 编译
vim提供了:make来编译程序，默认调用的是make， 如果你当前目录下有makefile，简单地:make即可。
如果你没有make程序，你可以通过配置makeprg选项来更改make调用的程序。 如果你只有一个abc.java文件，你可以这样设置：
&lt;span style="font-size:14px;">set makeprg=javac abc.java &lt;/span> 然后:make即可。如果程序有错，可以通过quickfix窗口查看错误。 不过如果要正确定位错误，需要设置好errorformat，让vim识别错误信息。 如：
&lt;span style="font-size:14px;">:setl efm=%A%f:%l: %m,%-Z%p^,%-C%.%# &lt;/span> %f表示文件名，%l表示行号， %m表示错误信息，其它的还不能理解。 请参考 :help errorformat。
13.6 快速修改窗口
其实是quickfix插件提供的功能， 对编译调试程序非常有用 :)
:copen &ndash; 打开快速修改窗口。 :cclose &ndash; 关闭快速修改窗口。 快速修改窗口在make程序时非常有用，当make之后：
:cl &ndash; 在快速修改窗口中列出错误。 :cn &ndash; 定位到下一个错误。 :cp &ndash; 定位到上一个错误。 :cr &ndash; 定位到第一个错误。 13.7 自动补全
C-x C-s &ndash; 拼写建议。 C-x C-v &ndash; 补全vim选项和命令。 C-x C-l &ndash; 整行补全。 C-x C-f &ndash; 自动补全文件路径。弹出菜单后，按C-f循环选择，当然也可以按 C-n和C-p。 C-x C-p 和C-x C-n &ndash; 用文档中出现过的单词补全当前的词。 直接按C-p和C-n也可以。 C-x C-o &ndash; 编程时可以补全关键字和函数名啊。 C-x C-i &ndash; 根据头文件内关键字补全。 C-x C-d &ndash; 补全宏定义。 C-x C-n &ndash; 按缓冲区中出现过的关键字补全。 直接按C-n或C-p即可。 当弹出补全菜单后：
C-p 向前切换成员； C-n 向后切换成员； C-e 退出下拉菜单，并退回到原来录入的文字； C-y 退出下拉菜单，并接受当前选项。 13.8 多行缩进缩出
正常模式下，按两下>;光标所在行会缩进。 如果先按了n，再按两下>;，光标以下的n行会缩进。 对应的，按两下&lt;;，光标所在行会缩出。 如果在编辑代码文件，可以用=进行调整。 在可视模式下，选择要调整的代码块，按=，代码会按书写规则缩排好。 或者n =，调整n行代码的缩排。 13.9 折叠
zf &ndash; 创建折叠的命令，可以在一个可视区域上使用该命令； zd &ndash; 删除当前行的折叠； zD &ndash; 删除当前行的折叠； zfap &ndash; 折叠光标所在的段； zo &ndash; 打开折叠的文本； zc &ndash; 收起折叠； za &ndash; 打开/关闭当前折叠； zr &ndash; 打开嵌套的折行； zm &ndash; 收起嵌套的折行； zR (zO) &ndash; 打开所有折行； zM (zC) &ndash; 收起所有折行； zj &ndash; 跳到下一个折叠处； zk &ndash; 跳到上一个折叠处； zi &ndash; enable/disable fold; 14. 命令行
normal模式下按:进入命令行模式
14.1 命令行模式下的快捷键：
上下方向键：上一条或者下一条命令。如果已经输入了部分命令，则找上一 条或者下一条匹配的命令。 左右方向键：左/右移一个字符。 C-w： 向前删除一个单词。 C-h： 向前删除一个字符，等同于Backspace。 C-u： 从当前位置移动到命令行开头。 C-b： 移动到命令行开头。 C-e： 移动到命令行末尾。 Shift-Left： 左移一个单词。 Shift-Right： 右移一个单词。 @： 重复上一次的冒号命令。 q： 正常模式下，q然后按&rsquo;:&rsquo;，打开命令行历史缓冲区， 可以像编辑文件一样编辑命令。 q/和q? 可以打开查找历史记录。 14.2 执行外部命令
:! cmd 执行外部命令。 :!! 执行上一次的外部命令。 :sh 调用shell，用exit返回vim。 :r !cmd 将命令的返回结果插入文件当前位置。 :m,nw !cmd 将文件的m行到n行之间的内容做为命令输入执行命令。 15. 其它
15.1 工作目录
:pwd 显示vim的工作目录。 :cd path 改变vim的工作目录。 :set autochdir 可以让vim 根据编辑的文件自动切换工作目录。 15.2 一些快捷键（收集中）
K: 打开光标所在词的manpage。 *: 向下搜索光标所在词。 g*: 同上，但部分符合即可。 #: 向上搜索光标所在词。 g#: 同上，但部分符合即可。 g C-g: 统计全文或统计部分的字数。 15.3 在线帮助
:h(elp)或F1 打开总的帮助。 :help user-manual 打开用户手册。 命令帮助的格式为：第一行指明怎么使用那个命令； 然后是缩进的一段解释这个命令的作用，然后是进一步的信息。 :helptags somepath 为somepath中的文档生成索引。 :helpgrep 可以搜索整个帮助文档，匹配的列表显示在quickfix窗口中。 Ctrl+] 跳转到tag主题，Ctrl+t 跳回。 :ver 显示版本信息。 15.4 一些小功能
简单计算器: 在插入模式下，输入C-r =，然后输入表达式，就能在 光标处得到计算结果。 vim命令小技巧 保存文件并退出 说起来有些惭愧，我也是最近才学到这个命令 x
和下面的命令是等价的： wq
都是保存当前文件并退出。
（译者注：这两个命令实际上并不完全等价，当文件被修改时两个命令时相同的。但如果未被修改，使用 : x 不会更改文件的修改时间，而使用 :wq 会改变文件的修改时间。）
基本计算器 在插入模式下，你可以使用 Ctrl+r 键然后输入 =，再输入一个简单的算式。按 Enter 键，计算结果就会插入到文件中。例如，尝试输入：
Ctrl+r '=2+2' ENTER 然后计算结果“4 ”会被插入到文件中。
查找重复的连续的单词 当你很快地打字时，很有可能会连续输入同一个单词两次，就像 this this。这种错误可能骗过任何一个人，即使是你自己重新阅读一遍也不可避免。幸运的是，有一个简单的正则表达式可以用来预防这个错误。使用搜索命令（默认是 /）然后输入：
这会显示所有重复的单词。要达到最好的效果，不要忘记把下面的命令：
set hlsearch 放到你的 .vimrc 文件中高亮所有的匹配。
缩写
一个很可能是最令人印象深刻的窍门是你可以在 Vim 中定义缩写，它可以实时地把你输入的东西替换为另外的东西。语法格式如下：
:ab [缩写] [要替换的文字] 一个通用的例子是：
:ab asap as soon as possible 会把你输入的 “asap” 替换为 “as soon as possible”。
在你忘记用 root 方式打开文件时的文件保存 这可能是一个在论坛中一直受欢迎的命令。每当你打开一个你没有写入权限的文件（比如系统配置文件）并做了一些修改，Vim 无法通过普通的 “:w” 命令来保存。
你不需要重新以 root 方式打开文件再进行修改，只需要运行：
:w !sudo tee % 这会直接以 root 方式保存。
实时加密文本 如果你不想让别人看懂你的屏幕上的内容，你可以使用一个内置的选项，通过下面的命令使用 ROT13
来对文本进行编码：
ggVGg? gg 把光标移动到 Vim 缓冲区的第一行，V 进入可视模式，G 把光标移动到缓冲区的最后一行。因此，ggVG 使可视模式覆盖这个当前缓冲区。最后 g? 使用 ROT13 对整个区域进行编码。
注意它可以被映射到一个最常使用的键。它对字母符号也可以很好地工作。要对它进行撤销，最好的方法就是使用撤销命令：u。
自动补全 这是另外一个令我感到惭愧的功能，但我发现周围很多人并不知道。Vim 默认有自动补全的功能。的确这个功能是很基本的，并且可以通过插件来增强，但它也很有帮助。方法很简单。Vim 尝试通过已经输入的单词来预测单词的结尾。比如当你在同一个文件中第二次输入 “compiler” 时，仅仅输入 “com” 然后保持在插入模式，按 Ctrl+n 键就可以看到 Vim 为你补全了单词。很简单，但也很有用。
比较两个文件的不同 你们中的大多数很可能都知道 vimdiff 命令，它可以使用分离模式打开 Vim 并比较两个文件的不同。语法如下：
$ vimdiff [文件1] [文件2] 但同样的结果也可以通过下面的 Vim 命令来获得：
:diffthis 首先在 Vim 中打开原始文件。然后使用分离模式带来第二个文件：
:vsp [文件2] 最后在第一个缓冲区里输入：
:diffthis 通过 Ctrl+w 来切换缓冲区并再次输入：
:diffthis 这样两个文件中不同的部分就会被高亮。
（译者注：可以直接在一个缓冲区里使用命令 :windo diffthis，而不用输入 :diffthis 两次）
要停止比较，使用：
:diffoff 按时间回退文件 Vim 会记录文件的更改，你很容易可以回退到之前某个时间。该命令是相当直观的。比如：
:earlier 1m 会把文件回退到 1 分钟以前的状态。
注意，你可以使用下面的命令进行相反的转换：
:later 删除标记内部的文字 当我开始使用 Vim 时，一件我总是想很方便做的事情是如何轻松的删除方括号或圆括号里的内容。转到开始的标记，然后使用下面的语法：
di[标记] 比如，把光标放在开始的圆括号上，使用下面的命令来删除圆括号内的文字：
di( 如果是方括号或者是引号，则使用：
di{ 和：
di" 删除指定标记前的内容 和删除标记内部有些相似，但目的不同。命令如下：
dt[标记] 会删除所有光标和标记之间的内容（保持标记不动），如果在同一行有这个标记的话。例如
dt. 会删除至句子的末尾，但保持 ‘.’ 不动。
把 Vim 变为十六进制编辑器 这不是我最喜欢的窍门，但有时会很有趣。你可以把 Vim 和 xxd 功能连起来来把文件转换为十六进制模式。命令如下：
:%!xxd 类似的，你可以通过下面的命令恢复原来的状态：
:%!xxd -r 把光标下的文字置于屏幕中央 我们所要做的事情如标题所示。如果你想强制滚动屏幕来把光标下的文字置于屏幕的中央，在可视模式中使用命令（译者注：在普通模式中也可以）：
zz
跳到上一个／下一个位置 当你编辑一个很大的文件时，经常要做的事是在某处进行修改，然后跳到另外一处。如果你想跳回之前修改的地方，使用命令：
Ctrl+o 来回到之前修改的地方
类似的：
Ctrl+i 会回退上面的跳动。
把当前文件转化为网页 这会生成一个 HTML 文件来显示文本，并在分开的窗口显示源代码：
:%TOhtml</content></entry><entry><title>Linux(CentOS 7)系统四种安装软件的方式及mysql5.7的详细安装配置</title><url>/post/linuxcentos-7%E7%B3%BB%E7%BB%9F%E5%9B%9B%E7%A7%8D%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%8Amysql5.7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url><categories><category>Linux</category><category>tool</category></categories><tags><tag>Linux</tag><tag>mysql</tag><tag>rpm</tag><tag>yum</tag><tag>CentOS 7</tag></tags><content type="html"> 本文以安装mysql为例介绍了Linux(CentOS 7)环境下安装软件的四种方式，比较四种方式的难易程度，以及详细介绍了mysql5.7的安装和配置方法
Linux安装软件的四种方式介绍 四种方式：rpm命令、yum命令、带bin目录的tar安装包和source源码编译安装
四种方式比较 rpm与yum为命令安装，不同的是npm在安装时需要手动下载依赖包，而yum会自动下载依赖
带bin的tar包和source源码安装区别是前者是编译好的，bin文件夹中就是编译之后的二进制可执行文件，后者需要自己手动编译
四种方式操作说明 一、npm和yum安装 rpm与yum命令对比（yum简单，rpm复杂）
RPM 全名 RedHat Package Managerment，是由Red Hat公司提出，被众多Linux发行版本所采用，是一种数据库记录的方式来将所需要的软件安装到到Linux系统的一套软件管理机制
rpm在安装时有严格的顺序限制，包与包有依赖关系，且安装过程中可能依赖别的包需要手动安装，而yum安装某个功能（例如mysql的server端）会自动下载安装依赖
安装软件：rpm -ivh [软件包名称] 卸载软件：rpm -e [软件包名称] 更新软件：rpm -Uvh [软件包名称]
yum check-update：列出所有可更新的软件清单命令;
yum update：更新所有软件或指定软件命令;
yum install ：仅安装指定的软件命令；
yum list：列出所有可安装的软件清单命令；
yum remove ：删除软件包命令；
yum search ：查找软件包命令：
以安装mysql为例，对比yum和rpm的安装过程
①查看是否安装了mysql/mariadb的服务
[root@localhost ~]# rpm -qa |grep -i mysql MySQL-client-5.6.23-1.sles11.x86_64 MySQL-server-5.6.23-1.sles11.x86_64 MySQL-shared-5.6.23-1.sles11.x86_64 MySQL-devel-5.6.23-1.sles11.x86_64 ... [root@localhost ~]# root@# rpm -qa |grep -i mariadb ... ②如果安装需要卸载所有服务
[root@localhost ~]# rpm -e --nodeps MySQL-client-5.6.23-1.sles11.x86_64 [root@localhost ~]# rpm -e --nodeps MySQL-server-5.6.23-1.sles11.x86_64 ... ③使用命令或者去mysql官网下载rpm包
yum安装mysql需要先下载一个基础包安装 [root@localhost ~]# wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm rpm安装需要下载bundle包（rpm文件打包合集）
rpm Bundle表示rpm的打包合集，包含基础包，lib包，client,server 等包如下图
④安装：
yum方式
先安装基础包： [root@localhost ~]# yum install y mysql57-community-release-el7-10.noarch.rpm [root@localhost ~]# yum install mysql-community-server --nogpgcheck --安装完毕 rpm
rpm需要依次安装bundle包中的rpm包，包括基础包，库，client和server等 安装mysql-server服务，只需要安装如下4个软件包即可，使⽤rpm -ivh进⾏安装（按顺序安装，后⾯的 服务依赖前⾯的服务 [root@localhost ~]# rpm -ivh mysql-community-common-5.7.23-1.el7.x86_64.rpm [root@localhost ~]# rpm -ivh mysql-community-libs-5.7.23-1.el7.x86_64.rpm [root@localhost ~]# rpm -ivh mysql-community-client-5.7.23-1.el7.x86_64.rpm [root@localhost ~]# rpm -ivh mysql-community-server-5.7.23-1.el7.x86_64.rpm 安装时如果出现缺少依赖，如少libaio、net-tools还需要yum install [名称]来安装依赖 ⑤启动mysql，设置密码
1、[root@localhost ~]# systemctl start mysqld.service -->启动mysql 2、[root@localhost ~]# grep "password" /var/log/mysqld.log -->查看密码 CSLQ:F=Um5i1 A temporary password is generated for root@localhost: CSLQ:F=Um5i1 3、[root@localhost ~]# mysql -uroot -p -->登录root用户 4、[root@localhost ~]# CSLQ:F=Um5i1 -->输入密码 5、登录进mysql之后设置密码规则(不设置有可能无法修改成简单密码) set global validate_password_policy=0; set global validate_password_length=1; 6、ALTER USER 'root'@'localhost' IDENTIFIED BY 'root'; -->修改密码为root ⑥查找并修改mysql配置文件
1、[root@localhost ~]# which mysql -->查找mysql命令在什么位置 usr/bin/mysql 2.[root@localhost ~]# /usr/bin/mysql --verbose --help | grep -A 1 'Default options' Default options are read from the following files in the given order: /etc/mysql/my.cnf /etc/my.cnf ~/.my.cnf 返回信息表示首先读取的是/etc/mysql/my.cnf文件，如果前一个文件不存在则继续读/etc/my.cnf文件，如若还不存在便会去读~/.my.cnf文件,这三处即是mysql配置文件存放处，找到修改即可 默认配置文件如下： # For advice on how to change settings please see # http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html [mysqld] # # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # # Remove leading # to turn on a very important data integrity option: logging # changes to the binary log between backups. # log_bin # # Remove leading # to set options mainly useful for reporting servers. # The server defaults are faster for transactions and fast SELECTs. # Adjust sizes as needed, experiment to find the optimal values. # join_buffer_size = 128M # sort_buffer_size = 2M # read_rnd_buffer_size = 2M datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid 二、tar包安装和source源码安装 已编译tar包(带bin目录的tar)和source源码安装区别是前者是编译好的，bin文件夹中就是编译之后的二进制可执行文件，后者需要自己手动编译
①带bin的tar包安装
1、[root@localhost ~]# tar -xvf mysql-5.7.26-linux-glibc2.12-x86_64.tar 2、[root@localhost ~]# cp -r mysql-5.7.26-linux-glibc2.12-x86_64 /usr/local/mysql -->拷贝文件夹到/usr/local目录下并重命名为mysql 3、[root@localhost ~]# mkdir /usr/local/mysql/data /usr/local/mysql/logs -->创建data和logs文件夹 4、[root@localhost ~]# groupadd mysql -->添加mysql用户组 5、[root@localhost ~]# useradd -r -g mysql mysql -->向mysql用户组添加mysql用户，-r &lt;参数表示mysql用户是系统用户，不可用于登录系统，-g 参数表示把mysql用户添加到mysql用户组中 6、chown -R mysql:mysql /usr/local/mysql/ -->将mysql目录权限分配给mysql用户组下的mysql用户 已编译tar包安装需要新建mysql用户和用户组，并将mysql目录权限分配给用户，若不进行此操作，在mysql服务启动时会报Starting MySQL. ERROR! The server quit without updating PID file错误 7、[root@localhost ~]# vim /etc/my.cnf -->在/etc下新增配置文件 [mysqld] port = 3306 user = mysql basedir = /usr/local/mysql datadir = /usr/local/mysql/data socket = /usr/local/mysql/data/mysql.sock bind-address = 0.0.0.0 pid-file = /usr/local/mysql/data/mysqld.pid character-set-server = utf8 collation-server = utf8_general_ci max_connections = 200 log-error = /usr/local/mysql/logs/mysqld.log 8、[root@localhost ~]# cd /usr/local/mysql/bin/ -->进入mysql的bin目录 9、[root@localhost ~]#./mysqld --defaults-file=/etc/my.cnf --user=mysql --basedir=/usr/local/mysql/ --datadir=/data/mysql/ --initialize -->初始化 10、[root@localhost ~]# cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql -->将mysql.server放置到/etc/init.d/mysql中 11、[root@localhost ~]# service mysql start -->启动服务 12、[root@localhost ~]# cat /usr/local/mysql/logs/mysqld.log -->日志最后一行有随机生成的初始密码,可登录mysql 13、[root@localhost ~]# ln -s /usr/local/mysql/bin/mysql /usr/bin -->创建软连接到/usr/bin可以全局使用mysql命令(方便登录mysql) 14、[root@localhost ~]# mysql -uroot -p -->登录mysql 15、[root@localhost ~]# 输入12步得到的密码，登录mysql 16、若登录出现Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)，需要将mysql.sock软连接到tmp目录即可 [root@localhost ~]# ln -s /usr/local/mysql/data/mysql.sock /tmp ②source源码安装
(1)创建安装目录 [root@localhost ~]# mkdir /usr/local/mysql/{data,logs,tmp,run} -p (2）首先安装源码编译所需要的包 [root@localhost ~]# yum -y install make gcc-c++ cmake bison-devel ncurses-devel (3)解压 [root@localhost ~]# tar -zxvf mysql-5.7.27.tar.gz [root@localhost ~]# tar -zxvf mysql-boost-5.7.27.tar.gz #（不需要安装，在安装mysql时自动安装，） 两个文件解压以后都会在同一个目录上 mysql-5.7.27 (4)编译安装（编译参数按实际情况制定） cd mysql-5.7.27 cmake . \ -DCMAKE_INSTALL_PREFIX=/usr/local/mysql/ \ -DMYSQL_DATADIR=/usr/local/mysql/data \ -DDOWNLOAD_BOOST=1 \ -DWITH_BOOST=/opt/software/mysql-5.7.27/boost \ -DSYSCONFDIR=/etc \ -DWITH_INNOBASE_STORAGE_ENGINE=1 \ -DWITH_PARTITION_STORAGE_ENGINE=1 \ -DWITH_FEDERATED_STORAGE_ENGINE=1 \ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \ -DWITH_MYISAM_STORAGE_ENGINE=1 \ -DENABLED_LOCAL_INFILE=1 \ -DMYSQL_UNIX_ADDR=/usr/local/mysql/run/mysql.sock \ -DENABLE_DTRACE=0 \ -DDEFAULT_CHARSET=utf8 \ -DDEFAULT_COLLATION=utf8_general_ci \ -DWITH_EMBEDDED_SERVER=1 make &amp; make install (5)/etc/my.cnf配置mysql (6) ./mysqld --defaults-file=/etc/my.cnf --initialize #初始化 (7) cp support-files/mysql.server /etc/init.d/mysql #启动项设置 (8) service mysqld start (9)登录不在赘述 mysql常见报错及处理方式： Can&rsquo;t connect to local MySQL server through socket &lsquo;/tmp/mysql.sock&rsquo; (2)
https://blog.csdn.net/weixin_43464964/article/details/121807241
防火墙问题导致mysql连不上：https://www.68idc.com/news/content/582.html
连接mysql报错误码1130：https://blog.csdn.net/web_15534274656/article/details/126493936
CentOS7查看开放端口命令、查看端口占用情况和开启端口命令、杀掉进程
https://blog.csdn.net/weixin_48053866/article/details/127715462
centos 7 tcp6端口地址不通，导致无法访问：https://www.pianshen.com/article/54202723812/
​</content></entry><entry><title>Markdown语法手册</title><url>/post/markdown-syntax/</url><categories><category>themes</category><category>syntax</category></categories><tags><tag>markdown</tag><tag>css</tag><tag>html</tag></tags><content type="html"> 本文提供了一个可以在 Hugo 内容文件中使用的基本Markdown语法示例，还展示了基本 HTML 元素在 Hugo 主题中是否使用 CSS 装饰。
标题 下面的 HTML 代码&lt;h1>—&lt;h6> 元素表示六个级别的节标题。 &lt;h1>是最高的节级别，&lt;h6>是最低的节级别。
H1 H2 H3 H4 H5 H6 段落 生活是什么？生活是柴米油盐的平淡；是行色匆匆早出晚归的奔波；生活是错的时间遇到对的人的遗憾；是爱的付出与回报；生活是看不同的风景，遇到不同的人；是行至水穷尽，坐看云起时的峰回路转；生活是灵魂经历伤痛后的微笑怒放；是挫折坎坷被晾晒后的坚强；生活是酸甜苦辣被岁月沉淀后的馨香；是经历风霜雪雨洗礼后的懂得；生活是走遍千山万水后，回眸一笑的洒脱。
有些事，猝不及防，不管你在不在乎；有些人，并非所想，不管你明不明白；有些路，必须得走，不管你愿不愿意。不怕事，不惹事，不避事，做好自己，用真心面对一切；少埋怨，少指责，少发火，学会沉静，用微笑考量一切；多体察，多包容，多思索，尽心尽力，虽缺憾但无悔。像蒲公英一样美丽，虽轻盈，但并不卑微，它有自己的生命，也有自己的世界！
引用 blockquote 元素表示从另一个来源引用的内容，可选的引用必须在 footer 或 cite元素内，也可选的内嵌更改，如注释和缩写。
引用没有归属 读懂自我，带着简单的心情，看复杂的人生，走坎坷的路！
注意： 可以在块引用中使用 Markdown 语法。
带归属的引用 不要通过分享记忆来交流，通过交流来分享记忆。
— 罗布·派克1
表格 表不是Markdown核心规范的一部分，但是Hugo支持开箱即用。
Name Age Bob 27 Alice 23 表格内使用Markdown语法 Italics Bold Code italics bold code 图像 ![图像描述](图像地址) 示例 常规用法 SVG图像 Google Chrome
Firefox Browser
小图标
点击图像可以打开图像浏览器，快试试吧。
代码块 带有引号的代码块 &lt;!doctype html> &lt;html lang="en"> &lt;head> &lt;meta charset="utf-8"> &lt;title>Example HTML5 Document&lt;/title> &lt;/head> &lt;body> &lt;p>Test&lt;/p> &lt;/body> &lt;/html> 用四个空格缩进的代码块 &lt;!doctype html>
&lt;html lang="en">
&lt;head>
&lt;meta charset="utf-8">
&lt;title>Example HTML5 Document&lt;/title>
&lt;/head>
&lt;body>
&lt;p>Test&lt;/p>
&lt;/body>
&lt;/html>
代码块引用Hugo的内部高亮短代码 &lt;!doctype html> &lt;html lang="en"> &lt;head> &lt;meta charset="utf-8"> &lt;title>Example HTML5 Document&lt;/title> &lt;/head> &lt;body> &lt;p>Test&lt;/p> &lt;/body> &lt;/html> 列表类型 有序列表 First item Second item Third item 无序列表 List item Another item And another item 嵌套列表 Fruit Apple Orange Banana Dairy Milk Cheese 其他元素 — abbr, sub, sup, kbd, mark GIF 是位图图像格式。
H2O
Xn + Yn = Zn
按 CTRL+ALT+Delete 组合键结束会话。
大多数蝾螈在夜间活动，捕食昆虫、蠕虫和其他小动物。
以上引文摘自Rob Pike在2015年11月18日 Gopherfest 上的演讲
。&#160;&#8617;&#xfe0e;</content></entry><entry><title>富文本内容测试pp</title><url>/post/rich-content/</url><categories/><tags><tag>shortcodes</tag><tag>privacy</tag></tags><content type="html"> Hugo 上有几个内置短码
，用于丰富内容，以及隐私配置
还有一组简单的短代码，支持各种社交媒体嵌入的静态和非 JS 版本。
YouTube 增强隐私短码 {{/&lt; youtube ZJthWmvUzzc >/}}
Twitter 短码 {{/&lt; twitter_simple 1085870671291310081 >/}}
Vimeo 短码 {{/&lt; vimeo_simple 48912912 >/}}
哔哩哔哩短码</content></entry><entry><title>图像占位符显示</title><url>/post/placeholder-text/</url><categories/><tags><tag>markdown</tag><tag>text</tag></tags><content type="html"> 范德格拉夫原理（Van de Graaf Canon）重构了曾经用于书籍设计中将页面划分为舒适比例的方法。这一原理也被称为“秘密原理”，用于许多中世纪的手稿和古板书中。在范德格拉夫原理中，文本区域和页面的长款具有相同的比例，并且文本区域的高度等于页面宽度，通过划分页面得到九分之一的订口边距和九分之二的切口边距，以及与页面长宽相同的比例的文本区域。
Vagus 示例
The Van de Graaf Canon
总结 当然设计中的黄金比例是为人所熟知的，黄金分割的公式为a:b=b:(a+b)。这是指较小的两个矩形与较大的两个矩形以相同的组合方式相关联。黄金分割比例为1:1.618。</content></entry><entry><title>数据公式设置显示</title><url>/post/math-typesetting/</url><categories/><tags/><content type="html"> Hugo 项目中的数学表示法可以通过使用第三方 JavaScript 库来实现。
在这个例子中，我们将使用 MathJax
创建一个文件 /content/en[zh-CN]/math.md
可以全局启用MathJax，请在项目配置中将参数math设置为true
或是在每页基础上启用MathJax，在内容文件中包括参数math: true
注意： 使用支持的TeX功能
的联机参考资料
例子 重复的分数 $$ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} \equiv 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } } $$
总和记号 $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$
几何级数之和 我把接下来的两个例子分成了几行，这样它在手机上表现得更好。这就是为什么它们包含 \displaystyle。
$$ \displaystyle\sum_{i=1}^{k+1}i $$
$$ \displaystyle= \left(\sum_{i=1}^{k}i\right) +(k+1) $$
$$ \displaystyle= \frac{k(k+1)}{2}+k+1 $$
$$ \displaystyle= \frac{k(k+1)+2(k+1)}{2} $$
$$ \displaystyle= \frac{(k+1)(k+2)}{2} $$
$$ \displaystyle= \frac{(k+1)((k+1)+1)}{2} $$
乘记号 $$ \displaystyle 1 + \frac{q^2}{(1-q)}+\frac{q^6}{(1-q)(1-q^2)}+\cdots = \displaystyle \prod_{j=0}^{\infty}\frac{1}{(1-q^{5j+2})(1-q^{5j+3})}, \displaystyle\text{ for }\lvert q\rvert &lt; 1. $$
随文数式 这是一些线性数学: $$ k_{n+1} = n^2 + k_n^2 - k_{n-1} $$ ， 然后是更多的文本。
希腊字母 $$ \Gamma\ \Delta\ \Theta\ \Lambda\ \Xi\ \Pi\ \Sigma\ \Upsilon\ \Phi\ \Psi\ \Omega \alpha\ \beta\ \gamma\ \delta\ \epsilon\ \zeta\ \eta\ \theta\ \iota\ \kappa\ \lambda\ \mu\ \nu\ \xi \ \omicron\ \pi\ \rho\ \sigma\ \tau\ \upsilon\ \phi\ \chi\ \psi\ \omega\ \varepsilon\ \vartheta\ \varpi\ \varrho\ \varsigma\ \varphi $$
箭头 $$ \gets\ \to\ \leftarrow\ \rightarrow\ \uparrow\ \Uparrow\ \downarrow\ \Downarrow\ \updownarrow\ \Updownarrow $$
$$ \Leftarrow\ \Rightarrow\ \leftrightarrow\ \Leftrightarrow\ \mapsto\ \hookleftarrow \leftharpoonup\ \leftharpoondown\ \rightleftharpoons\ \longleftarrow\ \Longleftarrow\ \longrightarrow $$
$$ \Longrightarrow\ \longleftrightarrow\ \Longleftrightarrow\ \longmapsto\ \hookrightarrow\ \rightharpoonup $$
$$ \rightharpoondown\ \leadsto\ \nearrow\ \searrow\ \swarrow\ \nwarrow $$
符号 $$ \surd\ \barwedge\ \veebar\ \odot\ \oplus\ \otimes\ \oslash\ \circledcirc\ \boxdot\ \bigtriangleup $$
$$ \bigtriangledown\ \dagger\ \diamond\ \star\ \triangleleft\ \triangleright\ \angle\ \infty\ \prime\ \triangle $$
微积分学 $$ \int u \frac{dv}{dx},dx=uv-\int \frac{du}{dx}v,dx $$
$$ f(x) = \int_{-\infty}^\infty \hat f(\xi),e^{2 \pi i \xi x} $$
$$ \oint \vec{F} \cdot d\vec{s}=0 $$
洛伦茨方程 $$ \begin{aligned} \dot{x} &amp; = \sigma(y-x) \ \dot{y} &amp; = \rho x - y - xz \ \dot{z} &amp; = -\beta z + xy \end{aligned} $$
交叉乘积 这在KaTeX中是可行的，但在这种环境中馏分的分离不是很好。
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
这里有一个解决方案:使用“mfrac”类(在MathJax情况下没有区别)的额外类使分数更小:
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
强调 $$ \hat{x}\ \vec{x}\ \ddot{x} $$
有弹性的括号 $$ \left(\frac{x^2}{y^3}\right) $$
评估范围 $$ \left.\frac{x^3}{3}\right|_0^1 $$
诊断标准 $$ f(n) = \begin{cases} \frac{n}{2}, &amp; \text{if } n\text{ is even} \ 3n+1, &amp; \text{if } n\text{ is odd} \end{cases} $$
麦克斯韦方程组 $$ \begin{aligned} \nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \ \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \ \nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; = \vec{\mathbf{0}} \ \nabla \cdot \vec{\mathbf{B}} &amp; = 0 \end{aligned} $$
这些方程式很狭窄。我们可以使用(例如)添加垂直间距 [1em] 在每个换行符(\)之后。正如你在这里看到的：
$$ \begin{aligned} \nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \[1em] \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \[0.5em] \nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; = \vec{\mathbf{0}} \[1em] \nabla \cdot \vec{\mathbf{B}} &amp; = 0 \end{aligned} $$
统计学 固定词组：
$$ \frac{n!}{k!(n-k)!} = {^n}C_k {n \choose k} $$
分数在分数 $$ \frac{\frac{1}{x}+\frac{1}{y}}{y-z} $$
ｎ次方根 $$ \sqrt[n]{1+x+x^2+x^3+\ldots} $$
矩阵 $$ \begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\ a_{21} &amp; a_{22} &amp; a_{23}\ a_{31} &amp; a_{32} &amp; a_{33} \end{pmatrix} \begin{bmatrix} 0 &amp; \cdots &amp; 0 \ \vdots &amp; \ddots &amp; \vdots \ 0 &amp; \cdots &amp; 0 \end{bmatrix} $$
标点符号 $$ f(x) = \sqrt{1+x} \quad (x \ge -1) f(x) \sim x^2 \quad (x\to\infty) $$
现在用标点符号:
$$ f(x) = \sqrt{1+x}, \quad x \ge -1 f(x) \sim x^2, \quad x\to\infty $$</content></entry><entry><title>关于我</title><url>/about.html</url><categories/><tags/><content type="html"> Hugo是用Go编写的一个开放源代码静态站点生成器，可在Apache许可证2.0
下使用。 Hugo支持TOML, YAML和JSON数据文件类型，Markdown和HTML内容文件，并使用短代码添加丰富的内容。其他值得注意的功能包括分类法、多语言模式、图像处理、自定义输出格式、HTML/CSS/JS缩小和对Sass SCSS工作流的支持。
Hugo使用了多种开源项目，包括:
https://github.com/yuin/goldmark
https://github.com/alecthomas/chroma
https://github.com/muesli/smartcrop
https://github.com/spf13/cobra
https://github.com/spf13/viper
Hugo是博客、企业网站、创意作品集、在线杂志、单页应用程序甚至是数千页的网站的理想选择。
Hugo适合那些想要手工编写自己的网站代码，而不用担心设置复杂的运行时、依赖关系和数据库的人。
使用Hugo建立的网站非常快速、安全，可以部署在任何地方，包括AWS、GitHub Pages、Heroku、Netlify和任何其他托管提供商。
更多信息请访问GitHub
.</content></entry></search>